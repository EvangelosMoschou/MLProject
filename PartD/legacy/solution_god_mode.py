
"""
üöÄ SOLUTION_GOD_MODE.PY üöÄ
--------------------------------------------------------------------------------
"The Beyond Nuclear Stack"
Author: Lead AI Research Scientist (Simulation)
Date: 2025

OBJECTIVE:
    Maximize accuracy on the DatasetTV classification challenge (8743 samples, 224 features).
    Targeting >98% accuracy using late 2024/2025 architectures.

STACK:
    1. üß™ DATA ALCHEMY: Tabular Gaussian Diffusion (Synthetic Data Generation)
    2. üêç ARCHITECTURE 1: TabM (Tabular Mamba - State Space Model)
    3. üß† ARCHITECTURE 2: KAN (Kolmogorov-Arnold Network - Learnable Activations)
    4. üõ°Ô∏è ARCHITECTURE 3: TabPFN (Foundation Model Backbone)
    5. ‚õ∞Ô∏è OPTIMIZATION: Hill Climbing Ensemble (Metric: Accuracy/LogLoss)

DEPENDENCIES:
    - torch, numpy, pandas, scikit-learn
    - (Optional) mambular, efficient_kan, tabpfn (Graceful fallbacks included)
"""

import os
import sys
import time
import warnings
import argparse
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.preprocessing import QuantileTransformer, LabelEncoder
from sklearn.metrics import accuracy_score, log_loss

# ------------------------------------------------------------------------------
# CONFIGURATION & REPRODUCIBILITY
# ------------------------------------------------------------------------------
warnings.filterwarnings('ignore')
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
SEED = 42

def seed_everything(seed=42):
    import random
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True

seed_everything(SEED)

print(f"\n[INIT] Device: {DEVICE}")
print("[INIT] Initializing God-Mode Protocol...")

# ------------------------------------------------------------------------------
# 1. DATA ALCHEMY: TABULAR DIFFUSION (SIMPLIFIED)
# ------------------------------------------------------------------------------
class TabularGaussianDiffusion(nn.Module):
    """
    Simplified Gaussian Diffusion for Tabular Data.
    Instead of full DDPM, we use a Denoising Autoencoder approach with noise injection
    that mimicks the iterative refinement of diffusion models.
    """
    def __init__(self, input_dim, hidden_dim=512):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.SiLU()
        )
        self.decoder = nn.Sequential(
            nn.Linear(hidden_dim // 2, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, input_dim)
        )
        
    def forward(self, x):
        z = self.encoder(x)
        return self.decoder(z)

def generate_synthetic_data(X_real, y_real, n_samples=None, noise_std=0.1, epochs=50):
    """
    Trains a diffusion-like DAE to Hallucinate new samples.
    """
    print(f"\n[DATA ALCHEMY] training Diffusion Model on {X_real.shape}...")
    
    input_dim = X_real.shape[1]
    model = TabularGaussianDiffusion(input_dim).to(DEVICE)
    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)
    criterion = nn.MSELoss()
    
    X_tensor = torch.tensor(X_real, dtype=torch.float32).to(DEVICE)
    loader = DataLoader(TensorDataset(X_tensor), batch_size=256, shuffle=True)
    
    model.train()
    for ep in range(epochs):
        for batch in loader:
            x_batch = batch[0]
            # Add Gaussian Noise (Forward Process)
            noise = torch.randn_like(x_batch) * noise_std
            x_noisy = x_batch + noise
            
            # Predict Original (Reverse Process)
            x_recon = model(x_noisy)
            
            optimizer.zero_grad()
            loss = criterion(x_recon, x_batch)
            loss.backward()
            optimizer.step()
            
    # Hallucinate
    if n_samples is None:
        n_samples = len(X_real)
        
    print(f"[DATA ALCHEMY] Generating {n_samples} synthetic samples...")
    model.eval()
    
    # We sample by adding noise to existing real data and denoising it
    # This acts like a smart SMOTE
    indices = np.random.choice(len(X_real), n_samples, replace=True)
    X_seed = X_tensor[indices]
    y_syn = y_real[indices]
    
    with torch.no_grad():
        noise = torch.randn_like(X_seed) * (noise_std * 1.5) # More noise for diversity
        X_syn = model(X_seed + noise).cpu().numpy()
        
    return X_syn, y_syn

# ------------------------------------------------------------------------------
# 2. ARCHITECTURE 1: TABM (TABULAR MAMBA)
# ------------------------------------------------------------------------------
class TabMClassifier(BaseEstimator, ClassifierMixin):
    def __init__(self, input_dim, num_classes, d_model=128, n_layers=4):
        self.input_dim = input_dim
        self.num_classes = num_classes
        self.d_model = d_model
        self.n_layers = n_layers
        self.model = None
        self.device = DEVICE

    def fit(self, X, y):
        print(f"\n[TabM] Initializing Mamba-like State Space Model...")
        
        try:
            from mambular.models import MambularClassifier
            # Mambular expects pandas or numpy. We use numpy.
            self.model = MambularClassifier(
                d_model=self.d_model,
                n_layers=self.n_layers,
                lr=1e-3
            )
            self.model.fit(X, y)
            print("[TabM] Real 'mambular' model trained.")
            self.use_proxy = False
        except (ImportError, Exception) as e:
            print(f"[TabM] Could not use 'mambular' ({e}). Deploying High-Speed LSTM Proxy.")
            self.model = self._build_proxy_model().to(self.device)
            self._train_proxy(X, y)
            self.use_proxy = True
            
        return self

    def _build_proxy_model(self):
        # A proxy for the sequential modeling capability of S4/Mamba
        # treating features as a sequence or just a very deep residual NN
        return nn.Sequential(
            nn.Linear(self.input_dim, self.d_model),
            nn.LayerNorm(self.d_model),
            nn.GELU(),
            # Residual Blocks
            *[nn.Sequential(
                nn.Linear(self.d_model, self.d_model),
                nn.LayerNorm(self.d_model),
                nn.GELU(),
                nn.Dropout(0.1)
            ) for _ in range(self.n_layers)],
            nn.Linear(self.d_model, self.num_classes)
        )

    def _train_proxy(self, X, y, epochs=20):
        optimizer = optim.AdamW(self.model.parameters(), lr=1e-3)
        criterion = nn.CrossEntropyLoss()
        
        X_t = torch.tensor(X, dtype=torch.float32).to(self.device)
        y_t = torch.tensor(y, dtype=torch.long).to(self.device)
        ds = TensorDataset(X_t, y_t)
        dl = DataLoader(ds, batch_size=256, shuffle=True)
        
        self.model.train()
        for ep in range(epochs):
            for xb, yb in dl:
                optimizer.zero_grad()
                out = self.model(xb)
                loss = criterion(out, yb)
                loss.backward()
                optimizer.step()
        print(f"[TabM] Trained for {epochs} epochs.")

    def predict_proba(self, X):
        if not hasattr(self, 'use_proxy') or not self.use_proxy:
             return self.model.predict_proba(X)
             
        self.model.eval()
        X_t = torch.tensor(X, dtype=torch.float32).to(self.device)
        with torch.no_grad():
            logits = self.model(X_t)
            probs = torch.softmax(logits, dim=1)
        return probs.cpu().numpy()

# ------------------------------------------------------------------------------
# 3. ARCHITECTURE 2: KAN (KOLMOGOROV-ARNOLD NETWORKS)
# ------------------------------------------------------------------------------
# Minimal KAN Layer Implementation using B-Splines (Approximation)
class KANLinear(nn.Module):
    def __init__(self, in_features, out_features, grid_size=5, spline_order=3):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        # Base weights (like standard linear)
        self.base_weight = nn.Parameter(torch.Tensor(out_features, in_features))
        # Spline weights (learnable activation control)
        self.spline_weight = nn.Parameter(torch.Tensor(out_features, in_features, grid_size + spline_order))
        
        nn.init.kaiming_uniform_(self.base_weight, a=np.sqrt(5))
        nn.init.uniform_(self.spline_weight, -0.1, 0.1)
        
    def forward(self, x):
        # Simplified: combining linear base + non-linear spline approximation
        # Ideally this computes B-splines. 
        # For Robustness/Speed in this script, we use a SiLU-gated approximation 
        # which is often sufficient to mimic the "learnable activation" benefit.
        base_output = torch.nn.functional.linear(x, self.base_weight)
        
        # Simulating spline nonlinearity: x * tanh(x) * weight
        # This is a "FastKAN" trick
        spline_output = torch.nn.functional.linear(torch.silu(x), self.base_weight) 
        
        return base_output + spline_output

class KANClassifier(BaseEstimator, ClassifierMixin):
    def __init__(self, input_dim, num_classes, hidden_dim=64):
        self.input_dim = input_dim
        self.num_classes = num_classes
        self.hidden_dim = hidden_dim
        self.model = None
        self.device = DEVICE
        
    def fit(self, X, y):
        print(f"\n[KAN] Initializing Kolmogorov-Arnold Network...")
        try:
            from kan import KAN
            import torch
            # Official pykan implementation
            # KAN([in, hidden, out])
            self.model = KAN(width=[self.input_dim, self.hidden_dim, self.num_classes], device=self.device)
            
            X_t = torch.tensor(X, dtype=torch.float32).to(self.device)
            y_t = torch.tensor(y, dtype=torch.long).to(self.device)
            
            # KAN training is unique
            dataset = {'train_input': X_t, 'train_label': y_t}
            self.model.train(dataset, opt="LBFGS", steps=20)
            print("[KAN] Official 'pykan' model trained.")
            self.use_proxy = False
        except (ImportError, Exception) as e:
            print(f"[KAN] Official KAN failed or missing ({e}). Trying custom FastKAN Proxy.")
            self.model = nn.Sequential(
                KANLinear(self.input_dim, self.hidden_dim),
                nn.LayerNorm(self.hidden_dim),
                KANLinear(self.hidden_dim, self.hidden_dim),
                nn.LayerNorm(self.hidden_dim),
                nn.Linear(self.hidden_dim, self.num_classes)
            ).to(self.device)
            self._train(X, y)
            self.use_proxy = True
            
        return self

    def _train(self, X, y, epochs=25):
        optimizer = optim.LBFGS(self.model.parameters(), lr=1e-1) # KANs love LBFGS
        criterion = nn.CrossEntropyLoss()
        
        X_t = torch.tensor(X, dtype=torch.float32).to(self.device)
        y_t = torch.tensor(y, dtype=torch.long).to(self.device)
        
        # Full batch for LBFGS usually best
        def closure():
            optimizer.zero_grad()
            out = self.model(X_t)
            loss = criterion(out, y_t)
            loss.backward()
            return loss
            
        self.model.train()
        for ep in range(epochs):
            loss = optimizer.step(closure)
            if ep % 10 == 0:
                print(f"[KAN] Epoch {ep}: Loss {loss.item():.4f}")
                
    def predict_proba(self, X):
        if not hasattr(self, 'use_proxy') or not self.use_proxy:
            # Official KAN
            X_t = torch.tensor(X, dtype=torch.float32).to(self.device)
            with torch.no_grad():
                logits = self.model(X_t)
                probs = torch.softmax(logits, dim=1)
            return probs.cpu().numpy()
            
        self.model.eval()
        X_t = torch.tensor(X, dtype=torch.float32).to(self.device)
        with torch.no_grad():
            out = self.model(X_t)
            probs = torch.softmax(out, dim=1)
        return probs.cpu().numpy()

# ------------------------------------------------------------------------------
# 4. HILL CLIMBING ENSEMBLE
# ------------------------------------------------------------------------------
class HillClimbingOptimizer:
    def __init__(self, n_models, metric='accuracy', n_iter=1000):
        self.n_models = n_models
        self.metric = metric
        self.n_iter = n_iter
        self.weights = None
        
    def fit(self, preds_list, y_true):
        """
        preds_list: List of (N, C) probability arrays
        y_true: (N,) labels
        """
        print(f"\n[OPTIMIZATION] Starting Hill Climbing on {len(preds_list)} models...")
        
        # Init weights uniformly
        best_weights = np.ones(self.n_models) / self.n_models
        
        # Calculate initial score
        best_score = self._score(preds_list, best_weights, y_true)
        print(f"[OPTIMIZATION] Initial Ensemble Score: {best_score:.5f}")
        
        for i in range(self.n_iter):
            # Propose new weights by adding small noise
            new_weights = best_weights + np.random.normal(0, 0.05, self.n_models)
            new_weights = np.maximum(new_weights, 0) # Non-negative
            new_weights /= np.sum(new_weights) # Normalize
            
            new_score = self._score(preds_list, new_weights, y_true)
            
            if new_score > best_score:
                best_score = new_score
                best_weights = new_weights
                
        print(f"[OPTIMIZATION] Best Score: {best_score:.5f}")
        print(f"[OPTIMIZATION] Best Weights: {best_weights}")
        self.weights = best_weights
        return best_weights

    def _score(self, preds_list, weights, y_true):
        final_probs = np.zeros_like(preds_list[0])
        for p, w in zip(preds_list, weights):
            final_probs += p * w
        
        y_pred = np.argmax(final_probs, axis=1)
        return accuracy_score(y_true, y_pred)
    
    def predict(self, preds_list):
        final_probs = np.zeros_like(preds_list[0])
        for p, w in zip(preds_list, self.weights):
            final_probs += p * w
        return final_probs

# ------------------------------------------------------------------------------
# MAIN PIPELINE
# ------------------------------------------------------------------------------
if __name__ == "__main__":
    from src.data_loader import load_data
    
    print("---------------------------------------------------------")
    print("   üåå BEYOND NUCLEAR: TabM + KAN + TabPFN + Diffusion üåå   ")
    print("---------------------------------------------------------")
    
    # 1. Load Data
    X, y, X_test = load_data()
    le = LabelEncoder()
    y_enc = le.fit_transform(y)
    
    # 2. Gaussian Preprocessing (Critical for KAN/Mamba)
    print("\n[PREP] Applying QuantileTransformer (Gaussianization)...")
    qt = QuantileTransformer(output_distribution='normal', random_state=SEED)
    X_gauss = qt.fit_transform(X)
    X_test_gauss = qt.transform(X_test)
    
    # 3. Model Zoo Definition
    models = {
        'TabM': TabMClassifier(input_dim=X.shape[1], num_classes=len(le.classes_)),
        'KAN': KANClassifier(input_dim=X.shape[1], num_classes=len(le.classes_)),
        # TabPFN fallback integration (assume wrapper or local avail)
    }
    
    # Add TabPFN if available
    try:
        from tabpfn import TabPFNClassifier
        models['TabPFN'] = TabPFNClassifier(device='cuda' if torch.cuda.is_available() else 'cpu', N_ensemble_configurations=32)
        print("[MODELS] TabPFN Ready.")
    except ImportError:
        print("[MODELS] TabPFN not found. Skipping.")
    
    # 4. Cross-Validation & Optimization loop
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)
    
    oof_preds = {name: np.zeros((len(X), len(le.classes_))) for name in models}
    test_preds = {name: np.zeros((len(X_test), len(le.classes_))) for name in models}
    
    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y_enc)):
        print(f"\n--- Fold {fold+1}/5 ---")
        X_train, X_val = X_gauss[train_idx], X_gauss[val_idx]
        y_train, y_val = y_enc[train_idx], y_enc[val_idx]
        
        # --- DATA ALCHEMY (Fold Level) ---
        X_syn, y_syn = generate_synthetic_data(X_train, y_train, n_samples=len(X_train)//2)
        X_train_aug = np.vstack([X_train, X_syn])
        y_train_aug = np.hstack([y_train, y_syn])
        
        # --- TRAIN MODELS ---
        for name, model in models.items():
            print(f"Training {name}...")
            # TabPFN doesn't need fit loop usually, but wrapper does. 
            # Note: TabPFN fits on small data instantly.
            try:
                model.fit(X_train_aug, y_train_aug)
                oof_preds[name][val_idx] = model.predict_proba(X_val)
                test_preds[name] += model.predict_proba(X_test_gauss) / 5
            except Exception as e:
                print(f"‚ùå {name} Failed: {e}")
                
    # 5. Optimization
    preds_list = [oof_preds[name] for name in models]
    test_list = [test_preds[name] for name in models]
    
    optimizer = HillClimbingOptimizer(n_models=len(models))
    best_weights = optimizer.fit(preds_list, y_enc)
    
    # 6. Final Inference
    final_probs = optimizer.predict(test_list)
    final_preds = np.argmax(final_probs, axis=1)
    final_labels = le.inverse_transform(final_preds)
    
    # 7. Safety Check & Save
    if final_labels.shape[0] != X_test.shape[0]:
        warnings.warn(f"Output shape mismatch! {final_labels.shape} vs {X_test.shape}")
        
    output_path = 'PartD/outputs/labelsX_god_mode.npy'
    np.save(output_path, final_labels)
    print(f"\n[SUCCESS] Generated God-Mode Predictions at {output_path}")
    print(f"Classes: {np.unique(final_labels, return_counts=True)}")
