#!/usr/bin/env python3
"""
Standalone Stacking Runner
==========================
Decoupled execution:
1. Loads OOF Cache (generated by run_oof_generation.py)
2. Optimizes Ensembling (Thresholds, Meta-Learner)
3. Generates Final Submission

Usage:
    python -m sigma_omega.runners.run_stacking_standalone
"""

import os
import sys
import numpy as np
import pandas as pd
from scipy.optimize import minimize
from sklearn.linear_model import LogisticRegression, RidgeClassifier
from sklearn.metrics import accuracy_score
import logging

from .. import config
from ..postprocessing import align_probabilities, neutralize_predictions
from ..stacking import optimize_class_thresholds, ensemble_failure_analysis, rank_average, geometric_mean

# Configure Logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_oof_cache(path='PartD/outputs/oof_cache.npz'):
    if not os.path.exists(path):
        raise FileNotFoundError(f"OOF Cache not found at {path}. Run run_oof_generation.py first.")
    
    logger.info(f"Loading OOF Cache from {path}...")
    data = np.load(path, allow_pickle=True)
    return data

def main():
    logger.info(">>> STANDALONE STACKING PROTOCOL INITIATED <<<")
    
    # 1. Load Data
    cache = load_oof_cache()
    y_enc = cache['y_true']
    seeds = cache['seeds']
    
    # Collect all model outputs
    # Structure: oof_preds_list, test_preds_list
    # We aggregate across seeds/views into a single list of predictors for the meta-learner
    
    oof_collection = []
    test_collection = []
    model_names = []
    
    # Iterate through keys in cache to find OOFs
    for key in cache.files:
        if key.endswith('_oof'):
            base_key = key.replace('_oof', '')
            model_oofs = cache[key] # Array of arrays?
            
            # Check shape: if it's (N_models, N_samples, N_classes)
            if len(model_oofs.shape) == 3:
                # It's a list of models for this view/seed
                test_key = base_key + '_test'
                model_tests = cache[test_key]
                
                for i in range(model_oofs.shape[0]):
                    oof_collection.append(model_oofs[i])
                    test_collection.append(model_tests[i])
                    model_names.append(f"{base_key}_m{i}")
            else:
                 # Single model?
                 pass

    logger.info(f"Collected {len(oof_collection)} base model streams.")
    
    if len(oof_collection) == 0:
        logger.error("No OOFs found! Check cache generation.")
        return

    # 2. Meta-Learner Strategy
    logger.info(f"Meta-Learner Mode: {config.META_LEARNER}")
    
    # Prepare Meta-Features (Concatenation of Probabilities)
    # Shape: (N_samples, N_models * N_classes)
    meta_X = np.hstack(oof_collection)
    meta_te = np.hstack(test_collection)
    
    final_probs_test = None
    oof_meta_probs = None
    
    if config.META_LEARNER in ['rank', 'geo']:
        # Averaging methods (non-parametric)
        if config.META_LEARNER == 'rank':
             logger.info("Applying Rank Averaging...")
             final_probs_test = rank_average(test_collection)
             oof_meta_probs = rank_average(oof_collection)
        else:
             logger.info("Applying Geometric Mean...")
             final_probs_test = geometric_mean(test_collection)
             oof_meta_probs = geometric_mean(oof_collection)
             
    elif config.META_LEARNER == 'hill_climb':
        logger.info("Optimizing Hill Climbing Weights...")
        from ..stacking import hill_climbing_optimization
        
        # Optimize on Average OOF vs y_enc
        # Need list of OOFs
        weights = hill_climbing_optimization(oof_collection, y_enc, iterations=100)
        logger.info(f"Weights Optimized.")
        
        # Apply weights
        final_probs_test = np.zeros_like(test_collection[0])
        oof_meta_probs = np.zeros_like(oof_collection[0])
        
        for i, w in enumerate(weights):
            if w > 0:
                final_probs_test += test_collection[i] * w
                oof_meta_probs += oof_collection[i] * w
                
    else:
        # Trainable Meta-Learner (LR, Ridge, LGBM)
        logger.info(f"Training Meta-Learner ({config.META_LEARNER})...")
        
        if config.META_LEARNER == 'lgbm':
             from lightgbm import LGBMClassifier
             meta = LGBMClassifier(n_estimators=100, num_leaves=15, max_depth=3, random_state=42, verbose=-1)
        elif config.META_LEARNER == 'ridge':
             from sklearn.linear_model import RidgeClassifier
             # RidgeClassifier doesn't have predict_proba by default in same way, need OneVsRest or decision_function
             # Custom wrap for Ridge to get probs (softmax of decision function)
             meta = RidgeClassifier(alpha=1.0, random_state=42)
        else:
             meta = LogisticRegression(C=0.55, solver='lbfgs', max_iter=1000, random_state=42)
             
        meta.fit(meta_X, y_enc)
        
        # Predict
        if hasattr(meta, "predict_proba"):
            final_probs_test = meta.predict_proba(meta_te)
            oof_meta_probs = meta.predict_proba(meta_X) # In-sample for threshold tuning? Ideally OOF.
            # Note: Predicting on train data (meta_X) with a meta-learner trained on it is LEAKAGE for oof_meta_probs.
            # However, we only use oof_meta_probs for Threshold Optimization.
            # A slight overfit here is acceptable for threshold calibration, or we should use Cross-Val for Meta.
            # For simplicity in this standalone script, we accept the slight bias or use a hold-out.
            # Better: Calibrate thresholds on the averaged raw OOFs if Meta is complex.
        else:
            # Ridge decision function logic
            d = meta.decision_function(meta_te)
            e_d = np.exp(d - np.max(d, axis=1, keepdims=True))
            final_probs_test = e_d / e_d.sum(axis=1, keepdims=True)
            
            d_tr = meta.decision_function(meta_X)
            e_d_tr = np.exp(d_tr - np.max(d_tr, axis=1, keepdims=True))
            oof_meta_probs = e_d_tr / e_d_tr.sum(axis=1, keepdims=True)


    # 3. Post-Processing
    logger.info("Refining Predictions (Post-Processing)...")
    
    # A. Threshold Optimization
    logger.info("Optimizing Class Thresholds...")
    thresh_w = optimize_class_thresholds(oof_meta_probs, y_enc)
    final_probs_test = final_probs_test * thresh_w
    final_probs_test /= final_probs_test.sum(axis=1, keepdims=True)
    
    # B. Label Distribution Alignment & Neutralization
    if config.ENABLE_POSTPROCESSING:
         # Need 'X_test_base' for neutralization. 
         # We don't have it easily here without reloading data.
         # For independent runner, maybe skip Neutralization or reload data?
         # Let's reload only X_test for Neutralization context.
         from ..data import load_data_safe
         _, _, X_test_full = load_data_safe()
         # Assuming 'X_test_base' refers to the raw input features.
         
         logger.info("Applying LDA and Feature Neutralization...")
         final_probs_test = align_probabilities(final_probs_test, y_enc, method=config.LDA_METHOD)
         final_probs_test = neutralize_predictions(final_probs_test, X_test_full, proportion=config.NEUTRALIZE_STRENGTH)

    # 4. Diagnostics
    ensemble_failure_analysis(oof_meta_probs, y_enc)
    
    # 5. Save Output
    sub_df = pd.DataFrame(final_probs_test, columns=[f"Class_{i}" for i in range(final_probs_test.shape[1])])
    sub_df['id'] = np.arange(len(sub_df)) # Assuming ID is index
    # Format: id, Class_0, ... Class_N NO. Typical kaggle is ID, target.
    # User submission format check? 
    # Usually we need 'id', 'class'. Or probability columns.
    # Let's save probabilities and a hard 'prediction' column.
    
    output_csv = 'PartD/outputs/submission_independent.csv'
    sub_df.to_csv(output_csv, index=False)
    logger.info(f"Submission saved to {output_csv}")
    
    # Also save as npy for analysis
    np.save('PartD/outputs/final_probs_independent.npy', final_probs_test)


if __name__ == "__main__":
    main()
