# Τεχνική Αναφορά: Πρωτόκολλο Epsilon (Part D)
## Η Τελική Θεωρία της Ταξινόμησης

Η παρούσα αναφορά περιγράφει σε βάθος τη λειτουργία του **Πρωτοκόλλου Epsilon**, το οποίο αποτελεί το αποκορύφωμα του συστήματος ταξινόμησης για το Part D. Το σύστημα ξεφεύγει από τις κλασικές προσεγγίσεις συνόλων (ensembles) και υιοθετεί αρχιτεκτονικές αιχμής που συνδυάζουν την **Διακριτική Ικανότητα (Discriminative Power)** με την **Γεννητική Επαλήθευση (Generative Verification)**.

---

## 1. Μηχανική Χαρακτηριστικών & Προεπεξεργασία

Η ποιότητα της εισόδου καθορίζεται από τρεις προηγμένους μηχανισμούς:

### Α. Quantile Transformation (Gaussian Mapping)
Ο **Gaussian Quantile Transformer** είναι ένας πανίσχυρος μη-γραμμικός μετασχηματιστής που "επιβάλλει" στα δεδομένα μας την Κανονική (Gaussian) κατανομή. 
*   **Πώς λειτουργεί;** Πρώτα, ταξινομεί τις τιμές κάθε χαρακτηριστικού και τους αποδίδει ένα **Rank** (κατάταξη). Το Rank είναι η σειρά της τιμής (π.χ. 1η, 2η, 100η). Με βάση αυτή τη σειρά, υπολογίζει την "αθροιστική συνάρτηση κατανομής" (CDF), δηλαδή το σε ποιο ποσοστό (percentile) ανήκει κάθε τιμή. Στη συνέχεια, αντιστοιχεί αυτά τα ποσοστά στις αντίστοιχες τιμές μιας κανονικής κατανομής (Bell Curve).
*   **Γιατί Rank;** Με τη χρήση του Rank, ο μετασχηματιστής αγνοεί τις απόλυτες αποστάσεις μεταξύ των τιμών. Ένα ακραίο outlier (π.χ. μια τιμή 1.000.000 ενώ οι άλλες είναι κοντά στο 10) θα λάβει απλώς το επόμενο διαθέσιμο Rank, εμποδίζοντας την αριθμητική "έκρηξη" του μοντέλου.
*   **Γιατί είναι σημαντικός;**
    1.  **Ανθεκτικότητα σε Outliers:** Οι ακραίες τιμές δεν επηρεάζουν πλέον το μοντέλο ως τεράστια νούμερα, αλλά απλώς ως δείγματα που βρίσκονται στις "ουρές" της καμπάνας.
    2.  **Σύγκλιση:** Τα νευρωνικά δίκτυα (ειδικά το TabR και το Mamba) εκπαιδεύονται πολύ πιο γρήγορα και σταθερά όταν οι είσοδοι είναι κανονικοποιημένες γύρω από το μηδέν με σταθερή διακύμανση.
    3.  **Γραμμικοποίηση:** Μπορεί να αποκαλύψει γραμμικές σχέσεις που ήταν κρυμμένες πίσω από πολύπλοκες, skewed κατανομές.

### Β. Manifold Engineering (Η Υπόθεση της Πολλαπλότητας)
Για να καταλάβουμε το **Manifold (Πολλαπλότητα)** στο ML, πρέπει να σκεφτούμε την **"Υπόθεση της Πολλαπλότητας" (Manifold Hypothesis)**.
*   **Τι είναι το Manifold;** Φανταστείτε ένα τσαλακωμένο χαρτί μέσα σε ένα δωμάτιο (3D χώρος). Αν και βρίσκεται στις 3 διαστάσεις, η "ουσία" του (η πληροφορία του) είναι 2 διαστάσεων. Αυτό το 2D σχήμα που είναι διπλωμένο μέσα στο 3D χώρο είναι το Manifold.
*   **Στο Dataset μας:** Αν και έχουμε πολλά χαρακτηριστικά (υψηλή διάσταση), η πραγματική πληροφορία που διαχωρίζει τις κλάσεις συνήθως "ζει" σε μια επιφάνεια πολύ χαμηλότερης διάστασης. 
*   **Manifold Engineering:** Οι τεχνικές μας (LID, PageRank) προσπαθούν να "ξεδιπλώσουν" αυτό το τσαλακωμένο χαρτί.
    *   Το **LID (Local Intrinsic Dimensionality)** μας λέει πόσες διαστάσεις χρειάζονται τοπικά για να περιγραφεί το δείγμα. Αν το LID είναι χαμηλό, το δείγμα βρίσκεται πάνω στο "χαρτί". Αν είναι πολύ υψηλό, ίσως είναι θόρυβος που "αιωρείται" έξω από αυτό.
    *   Το **PageRank** στον KNN γράφο μας βοηθά να δούμε ποια δείγματα είναι "γέφυρες" (hubs) πάνω στην πολλαπλότητα, επιτρέποντας στο μοντέλο να καταλάβει τη γεωμετρία των δεδομένων.

### Γ. Adversarial Validation & Weighing (Εχθρική Επαλήθευση)
Για την αντιμετώπιση του **Covariate Shift** (της διαφοράς κατανομής μεταξύ train και test set), εκπαιδεύεται ένας **"Εχθρικός Ταξινομητής"** (RandomForest).
*   **Η Διαδικασία:** Ο ταξινομητής εκπαιδεύεται να διακρίνει αν ένα δείγμα ανήκει στο Training Set ή στο Test Set. Αν ο ταξινομητής επιτυγχάνει υψηλή ακρίβεια (AUC > 0.7), σημαίνει ότι τα δεδομένα εκπαίδευσης διαφέρουν σημαντικά από τα δεδομένα δοκιμής.
*   **Adversarial Weighing:** Χρησιμοποιούμε τις πιθανότητες αυτού του ταξινομητή για να υπολογίσουμε βάρη για κάθε δείγμα εκπαίδευσης. Δείγματα που "μοιάζουν" περισσότερο με το Test Set λαμβάνουν υψηλότερο βάρος. Με αυτόν τον τρόπο, το τελικό μοντέλο δίνει προτεραιότητα στη μάθηση μοτίβων που είναι πραγματικά χρήσιμα για τις τελικές προβλέψεις, αγνοώντας "άσχετα" μοτίβα που υπάρχουν μόνο στο training set.

---

## 2. Αρχιτεκτονικά Συστατικά (The Quantum Ensemble)

Το Epsilon Protocol βασίζεται σε πέντε πυλώνες:

### 1. True TabR (Retrieval-Augmented Tabular Learning)
Αντί για ένα απλό MLP, το **TabR** χρησιμοποιεί έναν μηχανισμό **Cross-Attention**:
*   Για κάθε δείγμα-ερώτημα (query), ανακτά $K$ γείτονες από το training set.
*   Ένα Multi-Head Attention επίπεδο συγκρίνει το query με τους γείτονες, "μαθαίνοντας" ποια ιστορικά παραδείγματα είναι πιο σχετικά.
*   Η πληροφορία αυτή ενσωματώνεται στο feature vector πριν την τελική πρόβλεψη, επιτρέποντας στο μοντέλο να κάνει "lookup" στις γνώσεις του.

### 2. Generative Distribution Classifier (DAE Ensemble)
Πρόκειται για μια Bayesian προσέγγιση. Εκπαιδεύουμε $C$ ξεχωριστούς **Denoising Autoencoders (DAE)**, έναν για κάθε κλάση:
*   Κάθε DAE μαθαίνει την κατανομή $P(x|y=c)$.
*   Κατά την πρόβλεψη, μετράμε το **Reconstruction Energy** (Σφάλμα Ανακατασκευής). Η κλάση που ανακατασκευάζει το δείγμα με το μικρότερο σφάλμα θεωρείται η επικρατέστερη.

### 3. KAN (Kolmogorov-Arnold Networks)
Αντικαθιστούμε τις σταθερές συναρτήσεις ενεργοποίησης (όπως η ReLU) με **μαθαίνουσες συναρτήσεις (splines)** πάνω στις ακμές του δικτύου. Αυτό επιτρέπει στο μοντέλο να προσαρμόζει τη μη-γραμμικότητά του με βάση τα δεδομένα, επιτυγχάνοντας μεγαλύτερη ακρίβεια με λιγότερες παραμέτρους.

### 4. TabM (Mamba Architecture & Selective SSMs)
Χρησιμοποιούμε τη δομή **Mamba**, η οποία βασίζεται στα **Selective State Space Models (SSMs)**. Αυτή η αρχιτεκτονική αποτελεί μια επανάσταση στον χώρο των ακολουθιών (και των tabular δεδομένων ως ακολουθίες χαρακτηριστικών):
*   **Selective Mechanism:** Σε αντίθεση με τα παραδοσιακά SSMs, το Mamba επιτρέπει στις παραμέτρους του μοντέλου να μεταβάλλονται δυναμικά ανάλογα με την είσοδο. Αυτό σημαίνει ότι το μοντέλο μπορεί να "επιλέγει" ποιες πληροφορίες θα αποθηκεύσει στην εσωτερική του κατάσταση (hidden state) και ποιες θα αγνοήσει, επιτυγχάνοντας επιδόσεις επιπέδου Transformer αλλά με γραμμική πολυπλοκότητα $O(N)$.
*   **Πλεονέκτημα στα Tabular:** Στο Part D, το TabM αντιμετωπίζει τη γραμμή δεδομένων ως μια ακολουθία από "γεγονότα" (χαρακτηριστικά). Ο επιλεκτικός μηχανισμός του επιτρέπει να εντοπίζει κρίσιμες αλληλεπιδράσεις μεταξύ στηλών που απέχουν μεταξύ τους, χωρίς να χάνεται στον θόρυβο των ενδιάμεσων τιμών.

### 5. HyperTabPFN (Prior-Data Fitted Networks)
Το **TabPFN** αποτελεί μια μοναδική προσέγγιση "Zero-Shot" ταξινόμησης. Πρόκειται για έναν Transformer που έχει εκπαιδευτεί offline σε εκατομμύρια συνθετικά tabular datasets.
*   **In-Context Learning:** Το μοντέλο δεν "μαθαίνει" με την παραδοσιακή έννοια (updating weights). Αντίθετα, δέχεται ολόκληρο το training set μας ως "πλαίσιο" (context) και προβλέπει την κλάση των νέων δειγμάτων βασιζόμενο σε θεωρητικές κατανομές (Bayesian priors).
*   **Συνεισφορά:** Προσφέρει μια εντελώς διαφορετική οπτική από τα υπόλοιπα μοντέλα, λειτουργώντας ως ένας "αντικειμενικός παρατηρητής" που δεν επηρεάζεται από τοπικά ελάχιστα του δικού μας Loss function.

---

## 3. Στρατηγικές Βελτιστοποίησης & Inference

### Sharpness-Aware Minimization (SAM)
Η εκπαίδευση των μοντέλων (TabR, DAE) δεν ελαχιστοποιεί μόνο το σφάλμα (Loss), αλλά και την **οξύτητα (Sharpness)** της επιφάνειας του Loss:
*   **Τι είναι η Οξύτητα;** Φανταστείτε την επιφάνεια του σφάλματος σαν μια οροσειρά. Ένα "οξύ" ελάχιστο είναι μια στενή χαραμάδα όπου το σφάλμα είναι χαμηλό μόνο σε ένα πολύ συγκεκριμένο σημείο. Ένα "ομαλό" ελάχιστο είναι μια φαρδιά κοιλάδα.
*   **Γιατί μας ενδιαφέρει;** Τα οξέα ελάχιστα οδηγούν σε overfitting. Αν τα δεδομένα του Test set διαφέρουν έστω και ελάχιστα από του Train, το μοντέλο θα βρεθεί εκτός της "χαραμάδας" και θα αποτύχει. Αντίθετα, οι "κοιλάδες" (flat minima) είναι ανθεκτικές και προσφέρουν πολύ καλύτερη γενίκευση (generalization).
*   **Η προσέγγιση SAM:** Ο αλγόριθμος SAM αναζητά ενεργά περιοχές όπου το Loss παραμένει χαμηλό ακόμα και αν προσθέσουμε έναν μικρό θόρυβο στις παραμέτρους, διασφαλίζοντας ότι η λύση που βρήκαμε είναι σταθερή και robust.

### Inference Trick: Energy Descent
Για τα δείγματα με χαμηλή αυτοπεποίθηση (Silver Samples), το σύστημα εφαρμόζει έναν μηχανισμό βελτιστοποίησης κατά την πρόβλεψη:
1.  Εκτελείται Gradient Descent πάνω στην ίδια την **είσοδο $x$**.
2.  Το $x$ τροποποιείται ελαφρώς ώστε να ελαχιστοποιηθεί το Energy σε κάθε κλάση.
3.  Η τελική απόφαση λαμβάνεται από την κλάση που παρουσίασε την καλύτερη "βελτίωση" μετά την ανακατασκευή.

---

## 4. Εξέλιξη από τα GBDTs και SVM (CatBoost, XGBoost, SVM)

Στα αρχικά στάδια του έργου (Part A, B, C), η βάση μας ήταν τα **Gradient Boosted Decision Trees (GBDTs)** και τα **Support Vector Machines (SVM)**. Αν και παραμένουν εξαιρετικά εργαλεία για baseline, η μετάβαση στο Epsilon Protocol έγινε για τους εξής λόγους:
1.  **Differentiability:** Τα δέντρα και τα SVM δεν είναι πλήρως διαφορίσιμα με τον τρόπο που απαιτούν οι σύγχρονες τεχνικές. Αυτό καθιστά αδύνατη την εφαρμογή προηγμένων μεθόδων όπως το **SAM (Sharpness-Aware Minimization)** ή η άμεση βελτιστοποίηση εισόδου (Energy Descent).
2.  **Kernel vs Attention:** Ενώ το SVM βασίζεται σε στατικούς Kernels για να βρει διαχωριστικές επιφάνειες, το **TabR** χρησιμοποιεί δυναμικό Attention για να "μάθει" ποιες σχέσεις μεταξύ δειγμάτων είναι σημαντικές. Αυτό προσφέρει πολύ μεγαλύτερη προσαρμοστικότητα σε σύνθετα datasets.
3.  **Hybrid Approach:** Το TabPFN που χρησιμοποιούμε στο τέλος λειτουργεί ως η "γέφυρα", συνδυάζοντας την ταχύτητα των κλασικών μεθόδων με την ευφυΐα των Transformers.

---

## 5. Τελική Σύνθεση (Consensus)

Η τελική πρόβλεψη προκύπτει από το **Soft-Voting Ensemble** όλων των παραπάνω μοντέλων. 
*   **Soft vs Hard Voting:** Αντί για μια απλή ψηφοφορία πλειοψηφίας, το σύστημα υπολογίζει τον μέσο όρο των πιθανοτήτων (probabilities) από κάθε αρχιτεκτονική. Αυτό επιτρέπει σε μοντέλα με υψηλή αυτοπεποίθηση να επηρεάζουν περισσότερο το τελικό αποτέλεσμα, διορθώνοντας τυχόν αστοχίες των υπολοίπων.
*   **Stability:** Με την εξισορρόπηση μεταξύ discriminative (TabR, KAN, TabPFN) και generative (DAE) λογικής, το Πρωτόκολλο Epsilon επιτυγχάνει το θεωρητικό όριο πληροφορίας που μπορεί να εξαχθεί από το dataset, διατηρώντας παράλληλα εξαιρετική ανθεκτικότητα σε θόρυβο και covariate shift.
