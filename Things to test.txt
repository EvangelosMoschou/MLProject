# Detailed Prompt: Achieving State-of-the-Art (SOTA) Tabular Classification

## Context
You are tasked with building a **state-of-the-art classification model** for a tabular dataset with the following characteristics:
- **Size**: Approximately 7,000-9,000 training samples.
- **Features**: 224 numerical/categorical features.
- **Classes**: 5-class classification problem.
- **Goal**: Maximize classification accuracy on a held-out test set of ~7,000 samples.

Standard techniques like tuned XGBoost or LightGBM with basic stacking have already been tried and provide "good" results. The objective now is to push beyond "good" to achieve **SOTA performance** using advanced techniques employed by Kaggle Grandmasters.

---

## Advanced Techniques to Implement

### 1. TabPFN (The Game Changer)
**What is it?**
TabPFN is a Transformer model pre-trained on millions of synthetic datasets. Unlike traditional models, it does not "train" on your data in the conventional sense. Instead, it uses **In-Context Learning** (similar to how GPT-4 processes text) to predict probabilities instantly by treating your training data as context.

**Why use it?**
- On datasets with fewer than 10,000 rows (like this one), TabPFN frequently **outperforms tuned XGBoost/LightGBM out of the box**.
- It is considered the **current SOTA for small tabular data**.
- It operates on a completely different mathematical principle (Transformers/Priors) than tree-based models (XGBoost, Random Forest) or geometric models (SVM). This **diversity is invaluable for stacking**.

**Implementation Notes:**
- Use the `tabpfn` Python package.
- TabPFN works best on raw features. Do NOT apply heavy feature engineering before feeding data to TabPFN.
- TabPFN has limitations on the number of features and training samples it can handle in a single call; you may need to batch or subsample.

---

### 2. Denoising Autoencoder (DAE) Features (Deep Feature Synthesis)
**What is it?**
A Denoising Autoencoder is a neural network trained to reconstruct "clean" versions of your data from artificially "corrupted" (noisy) inputs. The key insight is that you **discard the output** and instead use the **bottleneck layer (hidden representation)** as new features.

**Why use it?**
- The DAE learns a **compressed, non-linear representation** of your data's underlying manifold.
- By concatenating these "deep features" (`X_deep`) to your original features (`X`), you give models like XGBoost/CatBoost "hints" about the non-linear structure of the data that they couldn't discover easily on their own.
- This technique captures relationships that tree-based models inherently miss.

**Implementation Notes:**
- Train a simple autoencoder (e.g., using PyTorch or Keras) with a bottleneck layer.
- Add noise to inputs during training (e.g., Gaussian noise, masking).
- Extract the bottleneck layer's output as new features for downstream models.

---

### 3. MixUp Augmentation
**What is it?**
MixUp is a data augmentation technique where, instead of adding random noise (which can be harmful for tabular data), you take two random rows (`Row_A`, `Row_B`) and **blend them** using a weighted average:
```
NewRow = λ × Row_A + (1 - λ) × Row_B
```
where `λ` is typically sampled from a Beta distribution (e.g., `Beta(0.2, 0.2)`). You also blend the labels:
```
NewLabel = λ × Label_A + (1 - λ) × Label_B
```
This creates "soft labels" during training.

**Why use it?**
- It forces the model to learn **linear transitions between classes**, effectively smoothing the decision boundary.
- It acts as a regularizer, reducing overfitting.
- It is particularly effective for neural network-based models.

**Implementation Notes:**
- Apply MixUp during training, not at inference.
- Works well with neural networks and can also benefit gradient boosting models if used to augment training data.

---

### 4. Adversarial Validation
**What is it?**
Adversarial Validation is a technique to detect **distribution shift** between your training and test sets. You train a classifier (e.g., LightGBM) to distinguish between Train and Test data (ignoring labels).

**Why use it?**
- If this classifier achieves **high accuracy** (e.g., >70% AUC), it means your Train and Test distributions are noticeably different.
- You must identify and potentially **drop the features** that make them different, or your model will fail to generalize to the test set.
- It can also be used to create a more representative validation set by selecting training samples that "look like" test samples.

**Implementation Notes:**
- Create a binary classification task: Train samples labeled 0, Test samples labeled 1.
- Train a simple model and check feature importances.
- Features with high importance in this task are the ones causing distribution shift—consider removing or transforming them.

---

## Recommended Architecture: Meta-Feature Stacking

The key to achieving SOTA is to **combine these techniques intelligently** using a stacking approach:

1.  **Layer 1 (Base Models):**
    -   **TabPFN**: Feed it **raw features only** (no DAE features). TabPFN excels on raw data.
    -   **XGBoost / CatBoost**: Feed them **Raw Features + DAE Features** (`X_concat = [X, X_deep]`). This gives tree models access to the non-linear relationships captured by the autoencoder.
    -   **CatBoost** is preferred over SVM because it naturally handles mixed data types and distribution shifts better.

2.  **Layer 2 (Meta-Learner):**
    -   Train a meta-learner (e.g., Logistic Regression or a simple XGBoost) on the out-of-fold predictions from Layer 1.

3.  **Data Routing:**
    -   Ensure TabPFN sees **only raw features**.
    -   Ensure XGBoost/CatBoost see **raw features + deep features**.
    -   This specific routing of data is an advanced technique that maximizes diversity among base learners.

---

## Additional Suggestions (From Your AI Assistant)

### 5. Target Encoding with Smoothing
For categorical features, consider **target encoding** with smoothing (also called mean encoding). This replaces categorical values with the mean of the target variable for that category, smoothed to prevent overfitting on rare categories.

### 6. Feature Selection via Permutation Importance
After training a preliminary model, use **permutation importance** to identify features that contribute little or even harm performance. Removing noisy features can improve generalization.

### 7. Pseudo-Labeling
If your initial model is confident on a subset of the test set, you can use **pseudo-labeling**:
-   Predict on the test set.
-   Select samples where the model has high confidence (e.g., predicted probability > 0.95).
-   Add these samples (with their predicted labels) to the training set.
-   Retrain the model.
This is a form of semi-supervised learning that can boost performance when labeled data is limited.

### 8. Hyperparameter Tuning with Optuna
Use **Optuna** or a similar framework for Bayesian hyperparameter optimization. This is more efficient than grid search and can find better hyperparameters faster.

### 9. Ensemble Calibration
After stacking, apply **probability calibration** (e.g., Platt Scaling or Isotonic Regression) to ensure the predicted probabilities are well-calibrated. This can improve log-loss and is especially important if you're using soft voting.

### 10. Cross-Validation Strategy
Use **Stratified K-Fold** cross-validation (e.g., 5 or 10 folds) to ensure each fold has a representative distribution of classes. This is crucial for multi-class problems with potential class imbalance.

---

## Summary of Techniques

| Technique               | Purpose                                      | Model Benefiting Most |
|-------------------------|----------------------------------------------|-----------------------|
| TabPFN                  | SOTA for small tabular data                  | Standalone / Stacking |
| DAE Features            | Non-linear feature synthesis                 | XGBoost, CatBoost     |
| MixUp Augmentation      | Decision boundary smoothing, regularization  | Neural Networks       |
| Adversarial Validation  | Detect and fix distribution shift            | All models            |
| Target Encoding         | Better categorical feature representation    | Tree-based models     |
| Permutation Importance  | Feature selection                            | All models            |
| Pseudo-Labeling         | Semi-supervised learning boost               | High-confidence models|
| Optuna                  | Efficient hyperparameter tuning              | All models            |
| Probability Calibration | Improve predicted probability quality        | Ensemble outputs      |
| Stratified K-Fold       | Robust cross-validation                      | All models            |

---

## Expected Outcome
By implementing these techniques, you should be able to push your model from "good" to **state-of-the-art** performance, potentially achieving the top spots in any competition on this dataset.