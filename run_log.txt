>>> INITIATING SIGMA-OMEGA GRANDMASTER PROTOCOL <<<
Loading data from: Datasets/datasetTV.csv and Datasets/datasetTest.csv

[ADV DIAGNOSTIC] Checking train/test distribution shift...
  âœ“ AUC = 0.522 (No significant distribution shift detected)

[RAZOR] Computing per-model CV-averaged feature importance...
  [CatBoost] Computing 5-fold CV importance...
  [XGBoost] Computing 5-fold CV importance...
  > CatBoost mask: 201/224 features kept
  > XGBoost mask: 201/224 features kept

>>> SEQUENCE START: SEED 42 <<<
  [VIEW] raw
  > Stacking meta-learner (OOF -> meta)...
  [STACKING] Cross-Validation (5 folds) | View: raw | Models: 3
   [DIFFUSION] Augmenting fold training data...
   [DIFFUSION] Synthesizing 200 samples/class...
Traceback (most recent call last):
  File "/home/vaggelis/Desktop/AUTH/Machine Learning/MLProject/PartD/solution_omega.py", line 15, in <module>
    print(f"DEBUG: Starting script {sys.argv[0]} from {os.getcwd()}", flush=True)
        ^^^^^^
  File "/home/vaggelis/Desktop/AUTH/Machine Learning/MLProject/PartD/sigma_omega/main.py", line 224, in main
    view_probs_total += predict_probs_for_view(
                        ~~~~~~~~~~~~~~~~~~~~~~^
        view, seed, X_razor, X_test_razor, y_enc, num_classes,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        X_train_raw=X, X_test_raw=X_test,  # Raw data for TabPFN
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        razor_masks=razor_masks,  # Per-model masks
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/vaggelis/Desktop/AUTH/Machine Learning/MLProject/PartD/sigma_omega/pipeline.py", line 147, in predict_probs_for_view
    p = fit_predict_stacking(
        names_models=names_models,
    ...<12 lines>...
        X_test_raw=X_test_raw,     # Raw data for TabPFN
    )
  File "/home/vaggelis/Desktop/AUTH/Machine Learning/MLProject/PartD/sigma_omega/stacking.py", line 164, in fit_predict_stacking
    model.fit(X_train_final, y_train_final, sample_weight=w_train_final)
    ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaggelis/miniconda3/lib/python3.13/site-packages/xgboost/core.py", line 774, in inner_f
    return func(**kwargs)
  File "/home/vaggelis/miniconda3/lib/python3.13/site-packages/xgboost/sklearn.py", line 1806, in fit
    self._Booster = train(
                    ~~~~~^
        params,
        ^^^^^^^
    ...<9 lines>...
        callbacks=self.callbacks,
        ^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/vaggelis/miniconda3/lib/python3.13/site-packages/xgboost/core.py", line 774, in inner_f
    return func(**kwargs)
  File "/home/vaggelis/miniconda3/lib/python3.13/site-packages/xgboost/training.py", line 199, in train
    bst.update(dtrain, iteration=i, fobj=obj)
    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaggelis/miniconda3/lib/python3.13/site-packages/xgboost/core.py", line 2434, in update
    _LIB.XGBoosterUpdateOneIter(
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        self.handle, ctypes.c_int(iteration), dtrain.handle
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
KeyboardInterrupt
