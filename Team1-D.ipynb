{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbfdd82f",
   "metadata": {},
   "source": [
    "# Machine Learning Project - Part D\n",
    "**Team 1**\n",
    "* Name: [Your Name Here]\n",
    "* AEM: [Your AEM Here]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde9a654",
   "metadata": {},
   "source": [
    "## Part D: Classification Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78474ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- GPU Setup (Only for XGBoost now) ---\n",
    "try:\n",
    "    import cuml\n",
    "    print(\"✅ cuML imported (Only using for XGBoost acceleration).\")\n",
    "    USE_GPU = True\n",
    "except ImportError:\n",
    "    print(\"⚠️ cuML not found. Using CPU completely.\")\n",
    "    USE_GPU = False\n",
    "\n",
    "DATA_PATH_TRAIN = '../Datasets/datasetTV.csv'\n",
    "DATA_PATH_TEST = '../Datasets/datasetTest.csv'\n",
    "MODEL_FILE = 'best_model_stacking_fast_cpu.pkl'\n",
    "OUTPUT_FILE = 'labels1.npy'\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load and return training features/labels and test features.\"\"\"\n",
    "    if not os.path.exists(DATA_PATH_TRAIN):\n",
    "        print(\"⚠️ Path warning: adjusting paths...\")\n",
    "        train_path = 'Datasets/datasetTV.csv'\n",
    "        test_path = 'Datasets/datasetTest.csv'\n",
    "    else:\n",
    "        train_path = DATA_PATH_TRAIN\n",
    "        test_path = DATA_PATH_TEST\n",
    "        \n",
    "    train_df = pd.read_csv(train_path, header=None)\n",
    "    test_df = pd.read_csv(test_path, header=None)\n",
    "    X = train_df.iloc[:, :-1].values\n",
    "    y = train_df.iloc[:, -1].values\n",
    "    X_test = test_df.values\n",
    "    \n",
    "    print(f\"Train Shape: {X.shape}, Test Shape: {X_test.shape}\")\n",
    "    return X, y, X_test\n",
    "\n",
    "def augment_data_gaussian(X, y, noise_level=0.05):\n",
    "    \"\"\"Augment data by adding Gaussian noise.\"\"\"\n",
    "    print(f\"Augmenting data (noise={noise_level})...\")\n",
    "    noise = np.random.normal(0, noise_level, X.shape)\n",
    "    X_aug = np.vstack((X, X + noise))\n",
    "    y_aug = np.hstack((y, y))\n",
    "    print(f\"New Training Shape: {X_aug.shape}\")\n",
    "    return X_aug, y_aug\n",
    "\n",
    "def get_svm_model():\n",
    "    \"\"\"Returns SVM Pipeline (Optimized CPU).\"\"\"\n",
    "    # Optimized: probability=False removes internal 5-fold CV\n",
    "    # StackingSVM works fine with decision_function output\n",
    "    clf = SVC(C=10, gamma='scale', kernel='rbf', probability=False, random_state=42)\n",
    "    pca = PCA(n_components=100)\n",
    "    return Pipeline([\n",
    "        ('scaler', StandardScaler()), \n",
    "        ('pca', pca), \n",
    "        ('svm', clf)\n",
    "    ])\n",
    "\n",
    "def get_rf_model():\n",
    "    \"\"\"Returns Random Forest Pipeline (CPU).\"\"\"\n",
    "    return Pipeline([\n",
    "        ('scaler', StandardScaler()), \n",
    "        # n_jobs=-1 is usually safe for RF on CPU\n",
    "        ('rf', RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=42))\n",
    "    ])\n",
    "\n",
    "def get_xgb_model():\n",
    "    \"\"\"Returns XGBoost Pipeline (GPU if available).\"\"\"\n",
    "    params = {\n",
    "        'n_estimators': 300, \n",
    "        'learning_rate': 0.05, \n",
    "        'max_depth': 6, \n",
    "        'n_jobs': -1, \n",
    "        'random_state': 42, \n",
    "        'eval_metric': 'mlogloss',\n",
    "        'verbosity': 0\n",
    "    }\n",
    "    if USE_GPU: \n",
    "        params.update({'device': 'cuda', 'tree_method': 'hist'})\n",
    "        \n",
    "    return Pipeline([\n",
    "        ('scaler', StandardScaler()), \n",
    "        ('xgb', xgb.XGBClassifier(**params))\n",
    "    ])\n",
    "\n",
    "def get_mlp_model():\n",
    "    \"\"\"Returns MLP Pipeline (Deep Architecture).\"\"\"\n",
    "    # Reduced max_iter slightly for speed, early_stopping handles it\n",
    "    return Pipeline([\n",
    "        ('scaler', StandardScaler()), \n",
    "        ('mlp', MLPClassifier(hidden_layer_sizes=(512, 256), \n",
    "                            max_iter=300, \n",
    "                            early_stopping=True,\n",
    "                            validation_fraction=0.1,\n",
    "                            random_state=42))\n",
    "    ])\n",
    "\n",
    "def get_stacking_ensemble():\n",
    "    \"\"\"Constructs the Stacking Classifier.\"\"\"\n",
    "    estimators = [\n",
    "        ('svm', get_svm_model()), \n",
    "        ('rf', get_rf_model()), \n",
    "        ('xgb', get_xgb_model()), \n",
    "        ('mlp', get_mlp_model())\n",
    "    ]\n",
    "    \n",
    "    # Optimizations:\n",
    "    # cv=3 instead of 5 (Reduces fitting runs by 2)\n",
    "    # n_jobs=1 for safety\n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=estimators,\n",
    "        final_estimator=LogisticRegression(),\n",
    "        cv=3, \n",
    "        n_jobs=1 \n",
    "    )\n",
    "    return stacking_clf\n",
    "\n",
    "def main():\n",
    "    print(\"--- Part D: Fast Stacking Ensemble (Optimized) ---\")\n",
    "    \n",
    "    # 1. Load & Encode\n",
    "    X, y, X_test_submit = load_data()\n",
    "    le = LabelEncoder()\n",
    "    y_enc = le.fit_transform(y)\n",
    "    \n",
    "    # 2. Initial Augmentation\n",
    "    X_train_full, y_train_full = augment_data_gaussian(X, y_enc, noise_level=0.05)\n",
    "    \n",
    "    # 3. Initialize Model\n",
    "    model = get_stacking_ensemble()\n",
    "    \n",
    "    # 4. First Training Pass\n",
    "    print(\"\\n[Phase 1] Training Initial Stacking Ensemble...\")\n",
    "    model.fit(X_train_full, y_train_full)\n",
    "    \n",
    "    # 5. Pseudo-Labeling Strategy\n",
    "    print(\"\\n[Phase 2] Pseudo-Labeling High Confidence Predictions...\")\n",
    "    \n",
    "    # Predict probabilities on Test Set\n",
    "    probs = model.predict_proba(X_test_submit)\n",
    "    max_probs = np.max(probs, axis=1)\n",
    "    preds = np.argmax(probs, axis=1)\n",
    "    \n",
    "    CONFIDENCE_THRESHOLD = 0.90\n",
    "    high_conf_idx = np.where(max_probs >= CONFIDENCE_THRESHOLD)[0]\n",
    "    \n",
    "    print(f\"Stats: Found {len(high_conf_idx)}/{len(X_test_submit)} samples with confidence >= {CONFIDENCE_THRESHOLD}\")\n",
    "    \n",
    "    if len(high_conf_idx) > 0:\n",
    "        # Create Pseudo-Labeled Dataset\n",
    "        X_pseudo = X_test_submit[high_conf_idx]\n",
    "        y_pseudo = preds[high_conf_idx]\n",
    "        \n",
    "        # Combine\n",
    "        X_final = np.vstack((X_train_full, X_pseudo))\n",
    "        y_final = np.hstack((y_train_full, y_pseudo))\n",
    "        \n",
    "        print(f\"Retraining on Expanded Dataset: {X_final.shape}\")\n",
    "        \n",
    "        # Retrain\n",
    "        model.fit(X_final, y_final)\n",
    "    else:\n",
    "        print(\"Not enough high-confidence samples. Skipping retraining.\")\n",
    "        \n",
    "    # 6. Final Prediction\n",
    "    print(\"\\n[Phase 3] Generating Final Predictions...\")\n",
    "    final_preds_enc = model.predict(X_test_submit)\n",
    "    final_preds = le.inverse_transform(final_preds_enc)\n",
    "    \n",
    "    # Save\n",
    "    np.save(OUTPUT_FILE, final_preds.astype(int))\n",
    "    print(f\"Predictions saved to {OUTPUT_FILE}\")\n",
    "    \n",
    "    # Save Model\n",
    "    try:\n",
    "        joblib.dump(model, MODEL_FILE)\n",
    "        print(f\"Model saved to {MODEL_FILE}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Warning: Could not save model: {e}\")\n",
    "\n",
    "    # Verify\n",
    "    print(\"Verifying file...\")\n",
    "    loaded = np.load(OUTPUT_FILE)\n",
    "    if loaded.shape[0] == 6955:\n",
    "        print(\"✅ Verification SUCCESS\")\n",
    "    else:\n",
    "        print(\"❌ Verification FAILED\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
