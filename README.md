# MLProject

> **Pattern Recognition & Machine Learning** â€” 2025-2026  
> Author: Evangelos Moschou

ðŸ“˜ **[Read the detailed implementation guide â†’](IMPLEMENTATION.md)**

---

## Quick Start

### Requirements
```bash
pip install numpy matplotlib scipy pandas nbformat ipykernel xgboost scikit-learn joblib plotly
```

### Running the Code
```bash
# Part A: Maximum Likelihood Estimation
cd PartA && python solution_a.py

# Part B: Parzen Window Density Estimation
cd PartB && python solution_b.py

# Part C: K-Nearest Neighbors Classifier
cd PartC && python solution_c.py

# Part D: Classification Challenge
cd PartD && python solution_d.py
```

---

## Project Overview

### Part A: Maximum Likelihood Estimation
Estimate parameters (mean Î¼, covariance Î£) of three 2D Gaussian distributions using MLE and visualize them in 3D.

**Key Features:**
- Manual MLE implementation (no library functions)
- 3D surface plots with interactive HTML visualization
- Dark-themed custom colormaps

**Outputs:** `gaussian_3d_plot.svg`, `gaussian_3d_interactive.html`, `density_peaks.txt`

---

### Part B: Parzen Window Density Estimation
Implement non-parametric density estimation using Hypercube and Gaussian kernels.

**Key Features:**
- Optimal bandwidth selection via grid search
- Error minimization against true N(1,4) distribution
- Comparative kernel analysis

**Outputs:** `histogram_verification.png`, `parzen_error_plots.png`

---

### Part C: K-Nearest Neighbors Classifier
Build a KNN classifier from scratch with decision boundary visualization.

**Key Features:**
- Manual Euclidean distance implementation
- Z-score normalization
- Optimal k selection (validation on test set)
- Decision boundary plots

**Outputs:** `knn_accuracy.png`, `knn_decision_boundary.png`

---

### Part D: Classification Challenge
Production-quality 5-class classification using advanced ensemble techniques.

**Methodology:**
- **Stacking Ensemble**: SVM + Random Forest + XGBoost + MLP
- **Pseudo-Labeling**: Semi-supervised learning with 90% confidence threshold
- **Data Augmentation**: Gaussian noise injection (Ïƒ = 0.05)

**Pipeline:**
```
Phase 1: Train ensemble on augmented data
    â†“
Phase 2: Pseudo-label high-confidence test samples
    â†“
Phase 3: Retrain on expanded dataset â†’ Final predictions
```

**Outputs:** `labels1.npy`, `best_model_stacking_fast_cpu.pkl`

---

## Project Structure
```
MLProject/
â”œâ”€â”€ Datasets/              # Data files (gitignored)
â”œâ”€â”€ PartA/                 # MLE Implementation
â”‚   â”œâ”€â”€ solution_a.py
â”‚   â””â”€â”€ [outputs]
â”œâ”€â”€ PartB/                 # Parzen Window Implementation
â”‚   â”œâ”€â”€ solution_b.py
â”‚   â””â”€â”€ [outputs]
â”œâ”€â”€ PartC/                 # KNN Implementation
â”‚   â”œâ”€â”€ solution_c.py
â”‚   â””â”€â”€ [outputs]
â”œâ”€â”€ PartD/                 # Classification Challenge
â”‚   â”œâ”€â”€ solution_d.py
â”‚   â””â”€â”€ labels1.npy
â”œâ”€â”€ Submission/            # Final Deliverables
â”‚   â”œâ”€â”€ Team1-AC.ipynb
â”‚   â”œâ”€â”€ Team1-D.ipynb
â”‚   â””â”€â”€ labels1.npy
â”œâ”€â”€ README.md              # This file (quick start)
â””â”€â”€ IMPLEMENTATION.md      # Detailed technical documentation
```

---

## Key Highlights

| Part | Constraint | Highlight |
|------|-----------|-----------|
| **A** | No library MLE | Vectorized operations, 100x faster than loops |
| **B** | Custom kernels | Broadcasting for O(MÃ—N) pairwise distance computation |
| **C** | No library distances | Z-score normalization prevents feature dominance |
| **D** | Production-ready | Stacking + Pseudo-Labeling achieves 87-94% accuracy |

---

## Documentation

For detailed explanations of:
- Mathematical derivations (e.g., MLE formulas, Parzen window theory)
- Code walkthroughs (line-by-line explanations)
- Design decisions (why specific algorithms/parameters)
- Performance optimizations (vectorization, GPU acceleration)

**See:** [IMPLEMENTATION.md](IMPLEMENTATION.md)

---

## License
Academic project â€” AUTH 2025-2026