\documentclass[aspectratio=169,10pt]{beamer}
\usepackage{fontspec}
\usepackage{unicode-math}
\usepackage{polyglossia}
\setmainlanguage{greek}
\setotherlanguage{english}

% Fonts
\setmainfont{Liberation Serif}
\setsansfont{Liberation Sans}
\setmonofont{Liberation Mono}

% Theme
\usetheme{Madrid}
\usecolortheme{default}

% Packages
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{xcolor}

% Colors
\definecolor{authblue}{RGB}{0,51,102}
\definecolor{codegreen}{RGB}{0,128,0}
\definecolor{codegray}{RGB}{128,128,128}
\definecolor{codepurple}{RGB}{153,0,153}
\definecolor{backcolour}{RGB}{245,245,245}

\setbeamercolor{structure}{fg=authblue}
\setbeamercolor{title}{fg=white,bg=authblue}
\setbeamercolor{frametitle}{fg=white,bg=authblue}

% Code listing style
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

% Title page info
\title{Εργασία: Αναγνώριση Προτύπων \& Μηχανική Μάθηση}
\subtitle{Μέρη Α, Β, Γ, Δ}
\author{Ευάγγελος Μόσχου\\ΑΕΜ: 10986}
\institute{Αριστοτέλειο Πανεπιστήμιο Θεσσαλονίκης\\Τμήμα Ηλεκτρολόγων Μηχανικών και Μηχανικών Υπολογιστών}
\date{Ιανουάριος 2026}

\begin{document}

% ============================================================================
% TITLE SLIDE
% ============================================================================
\begin{frame}
\titlepage
\end{frame}

% ============================================================================
% TABLE OF CONTENTS
% ============================================================================
\begin{frame}{Περιεχόμενα}
\tableofcontents
\end{frame}

% ============================================================================
% PART A (10 slides)
% ============================================================================
\section{Μέρος Α: Εκτίμηση Παραμέτρων με Μέγιστη Πιθανοφάνεια}

\begin{frame}{Μέρος Α: Περιγραφή Προβλήματος}
\begin{block}{Στόχος}
Εκτίμηση παραμέτρων τριών κανονικών κατανομών χρησιμοποιώντας την τεχνική της Μέγιστης Πιθανοφάνειας (Maximum Likelihood Estimation).
\end{block}

\begin{itemize}
    \item \textbf{Σύνολο Δεδομένων:} dataset1.csv
    \item \textbf{Δείγματα:} 300 δείγματα (100 ανά κλάση)
    \item \textbf{Διαστάσεις:} 2 χαρακτηριστικά (features)
    \item \textbf{Κλάσεις:} 3 (0, 1, 2)
\end{itemize}

\vfill
\begin{alertblock}{Περιορισμός}
Υλοποίηση χωρίς χρήση έτοιμων συναρτήσεων βιβλιοθηκών για MLE.
\end{alertblock}
\end{frame}

\begin{frame}{Θεωρητικό Υπόβαθρο}
\begin{block}{Πολυδιάστατη Κανονική Κατανομή}
Για κάθε κλάση $c$, η πυκνότητα πιθανότητας είναι:
\[
p(\mathbf{x}|\mu_c, \Sigma_c) = \frac{1}{(2\pi)^{d/2}|\Sigma_c|^{1/2}} \exp\left(-\frac{1}{2}(\mathbf{x}-\mu_c)^T\Sigma_c^{-1}(\mathbf{x}-\mu_c)\right)
\]
\end{block}

\begin{block}{Εκτιμητές Μέγιστης Πιθανοφάνειας}
Για $N_c$ δείγματα της κλάσης $c$:
\begin{align*}
\hat{\mu}_c &= \frac{1}{N_c}\sum_{i=1}^{N_c} \mathbf{x}_i^{(c)}\\
\hat{\Sigma}_c &= \frac{1}{N_c}\sum_{i=1}^{N_c} (\mathbf{x}_i^{(c)} - \hat{\mu}_c)(\mathbf{x}_i^{(c)} - \hat{\mu}_c)^T
\end{align*}
\end{block}
\end{frame}

\begin{frame}{Υλοποίηση}
\begin{itemize}
    \item \textbf{Βήμα 1:} Διαχωρισμός δεδομένων ανά κλάση
    \item \textbf{Βήμα 2:} Υπολογισμός μέσου όρου $\hat{\mu}_c$ για κάθε κλάση
    \item \textbf{Βήμα 3:} Υπολογισμός πίνακα συνδιακύμανσης $\hat{\Sigma}_c$
    \item \textbf{Βήμα 4:} Οπτικοποίηση με 3D plot
\end{itemize}

\vfill
\begin{exampleblock}{Τεχνικές Λεπτομέρειες}
\begin{itemize}
    \item Χρήση NumPy για πράξεις πινάκων
    \item Matplotlib για 3D visualization
    \item Meshgrid για δημιουργία επιφανειών
\end{itemize}
\end{exampleblock}
\end{frame}

\begin{frame}{Αποτελέσματα - Εκτιμημένες Παράμετροι}
\small
\textbf{Κλάση 0} (N=99):
\[
\hat{\mu}_0 = \begin{pmatrix} 29.25 \\ 16.87 \end{pmatrix}, \quad
\hat{\Sigma}_0 = \begin{pmatrix} 47.76 & 23.27 \\ 23.27 & 49.57 \end{pmatrix}
\]

\textbf{Κλάση 1} (N=100):
\[
\hat{\mu}_1 = \begin{pmatrix} 40.20 \\ 34.28 \end{pmatrix}, \quad
\hat{\Sigma}_1 = \begin{pmatrix} 9.52 & 11.61 \\ 11.61 & 20.31 \end{pmatrix}
\]

\textbf{Κλάση 2} (N=100):
\[
\hat{\mu}_2 = \begin{pmatrix} 27.55 \\ 34.79 \end{pmatrix}, \quad
\hat{\Sigma}_2 = \begin{pmatrix} 14.11 & 11.89 \\ 11.89 & 25.54 \end{pmatrix}
\]
\end{frame}

\begin{frame}{3D Οπτικοποίηση}
\begin{center}
\includegraphics[width=0.75\textwidth,height=0.7\textheight,keepaspectratio]{plots/partA_3d_gaussians.png}
\end{center}
\vspace{-0.2cm}
\footnotesize
Οι τρεις εκτιμημένες κανονικές κατανομές οπτικοποιούνται ως τρισδιάστατες επιφάνειες πυκνότητας πιθανότητας.
\end{frame}

% ============================================================================
% PART B (10 slides)
% ============================================================================
\section{Μέρος Β: Εκτίμηση Συνάρτησης Πυκνότητας με Παράθυρα Parzen}

\begin{frame}{Μέρος Β: Περιγραφή Προβλήματος}
\begin{block}{Στόχος}
Εκτίμηση της συνάρτησης πυκνότητας πιθανότητας (PDF) χρησιμοποιώντας τη μέθοδο παραθύρων Parzen.
\end{block}

\begin{itemize}
    \item \textbf{Σύνολο Δεδομένων:} dataset2.csv
    \item \textbf{Δείγματα:} 200 μονοδιάστατα
    \item \textbf{Υπόθεση:} Δεδομένα από $\mathcal{N}(1, 4)$
    \item \textbf{Kernels:} Υπερκύβος και Gaussian
\end{itemize}

\vfill
\begin{alertblock}{Ζητούμενο}
Εύρεση της βέλτιστης τιμής $h$ (bandwidth) για κάθε kernel.
\end{alertblock}
\end{frame}

\begin{frame}{Θεωρητικό Υπόβαθρο - Parzen Windows}
\begin{block}{Εκτιμητής Parzen}
\[
\hat{p}(x) = \frac{1}{n \cdot h} \sum_{i=1}^{n} K\left(\frac{x - x_i}{h}\right)
\]
όπου $K$ η kernel function και $h$ το bandwidth.
\end{block}

\begin{columns}[c]
\column{0.5\textwidth}
\textbf{Υπερκύβος (Hypercube):}
\[
K(u) = \begin{cases}
1 & |u| \leq \frac{1}{2}\\
0 & \text{αλλιώς}
\end{cases}
\]

\column{0.5\textwidth}
\textbf{Gaussian:}
\[
K(u) = \frac{1}{\sqrt{2\pi}} e^{-\frac{u^2}{2}}
\]
\end{columns}
\end{frame}

\begin{frame}{Μεθοδολογία Βελτιστοποίησης}
\begin{enumerate}
    \item \textbf{Εύρος $h$:} [0.1, 10] με βήμα 0.1
    \item \textbf{Για κάθε $h$:}
    \begin{itemize}
        \item Υπολογισμός προβλεπόμενης πιθανοφάνειας $\hat{p}(x_i)$
        \item Υπολογισμός πραγματικής πιθανοφάνειας από $\mathcal{N}(1, 4)$
        \item Υπολογισμός MSE: $\text{MSE} = \frac{1}{n}\sum_{i=1}^{n}(p(x_i) - \hat{p}(x_i))^2$
    \end{itemize}
    \item \textbf{Επιλογή:} $h^* = \arg\min_h \text{MSE}(h)$
\end{enumerate}
\end{frame}

\begin{frame}{Αποτελέσματα - Βέλτιστο Bandwidth}
\begin{columns}[c]
\column{0.5\textwidth}
\begin{block}{Hypercube Kernel}
\begin{itemize}
    \item $h^*_{hypercube} = 2.80$
    \item MSE = $1.091 \times 10^{-3}$
\end{itemize}
\end{block}

\column{0.5\textwidth}
\begin{block}{Gaussian Kernel}
\begin{itemize}
    \item $h^*_{gaussian} = 0.80$
    \item MSE = $1.131 \times 10^{-3}$
\end{itemize}
\end{block}
\end{columns}

\vfill
\begin{exampleblock}{Σύγκριση}
Το Hypercube kernel έχει λίγο μικρότερο MSE αλλά το Gaussian παρέχει ομαλότερη εκτίμηση.
\end{exampleblock}
\end{frame}

\begin{frame}{Plots: Σφάλμα vs Bandwidth}
\begin{columns}[c]
\column{0.5\textwidth}
\includegraphics[width=\textwidth]{plots/partB_hypercube_mse.png}
\footnotesize Hypercube Kernel

\column{0.5\textwidth}
\includegraphics[width=\textwidth]{plots/partB_gaussian_mse.png}
\footnotesize Gaussian Kernel
\end{columns}
\vspace{0.2cm}
\footnotesize
Η βέλτιστη τιμή $h$ είναι στο ελάχιστο σημείο MSE (κόκκινη διακεκομμένη γραμμή).
\end{frame}

% ============================================================================
% PART C (10 slides)
% ============================================================================
\section{Μέρος Γ: k-Nearest Neighbors Classifier}

\begin{frame}{Μέρος Γ: Περιγραφή Προβλήματος}
\begin{block}{Στόχος}
Υλοποίηση k-Nearest Neighbors (KNN) classifier από την αρχή.
\end{block}

\begin{itemize}
    \item \textbf{Training Set:} dataset3.csv (50 δείγματα, 2D)
    \item \textbf{Test Set:} testset.csv (50 δείγματα, 2D)
    \item \textbf{Κλάσεις:} 2 (0, 1)
    \item \textbf{Εύρος k:} [1, 30]
\end{itemize}

\vfill
\begin{alertblock}{Υλοποίηση}
Χωρίς χρήση έτοιμων βιβλιοθηκών KNN (π.χ., sklearn.neighbors).
\end{alertblock}
\end{frame}

\begin{frame}{Θεωρητικό Υπόβαθρο - KNN}
\begin{block}{Αλγόριθμος KNN}
Για ένα test δείγμα $\mathbf{x}$:
\begin{enumerate}
    \item Υπολογισμός απόστασης από όλα τα training δείγματα
    \item Επιλογή των $k$ πλησιέστερων γειτόνων
    \item Υπολογισμός πιθανότητας ανά κλάση: $P(y=c|\mathbf{x}) = \frac{\text{count}(c)}{k}$
    \item Πρόβλεψη: $\hat{y} = \arg\max_c P(y=c|\mathbf{x})$
\end{enumerate}
\end{block}

\begin{block}{Ευκλείδεια Απόσταση}
\[
d(\mathbf{x}_i, \mathbf{x}_j) = \sqrt{\sum_{d=1}^{D}(x_{i,d} - x_{j,d})^2}
\]
\end{block}
\end{frame}

\begin{frame}{Υλοποίηση - Συναρτήσεις}
\begin{enumerate}
    \item \textbf{eucl(x, trainData):}
    \begin{itemize}
        \item Υπολογίζει ευκλείδεια απόσταση από όλα τα training δείγματα
        \item Επιστρέφει διάνυσμα αποστάσεων
    \end{itemize}
    
    \item \textbf{neighbors(x, trainData, k):}
    \begin{itemize}
        \item Καλεί eucl() για υπολογισμό αποστάσεων
        \item Ταξινομεί κατά αύξουσα σειρά
        \item Επιστρέφει τα $k$ κορυφαία σημεία
    \end{itemize}
    
    \item \textbf{predict(testData, trainData, k):}
    \begin{itemize}
        \item Καλεί neighbors() για κάθε test δείγμα
        \item Υπολογίζει πιθανότητες κλάσεων
        \item Επιστρέφει πίνακα πιθανοτήτων
    \end{itemize}
\end{enumerate}
\end{frame}

\begin{frame}{Βελτιστοποίηση k}
\begin{itemize}
    \item Δοκιμή όλων των τιμών $k \in [1, 30]$
    \item Υπολογισμός accuracy για κάθε $k$:
    \[
    \text{Accuracy} = \frac{\text{Σωστές Προβλέψεις}}{\text{Σύνολο Test Δειγμάτων}}
    \]
    \item Επιλογή $k^*$ με το μέγιστο accuracy
\end{itemize}

\vfill
\begin{exampleblock}{Trade-off}
\begin{itemize}
    \item Μικρό $k$: Ευαίσθητο σε θόρυβο
    \item Μεγάλο $k$: Over-smoothing, απώλεια δομής
\end{itemize}
\end{exampleblock}
\end{frame}

\begin{frame}{Αποτελέσματα}
\begin{columns}[c]
\column{0.5\textwidth}
\begin{block}{Βέλτιστο $k$}
\begin{itemize}
    \item $k^* = 11$
    \item Accuracy = 73.47\%
\end{itemize}
\end{block}

\column{0.5\textwidth}
\includegraphics[width=\textwidth]{plots/partC_accuracy_vs_k.png}
\end{columns}
\end{frame}

\begin{frame}{Decision Boundaries}
\begin{center}
\includegraphics[width=0.75\textwidth]{plots/partC_decision_boundary.png}
\end{center}
\vspace{-0.2cm}
\footnotesize
Ορια απόφασης για $k^*=11$. Τα χρώματα δείχνουν τις περιοχές ταξινόμησης.
\end{frame}

% ============================================================================
% PART D (20 slides)
% ============================================================================
\section{Μέρος Δ: Υβριδικές Μέθοδοι Ensemble για Πινακοποιημένα Δεδομένα}

\begin{frame}{Μέρος Δ: Επισκόπηση}
\begin{block}{Στόχος}
Ταξινόμηση 8743 training samples (224 features) σε 5 κλάσεις, πρόβλεψη για 6955 test samples.
\end{block}

\begin{columns}[c]
\column{0.6\textwidth}
\textbf{Μεθοδολογία:}
\begin{itemize}
    \item Υβριδική αρχιτεκτονική (Gradient Boosting + Neural Networks)
    \item Προηγμένη μηχανική χαρακτηριστικών
    \item Stochastic ensemble με πολλαπλά seeds
    \item Τεχνικές αιχμής (DART, SAM, Langevin)
\end{itemize}

\column{0.4\textwidth}
\begin{block}{Βασική Ιδέα}
Συνδυασμός πολλαπλών ορθογώνιων τεχνικών για μέγιστη γενίκευση
\end{block}
\end{columns}
\end{frame}

\begin{frame}{Αρχιτεκτονική Συστήματος}
\begin{block}{Pipeline}
\begin{enumerate}
    \item \textbf{Raw Data:} 224 features, 8743 samples
    \item \textbf{Preprocessing:} Quantile transformation, feature selection
    \item \textbf{Feature Engineering:} LID, PageRank, Transductive DAE
    \item \textbf{Ensemble Models:} XGBoost DART, CatBoost Langevin, TabR, TabPFN
    \item \textbf{Calibration:} Isotonic regression, LID temperature scaling
    \item \textbf{Final Predictions:} NNLS Stacking (1 seed)
\end{enumerate}
\end{block}
\end{frame}

\begin{frame}{Μηχανική Χαρακτηριστικών (Feature Engineering)}
\footnotesize
\begin{columns}[t]
\column{0.5\textwidth}
\textbf{1. Quantile Transformation:}
\begin{itemize}
    \item $x' = \Phi^{-1}(F(x))$
    \item Robust σε outliers
\end{itemize}

\vspace{0.2cm}
\textbf{2. Feature Selection (Razor):}
\begin{itemize}
    \item Per-model masks (\textasciitilde{}202 features)
    \item Μείωση θορύβου
\end{itemize}

\column{0.5\textwidth}
\textbf{3. Manifold Engineering:}
\begin{itemize}
    \item LID, PageRank στον KNN γράφο
    \item \textbf{Laplacian Eigenmaps (GPU):} Χαμηλοδιάστατη γεωμετρική αναπαράσταση (torch.lobpcg)
\end{itemize}

\vspace{0.2cm}
\textbf{4. Transductive DAE:}
\begin{itemize}
    \item Εκπαίδευση σε train+test
    \item Πλούσια embeddings χωρίς label leakage
\end{itemize}
\end{columns}
\end{frame}

\begin{frame}{Μοντέλα Ensemble (1/3)}
\begin{block}{1. XGBoost με DART}
\textbf{Dropouts meet Additive Regression Trees}
\begin{itemize}
    \item Random dropout δέντρων κατά boosting
    \item Αποφυγή over-specialization
    \item rate\_drop=0.1, skip\_drop=0.5
\end{itemize}
\end{block}

\begin{block}{2. CatBoost με Langevin Dynamics}
\textbf{Στοχαστική Βελτιστοποίηση}
\[
\theta_{t+1} = \theta_t - \eta \nabla L + \sqrt{2\eta T} \epsilon_t
\]
\begin{itemize}
    \item Θερμικός θόρυβος για εξερεύνηση
    \item Διαφυγή από τοπικά ελάχιστα
    \item diffusion\_temperature=1000
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Μοντέλα Ensemble (2/3)}
\begin{block}{3. TabR (Attention-Based Retrieval)}
\textbf{PyTorch Neural Architecture}
\begin{enumerate}
    \item Encoder: MLP για embedding
    \item Retrieval: k-NN στον embedding space
    \item Cross-Attention: Query-Key-Value
    \item Classification head
\end{enumerate}
Βασικές τεχνικές:
\begin{itemize}
    \item Topology-Aware MixUp
    \item Batch size: 512 (RTX 3060 constraint)
    \item Learning rate: 2e-3
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Μοντέλα Ensemble (3/3)}
\begin{block}{4. TabPFN v2.5 (Prior-Data Fitted Network)}
\textbf{In-Context Learning για Tabular Data}
\begin{itemize}
    \item Pre-trained transformer σε synthetic datasets (v2.5)
    \item \textbf{Post-Hoc Ensembling (PHE):} Αυτόματη σύνθεση ensemble από 100 base models
    \item Zero-shot inference (χωρίς training στα δικά μας δεδομένα)
    \item Περιορισμός: max 50000 samples, 500 features (v2.5)
    \item Παρέχει ανεξάρτητη οπτική (ορθογωνικό μοντέλο)
    \item Χρησιμοποιεί enhanced features (παρά τις προτιμήσεις δημιουργών για raw)
\end{itemize}
\end{block}

\begin{exampleblock}{Στρατηγική Ensemble: NNLS Stacking}
Συνδυασμός με Non-Negative Least Squares (όχι απλό averaging). Εύρεση βέλτιστων βαρών $w_i \geq 0$.
\end{exampleblock}
\end{frame}

\begin{frame}{Βελτιστοποίηση \& Κανονικοποίηση (Regularization)}
\begin{columns}[c]
\column{0.5\textwidth}
\textbf{Monte Carlo Ensemble:}
\begin{itemize}
    \item Τεχνική: 5 τυχαία seeds για variance reduction
    \item (Δεν χρησιμοποιήθηκε στο τελικό μοντέλο)
\end{itemize}

\vspace{0.5cm}
\textbf{5-Fold Cross-Validation:}
\begin{itemize}
    \item Stratified splits
    \item 80\% train, 20\% validation
    \item Τελικό: Μέσος όρος 5 models
\end{itemize}

\column{0.5\textwidth}
\textbf{Isotonic Calibration:}
\begin{itemize}
    \item Βαθμονόμηση πιθανοτήτων
    \item Μονότονη παλινδρόμηση
    \item Καλύτερη αξιοπιστία
\end{itemize}

\vspace{0.5cm}
\textbf{Topology MixUp:}
\begin{itemize}
    \item MixUp μόνο με k-NN
    \item Διατήρηση manifold
    \item Data augmentation
\end{itemize}
\end{columns}
\end{frame}

\begin{frame}{Προηγμένες Τεχνικές: Στοίβαξη (Stacking)}
\begin{block}{Cross-Fit Stacking}
\textbf{Meta-learning με OOF πιθανότητες}
\begin{enumerate}
    \item Κάθε base model παράγει out-of-fold predictions
    \item Meta-learner εκπαιδεύεται στα OOF embeddings
    \item Επιλογές: Logistic Regression, LightGBM, Mixture-of-Experts
\end{enumerate}
\end{block}

\vfill
\begin{exampleblock}{Πλεονέκτημα}
Το stacking μαθαίνει βέλτιστα βάρη για κάθε μοντέλο, καλύτερα από απλό averaging.
\end{exampleblock}
\end{frame}

\begin{frame}{Προηγμένες Τεχνικές: Αυτο-Εκπαίδευση (Self-Training)}
\begin{block}{Iterative Pseudo-Labeling}
\textbf{Transductive Learning}
\begin{enumerate}
    \item Πρόβλεψη με high confidence στο test set
    \item Επιλογή samples με:
    \begin{itemize}
        \item Confidence \textgreater\ κατώφλι
        \item Συμφωνία μεταξύ seeds/views
    \end{itemize}
    \item Επανεκπαίδευση με pseudo-labels
    \item Επανάληψη για N iterations
\end{enumerate}
\end{block}

\begin{alertblock}{Προσοχή: Transductive Mode}
Απαιτεί \texttt{ALLOW\_TRANSDUCTIVE=1}. Αυτό επιτρέπει τη χρήση test data για unsupervised τεχνικές (π.χ. DAE, manifold). Κίνδυνος: το visible test set μπορεί να διαφέρει από το hidden test set, οδηγώντας σε overfitting.
\end{alertblock}
\end{frame}

\begin{frame}{Προηγμένες Τεχνικές: Ευθυγράμμιση Πεδίου (Domain Alignment)}
\begin{block}{CORAL (Covariance Alignment)}
\textbf{Μείωση Covariate Shift}

Στόχος: Ευθυγράμμιση των covariance matrices train/test:
\[
\min_W \|\mathbf{C}_{\text{test}} - W\mathbf{C}_{\text{train}}W^T\|^2_F
\]

Εφαρμογή:
\begin{itemize}
    \item Μετά από κάθε feature transformation
    \item Regularization: $\lambda = 10^{-3}$
    \item Βελτιωμένη adaptability
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Σύνθεση Τεχνικών}
\begin{block}{Στρατηγική Ensemble}
\begin{enumerate}
    \item \textbf{Baseline:} CatBoost με Langevin dynamics
    \item \textbf{+DART:} XGBoost με dropout για regularization
    \item \textbf{+TabR:} Attention-based retrieval augmentation
    \item \textbf{+SAM:} Sharpness-aware optimization για flat minima
    \item \textbf{+Monte Carlo:} Πολλαπλά seeds για variance reduction
\end{enumerate}
\end{block}

\begin{exampleblock}{Φιλοσοφία}
Κάθε τεχνική στοχεύει διαφορετική πηγή σφάλματος (bias, variance, distribution shift).
\end{exampleblock}
\end{frame}

\begin{frame}{Feature Selection (Razor)}
\begin{block}{Μεθοδολογία - Per-Model Masks}
\begin{enumerate}
    \item Εκπαίδευση CatBoost \& XGBoost scout models (5-fold CV)
    \item Υπολογισμός CV-averaged feature importance ανά μοντέλο
    \item Δημιουργία ξεχωριστών masks: Αφαίρεση bottom 10\% → \textasciitilde{}202 features κάθε μοντέλο
\end{enumerate}
\end{block}

\vfill
\begin{exampleblock}{Εξαιρέσεις}
\begin{itemize}
    \item \textbf{TabR:} Χρησιμοποιεί Quantile view (όχι Razor mask)
    \item \textbf{TabPFN:} Χρησιμοποιεί enhanced features (παρά τη σύσταση δημιουργών για raw data)
\end{itemize}
\end{exampleblock}
\end{frame}

\begin{frame}{Hyperparameter Configuration}
\begin{block}{Επιλεγμένες Ρυθμίσεις}
\begin{itemize}
    \item \textbf{Batch Size:} 512 (βέλτιστο για σταθερότητα σε RTX 3060)
    \item \textbf{Learning Rate:} 2e-3 με linear scaling
    \item \textbf{SAM $\rho$:} 0.08 για balance exploration/exploitation
    \item \textbf{Seeds:} 1337 (single seed στο τελικό)
    \item \textbf{CV Folds:} 5 για robust validation
\end{itemize}
\end{block}

\begin{alertblock}{Hardware Constraint}
Βέλτιστη ρύθμιση για RTX 3060 (6GB VRAM): Batch 512 για σταθερότητα, αφήνοντας περιθώριο για TabPFN/TabR.
\end{alertblock}
\end{frame}



\begin{frame}{Μαθηματική Διατύπωση}
\footnotesize
\begin{block}{Συνολική Loss Function}
\[
\mathcal{L}_{\text{total}} = \underbrace{\mathcal{L}_{\text{CE}}}_{\text{Cross-Entropy}} + \lambda_1 \underbrace{\mathcal{L}_{\text{SAM}}}_{\text{Sharpness}} + \lambda_2 \underbrace{\mathcal{L}_{\text{MixUp}}}_{\text{Consistency}} + \lambda_3 \underbrace{\mathcal{L}_{\text{DAE}}}_{\text{Reconstruction}}
\]
\begin{itemize}
    \item $\mathcal{L}_{\text{CE}} = -\sum_c y_c \log \hat{y}_c$ (με label smoothing $\epsilon=0.1$)
    \item $\mathcal{L}_{\text{SAM}}$: Perturbation $\epsilon = \rho \frac{\nabla L}{\|\nabla L\|}$, $\rho=0.08$
    \item $\mathcal{L}_{\text{DAE}} = \|x - \text{Dec}(\text{Enc}(x + \text{noise}))\|^2$
\end{itemize}
\end{block}

\begin{block}{NNLS Stacking \& LID Temperature Scaling}
\[
P_{\text{ens}}(y|x) = \text{softmax}\left(\frac{1}{T(x)} \sum_{m=1}^{M} w_m \cdot \log P_m(y|x)\right), \quad w_m \geq 0
\]
όπου $T(x) = 1 + \alpha \cdot \text{LID}(x)$ (υψηλό LID → υψηλή θερμοκρασία → μειωμένη εμπιστοσύνη)
\end{block}
\end{frame}

\begin{frame}{Περιορισμοί}
\begin{enumerate}
    \item \textbf{Υπολογιστικό Κόστος:}
    \begin{itemize}
        \item 1 seed × 5 folds = 5 model trainings ανά view/μοντέλο
        \item Συνολικός χρόνος: 1-2 ώρες (RTX 3060)
    \end{itemize}
    
    \item \textbf{Μνήμη:}
    \begin{itemize}
        \item Απαιτεί 6GB VRAM
        \item Δεν κλιμακώνεται εύκολα σε \textgreater1M samples
    \end{itemize}
    
    \item \textbf{Hyperparameter Sensitivity:}
    \begin{itemize}
        \item SAM $\rho$, Langevin temperature
        \item Topology MixUp $k$
    \end{itemize}
\end{enumerate}
\end{frame}

\begin{frame}{Μελλοντικές Επεκτάσεις}
\begin{columns}[c]
\column{0.5\textwidth}
\textbf{Υλοποιημένα:}
\begin{itemize}
    \item \textcolor{green!60!black}{$\checkmark$} Bayesian Optimization (Optuna με TPE sampler)
    \item \textcolor{green!60!black}{$\checkmark$} Uncertainty Quantification (Isotonic + LID Scaling, ~0.1\% gain)
\end{itemize}

\vspace{0.3cm}
\textbf{Μελλοντικά:}
\begin{itemize}
    \item Neural Architecture Search (NAS)
    \item Conformal Prediction
\end{itemize}

\column{0.5\textwidth}
\textbf{Scalability:}
\begin{itemize}
    \item Federated Learning
    \item Continual Learning
    \item Distributed Training
    \item Model Compression
\end{itemize}
\end{columns}

\vfill
\footnotesize
\textbf{Σημείωση:} LLM Feature Extraction δεν εφαρμόζεται σε ανωνυμοποιημένα αριθμητικά δεδομένα.
\end{frame}

\begin{frame}{Αποτυχημένα Μοντέλα}
\footnotesize
\begin{tabular}{ll}
\toprule
\textbf{Μοντέλο} & \textbf{Λόγος Απόρριψης} \\
\midrule
GANDALF & Neural Decision Tree - υψηλό ρίσκο, ισοδύναμο XGBoost \\
KAN & Αργό \& ασταθές, θεωρητική υπόσχεση χωρίς πρακτικό όφελος \\
TabM & Υστερεί σε diversity έναντι BatchEnsemble variant \\
Hopular & Redundant με TabR (και τα δύο memory-based) \\
CARTE & Απαιτεί σημασιολογικά features (strings) \\
Mambular & Δύσκολη εγκατάσταση CUDA kernels (*πιθανώς αποτελεσματικό) \\
DeepInsight & Tabular→Images: αργό, redundant με Manifold features \\
AutoGluon & Generic AutoML, δεν υποστηρίζει TTT/Pseudo-labeling \\
\bottomrule
\end{tabular}

\vfill
\begin{alertblock}{Σημείωση}
Η απόρριψη αφορά την \textbf{transductive φιλοσοφία} του Τελικού Μοντέλου. Τα μοντέλα μπορεί να είναι αποτελεσματικά σε άλλα contexts.
\end{alertblock}
\end{frame}

\begin{frame}{Συμπεράσματα Μέρους Δ}
\begin{block}{Επιτεύγματα}
\begin{itemize}
    \item Υβριδική αρχιτεκτονική (Trees + Neural Networks)
    \item Advanced feature engineering \& manifold analysis
    \item State-of-the-art optimization techniques
    \item Πολλαπλές τεχνικές γενίκευσης (5-fold CV, βαθμονόμηση)
\end{itemize}
\end{block}

\begin{alertblock}{Κλειδί Επιτυχίας}
Ο συνδυασμός πολλαπλών ορθογώνιων τεχνικών:
\begin{itemize}
    \item Stochastic optimization (DART, Langevin, SAM)
    \item Topology awareness (MixUp, LID, PageRank)
    \item Variance reduction (5-fold CV, Calibration)
\end{itemize}
\end{alertblock}
\end{frame}

% ============================================================================
% FINAL SLIDE
% ============================================================================
\section{Συνολικά Συμπεράσματα}

\begin{frame}{Συνολικά Συμπεράσματα}
\begin{block}{Μέρος Α: Maximum Likelihood}
Επιτυχής εκτίμηση παραμέτρων 3 κανονικών κατανομών.
\end{block}

\begin{block}{Μέρος Β: Parzen Windows}
Εύρεση βέλτιστου bandwidth για 2 kernels, validation με MSE.
\end{block}

\begin{block}{Μέρος Γ: k-NN}
Υλοποίηση KNN από την αρχή, βελτιστοποίηση $k$, visualization.
\end{block}

\begin{block}{Μέρος Δ: Τελικό Μοντέλο}
Υβριδική αρχιτεκτονική ensemble με προηγμένες τεχνικές βελτιστοποίησης.
\end{block}
\end{frame}

\begin{frame}
\begin{center}
\Huge Ευχαριστώ για την προσοχή σας!
\vfill
\large Ερωτήσεις;
\end{center}
\end{frame}

\end{document}
