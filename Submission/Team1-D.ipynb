{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "fbfdd82f",
            "metadata": {},
            "source": [
                "# Machine Learning Project - Part D\n",
                "**Team 1**\n",
                "* Name: Evangelos Moschou\n",
                "* AEM: 10986\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dde9a654",
            "metadata": {},
            "source": [
                "## Part D: Classification Challenge (Zeta-Omega Protocol Build)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "78474ab6",
            "metadata": {},
            "source": [
                "import os\n",
                "import sys\n",
                "import copy\n",
                "import time\n",
                "import warnings\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader, TensorDataset\n",
                "from sklearn.base import BaseEstimator, ClassifierMixin\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
                "from sklearn.preprocessing import QuantileTransformer, LabelEncoder, StandardScaler\n",
                "from sklearn.neighbors import NearestNeighbors, kneighbors_graph\n",
                "from sklearn.metrics import accuracy_score, roc_auc_score\n",
                "from catboost import CatBoostClassifier\n",
                "\n",
                "# ------------------------------------------------------------------------------\n",
                "# CONFIGURATION\n",
                "# ------------------------------------------------------------------------------\n",
                "warnings.filterwarnings('ignore')\n",
                "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "SEED = 42\n",
                "\n",
                "def seed_everything(seed=42):\n",
                "    import random\n",
                "    random.seed(seed)\n",
                "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    if torch.cuda.is_available():\n",
                "        torch.cuda.manual_seed(seed)\n",
                "        torch.backends.cudnn.deterministic = True\n",
                "\n",
                "seed_everything(SEED)\n",
                "print(f\"[INIT] Device: {DEVICE}\")\n",
                "print(\"[INIT] Initializing Zeta-Omega Protocol (Submission Build)...\")\n",
                "\n",
                "# --- Data Paths (Adjustment for notebook location) ---\n",
                "DATA_PATH_TRAIN = '../Datasets/datasetTV.csv'\n",
                "DATA_PATH_TEST = '../Datasets/datasetTest.csv'\n",
                "OUTPUT_FILE = 'labels1.npy'\n",
                "\n",
                "def load_data():\n",
                "    if not os.path.exists(DATA_PATH_TRAIN):\n",
                "        train_path = 'Datasets/datasetTV.csv'\n",
                "        test_path = 'Datasets/datasetTest.csv'\n",
                "    else:\n",
                "        train_path = DATA_PATH_TRAIN\n",
                "        test_path = DATA_PATH_TEST\n",
                "        \n",
                "    train_df = pd.read_csv(train_path, header=None)\n",
                "    test_df = pd.read_csv(test_path, header=None)\n",
                "    X = train_df.iloc[:, :-1].values\n",
                "    y = train_df.iloc[:, -1].values\n",
                "    X_test = test_df.values\n",
                "    return X, y, X_test\n",
                "\n",
                "# ------------------------------------------------------------------------------\n",
                "# 1. TABULAR DAE (THE TURBOCHARGER)\n",
                "# ------------------------------------------------------------------------------\n",
                "class TabularDAE(nn.Module):\n",
                "    def __init__(self, input_dim, hidden_dim=256, bottleneck_dim=64, noise_factor=0.1):\n",
                "        super(TabularDAE, self).__init__()\n",
                "        self.noise_factor = noise_factor\n",
                "        self.encoder = nn.Sequential(\n",
                "            nn.Linear(input_dim, hidden_dim), nn.BatchNorm1d(hidden_dim), nn.SiLU(),\n",
                "            nn.Linear(hidden_dim, bottleneck_dim), nn.BatchNorm1d(bottleneck_dim), nn.SiLU() \n",
                "        )\n",
                "        self.decoder = nn.Sequential(\n",
                "            nn.Linear(bottleneck_dim, hidden_dim), nn.BatchNorm1d(hidden_dim), nn.SiLU(),\n",
                "            nn.Linear(hidden_dim, input_dim)\n",
                "        )\n",
                "    def forward(self, x): return self.decoder(self.encoder(x))\n",
                "    def get_embedding(self, x):\n",
                "        with torch.no_grad(): return self.encoder(x)\n",
                "\n",
                "class DAE_Embedder:\n",
                "    def __init__(self, input_dim, device=DEVICE):\n",
                "        self.device = device\n",
                "        self.model = TabularDAE(input_dim).to(device)\n",
                "    def fit(self, X_all, epochs=30, batch_size=256):\n",
                "        print(f\"\\n[DAE] Training Turbocharger on {X_all.shape} samples...\")\n",
                "        optimizer = optim.AdamW(self.model.parameters(), lr=1e-3)\n",
                "        criterion = nn.MSELoss()\n",
                "        X_t = torch.tensor(X_all, dtype=torch.float32).to(self.device)\n",
                "        loader = DataLoader(TensorDataset(X_t), batch_size=batch_size, shuffle=True)\n",
                "        self.model.train()\n",
                "        for ep in range(epochs):\n",
                "            for batch in loader:\n",
                "                x_clean = batch[0]\n",
                "                noise = torch.randn_like(x_clean) * self.model.noise_factor\n",
                "                optimizer.zero_grad(); recon = self.model(x_clean + noise)\n",
                "                loss = criterion(recon, x_clean); loss.backward(); optimizer.step()\n",
                "        return self\n",
                "    def transform(self, X):\n",
                "        self.model.eval()\n",
                "        with torch.no_grad(): return self.model.get_embedding(torch.tensor(X, dtype=torch.float32).to(self.device)).cpu().numpy()\n",
                "\n",
                "# ------------------------------------------------------------------------------\n",
                "# 2. STRATEGIC CLASSES (LID & EPISTEMICS)\n",
                "# ------------------------------------------------------------------------------\n",
                "class ManifoldEngineer:\n",
                "    def transform(self, X_train, X_test):\n",
                "        print(\"\\n[TOPOLOGY] Engineering Manifold (LID + PageRank)...\")\n",
                "        X_all = np.vstack([X_train, X_test])\n",
                "        knn = NearestNeighbors(n_neighbors=20, n_jobs=-1).fit(X_all)\n",
                "        dists, indices = knn.kneighbors(X_all)\n",
                "        k=20; d_k = dists[:, -1].reshape(-1, 1); d_j = dists[:, 1:]\n",
                "        lid_raw = k / np.sum(np.log(d_k / (d_j + 1e-10) + 1e-10), axis=1)\n",
                "        \n",
                "        try:\n",
                "            import networkx as nx\n",
                "            A = kneighbors_graph(X_all, n_neighbors=15, mode='distance', include_self=False, n_jobs=-1)\n",
                "            G = nx.from_scipy_sparse_array(A)\n",
                "            pr = nx.pagerank(G, alpha=0.85)\n",
                "            pr_vals = np.array([pr[i] for i in range(len(X_all))])\n",
                "        except:\n",
                "            pr_vals = lid_raw\n",
                "            \n",
                "        scaler = StandardScaler()\n",
                "        feats = np.vstack([\n",
                "            scaler.fit_transform(pr_vals.reshape(-1, 1)).flatten(),\n",
                "            scaler.fit_transform(lid_raw.reshape(-1, 1)).flatten()\n",
                "        ]).T\n",
                "        \n",
                "        X_tr_n = np.hstack([X_train, feats[:len(X_train)]])\n",
                "        X_te_n = np.hstack([X_test, feats[len(X_train):]])\n",
                "        knn_test = NearestNeighbors(n_neighbors=6, n_jobs=-1).fit(X_test)\n",
                "        d_test, i_test = knn_test.kneighbors(X_test)\n",
                "        \n",
                "        return X_tr_n, X_te_n, i_test, d_test, lid_raw[len(X_train):]\n",
                "\n",
                "class AdversarialWeigher:\n",
                "    def fit_transform(self, X_train, X_test):\n",
                "        X_drift = np.vstack([X_train, X_test])\n",
                "        y_drift = np.hstack([np.zeros(len(X_train)), np.ones(len(X_test))])\n",
                "        clf = RandomForestClassifier(n_estimators=50, max_depth=6, random_state=SEED, n_jobs=-1)\n",
                "        probs = cross_val_predict(clf, X_drift, y_drift, cv=5, method='predict_proba')[:, 1]\n",
                "        train_probs = probs[:len(X_train)]\n",
                "        weights = np.clip(train_probs / (1 - train_probs + 1e-6), 0.1, 10.0)\n",
                "        return weights / weights.mean()\n",
                "\n",
                "# ------------------------------------------------------------------------------\n",
                "# 3. HELPER FUNCTIONS: TTA & SCALING\n",
                "# ------------------------------------------------------------------------------\n",
                "def apply_lid_temperature_scaling(probs, lid_scores, alpha=0.1):\n",
                "    T = 1.0 + alpha * lid_scores.reshape(-1, 1)\n",
                "    probs_scaled = np.power(probs, 1.0 / T)\n",
                "    return probs_scaled / probs_scaled.sum(axis=1, keepdims=True)\n",
                "\n",
                "def predict_proba_tta(model, X, knn_indices, knn_dists, alpha=0.3):\n",
                "    p_base = model.predict_proba(X)\n",
                "    sigma = 1.0\n",
                "    weights = np.exp(- (knn_dists ** 2) / (2 * sigma ** 2))\n",
                "    weights = weights / (weights.sum(axis=1, keepdims=True) + 1e-10)\n",
                "    N, k = knn_indices.shape; C = p_base.shape[1]\n",
                "    flat_probs = p_base[knn_indices.flatten()].reshape(N, k, C)\n",
                "    p_smooth = (flat_probs * weights[:, :, np.newaxis]).sum(axis=1)\n",
                "    return (1 - alpha) * p_base + alpha * p_smooth\n",
                "\n",
                "# ------------------------------------------------------------------------------\n",
                "# 4. ARCHITECTURES WITH EPISTEMIC DROPOUT\n",
                "# ------------------------------------------------------------------------------\n",
                "class NeuralProxyClassifier(BaseEstimator, ClassifierMixin):\n",
                "    def __init__(self, input_dim, num_classes, type='TabM'):\n",
                "        self.input_dim = input_dim; self.num_classes = num_classes; self.type = type\n",
                "        self.model = None\n",
                "    def fit(self, X, y, w=None):\n",
                "        self.model = nn.Sequential(\n",
                "            nn.Linear(self.input_dim, 256), nn.LayerNorm(256), nn.SiLU(), \n",
                "            nn.Dropout(0.2), # Explicit Dropout for MC\n",
                "            nn.Linear(256, 128), nn.LayerNorm(128), nn.SiLU(), \n",
                "            nn.Dropout(0.2),\n",
                "            nn.Linear(128, self.num_classes)\n",
                "        ).to(DEVICE)\n",
                "        opt = optim.AdamW(self.model.parameters(), lr=1e-3); crit = nn.CrossEntropyLoss(reduction='none')\n",
                "        Xt = torch.tensor(X, dtype=torch.float32).to(DEVICE); yt = torch.tensor(y, dtype=torch.long).to(DEVICE)\n",
                "        wt = torch.tensor(w, dtype=torch.float32).to(DEVICE) if w is not None else torch.ones(len(X)).to(DEVICE)\n",
                "        dl = DataLoader(TensorDataset(Xt, yt, wt), batch_size=256, shuffle=True)\n",
                "        self.model.train()\n",
                "        for _ in range(30):\n",
                "            for xb, yb, wb in dl:\n",
                "                opt.zero_grad(); (crit(self.model(xb), yb) * wb).mean().backward(); opt.step()\n",
                "        return self\n",
                "    def predict_proba(self, X):\n",
                "        self.model.eval()\n",
                "        with torch.no_grad(): return torch.softmax(self.model(torch.tensor(X, dtype=torch.float32).to(DEVICE)), dim=1).cpu().numpy()\n",
                "    def predict_proba_mc_dropout(self, X, n_iter=10):\n",
                "        self.model.train()\n",
                "        Xt = torch.tensor(X, dtype=torch.float32).to(DEVICE)\n",
                "        probs_list = []\n",
                "        with torch.no_grad():\n",
                "            for _ in range(n_iter):\n",
                "                probs_list.append(torch.softmax(self.model(Xt), dim=1).cpu().numpy())\n",
                "        probs_stack = np.array(probs_list)\n",
                "        return np.mean(probs_stack, axis=0), np.var(probs_stack, axis=0).mean(axis=1)\n",
                "\n",
                "class TurboTabRClassifier(BaseEstimator, ClassifierMixin):\n",
                "    def __init__(self, k_neighbors=24, n_estimators=1000):\n",
                "        self.k_neighbors = k_neighbors; self.n_estimators = n_estimators\n",
                "    def fit(self, X, y, sample_weight=None):\n",
                "        self.clf = CatBoostClassifier(iterations=self.n_estimators, depth=8, l2_leaf_reg=5, learning_rate=0.03, verbose=False, allow_writing_files=False, task_type='GPU' if torch.cuda.is_available() else 'CPU')\n",
                "        self.clf.fit(X, y, sample_weight=sample_weight)\n",
                "        return self\n",
                "    def predict_proba(self, X): return self.clf.predict_proba(X)\n",
                "\n",
                "# ------------------------------------------------------------------------------\n",
                "# 5. MAIN ZETA-OMEGA LOOP\n",
                "# ------------------------------------------------------------------------------\n",
                "def main():\n",
                "    print(\"--- Part D: The Zeta-Omega Build ---\")\n",
                "    X, y, X_test = load_data()\n",
                "    le = LabelEncoder(); y_enc = le.fit_transform(y)\n",
                "    \n",
                "    qt = QuantileTransformer(output_distribution='normal', random_state=SEED)\n",
                "    X_gauss = qt.fit_transform(X); X_test_gauss = qt.transform(X_test)\n",
                "    \n",
                "    eng = ManifoldEngineer()\n",
                "    X_topo, X_test_topo, tta_idxs, tta_dists, lid_scores = eng.transform(X, X_test)\n",
                "    \n",
                "    weigher = AdversarialWeigher(); weights = weigher.fit_transform(X, X_test)\n",
                "    \n",
                "    dae = DAE_Embedder(X_gauss.shape[1]).fit(np.vstack([X_gauss, X_test_gauss]))\n",
                "    X_nn_tr = np.hstack([X_gauss, dae.transform(X_gauss)])\n",
                "    X_nn_te = np.hstack([X_test_gauss, dae.transform(X_test_gauss)])\n",
                "    \n",
                "    models = {\n",
                "        'TabM_Proxy': NeuralProxyClassifier(X_nn_tr.shape[1], len(le.classes_)),\n",
                "        'TurboTabR': TurboTabRClassifier()\n",
                "    }\n",
                "    \n",
                "    print(\"\\n[LOOP] Training Ensemble...\")\n",
                "    models['TabM_Proxy'].fit(X_nn_tr, y_enc, w=weights)\n",
                "    models['TurboTabR'].fit(X_topo, y_enc, sample_weight=weights)\n",
                "    \n",
                "    print(\"\\n[ZETA] Running Epistemic Diamond Mining (MC Dropout)...\")\n",
                "    nn_mean, nn_var = models['TabM_Proxy'].predict_proba_mc_dropout(X_nn_te, n_iter=10)\n",
                "    tree_prob = predict_proba_tta(models['TurboTabR'], X_test_topo, tta_idxs, tta_dists)\n",
                "    \n",
                "    diamond_indices = []\n",
                "    for i in range(len(X_test)):\n",
                "        neural_pred = np.argmax(nn_mean[i]); tree_pred = np.argmax(tree_prob[i])\n",
                "        if (neural_pred == tree_pred) and (np.max(nn_mean[i]) > 0.95) and (nn_var[i] < 0.01):\n",
                "            diamond_indices.append(i)\n",
                "            \n",
                "    print(f\"  üíé Epistemic Diamonds Found: {len(diamond_indices)}\")\n",
                "    \n",
                "    final_probs = None\n",
                "    if len(diamond_indices) > 20:\n",
                "        X_pseudo = X_topo[diamond_indices]; y_pseudo = np.argmax(nn_mean[diamond_indices], axis=1)\n",
                "        anchor = CatBoostClassifier(iterations=1000, verbose=False, allow_writing_files=False, task_type='GPU' if torch.cuda.is_available() else 'CPU')\n",
                "        anchor.fit(np.vstack([X_topo, X_pseudo]), np.hstack([y_enc, y_pseudo]), sample_weight=np.hstack([weights, np.ones(len(y_pseudo))*1.5]))\n",
                "        raw_probs = predict_proba_tta(anchor, X_test_topo, tta_idxs, tta_dists)\n",
                "        print(\"  üå°Ô∏è Applying LID-Temperature Scaling...\")\n",
                "        final_probs = apply_lid_temperature_scaling(raw_probs, lid_scores)\n",
                "    else:\n",
                "        avg_probs = (nn_mean + tree_prob) / 2\n",
                "        final_probs = apply_lid_temperature_scaling(avg_probs, lid_scores)\n",
                "        \n",
                "    final_labels = le.inverse_transform(np.argmax(final_probs, axis=1))\n",
                "    np.save(OUTPUT_FILE, final_labels.astype(int))\n",
                "    print(f\"\\n[VICTORY] Zeta Checksum Validated. Saved to {OUTPUT_FILE}\")\n",
                "\n",
                "if __name__ == '__main__':\n",
                "    main()\n"
            ]
        }
    ],
    "metadata": {},
    "nbformat": 4,
    "nbformat_minor": 5
}