{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "fbfdd82f",
            "metadata": {},
            "source": [
                "# Machine Learning Project - Part D\n",
                "**Team 1**\n",
                "* Name: Evangelos Moschou\n",
                "* AEM: 10986\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dde9a654",
            "metadata": {},
            "source": [
                "## \u039c\u03ad\u03c1\u03bf\u03c2 D: \u03a0\u03c1\u03cc\u03ba\u03bb\u03b7\u03c3\u03b7 \u03a4\u03b1\u03be\u03b9\u03bd\u03cc\u03bc\u03b7\u03c3\u03b7\u03c2 (\u03a4\u03bf \u03a0\u03c1\u03c9\u03c4\u03cc\u03ba\u03bf\u03bb\u03bb\u03bf Epsilon)\n",
                "\n",
                "\u0391\u03c5\u03c4\u03cc \u03c4\u03bf notebook \u03c5\u03bb\u03bf\u03c0\u03bf\u03b9\u03b5\u03af \u03bc\u03b9\u03b1 \u03c0\u03c1\u03bf\u03b7\u03b3\u03bc\u03ad\u03bd\u03b7 \u03c3\u03c4\u03c1\u03b1\u03c4\u03b7\u03b3\u03b9\u03ba\u03ae ensemble \u03bc\u03b7\u03c7\u03b1\u03bd\u03b9\u03ba\u03ae\u03c2 \u03bc\u03ac\u03b8\u03b7\u03c3\u03b7\u03c2 \u03b3\u03b9\u03b1 \u03c4\u03b1\u03be\u03b9\u03bd\u03cc\u03bc\u03b7\u03c3\u03b7 \u03c0\u03bf\u03bb\u03bb\u03b1\u03c0\u03bb\u03ce\u03bd \u03ba\u03bb\u03ac\u03c3\u03b5\u03c9\u03bd. \u0397 \u03c0\u03c1\u03bf\u03c3\u03ad\u03b3\u03b3\u03b9\u03c3\u03b7 \u03c3\u03c5\u03bd\u03b4\u03c5\u03ac\u03b6\u03b5\u03b9 \u03b4\u03b5\u03bd\u03b4\u03c1\u03b9\u03ba\u03ac \u03bc\u03bf\u03bd\u03c4\u03ad\u03bb\u03b1 (XGBoost DART, CatBoost Langevin), \u03bd\u03b5\u03c5\u03c1\u03c9\u03bd\u03b9\u03ba\u03ac \u03b4\u03af\u03ba\u03c4\u03c5\u03b1 (TabR, ThetaTabM), \u03ba\u03b1\u03b9 \u03c0\u03c1\u03bf\u03b7\u03b3\u03bc\u03ad\u03bd\u03b5\u03c2 \u03c4\u03b5\u03c7\u03bd\u03b9\u03ba\u03ad\u03c2 \u03cc\u03c0\u03c9\u03c2 domain adaptation, calibration, \u03ba\u03b1\u03b9 stacking."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "51d368a4",
            "metadata": {},
            "source": [
                "**\u039a\u03b5\u03bb\u03af 3: \u03a1\u03cd\u03b8\u03bc\u03b9\u03c3\u03b7 \u0395\u03ba\u03c4\u03ad\u03bb\u03b5\u03c3\u03b7\u03c2 & \u0394\u03b9\u03b1\u03b4\u03c1\u03bf\u03bc\u03ad\u03c2**\n",
                "\n",
                "\u039f\u03c1\u03af\u03b6\u03b5\u03b9 \u03c4\u03b7 \u03c1\u03af\u03b6\u03b1 \u03c4\u03bf\u03c5 \u03b1\u03c0\u03bf\u03b8\u03b5\u03c4\u03b7\u03c1\u03af\u03bf\u03c5, \u03c4\u03b9\u03c2 \u03c0\u03c1\u03bf\u03c3\u03b1\u03c1\u03bc\u03bf\u03b3\u03ad\u03c2 \u03b4\u03b9\u03b1\u03b4\u03c1\u03bf\u03bc\u03ce\u03bd \u03ba\u03b1\u03b9 \u03c4\u03b9\u03c2 \u03c0\u03c1\u03bf\u03b1\u03b9\u03c1\u03b5\u03c4\u03b9\u03ba\u03ad\u03c2 \u03c1\u03c5\u03b8\u03bc\u03af\u03c3\u03b5\u03b9\u03c2. \u0395\u03ba\u03c4\u03b5\u03bb\u03ad\u03c3\u03c4\u03b5 \u03b1\u03c5\u03c4\u03cc \u03c4\u03bf \u03ba\u03b5\u03bb\u03af \u03c0\u03c1\u03ce\u03c4\u03bf \u03b3\u03b9\u03b1 \u03bd\u03b1 \u03c0\u03c1\u03bf\u03b5\u03c4\u03bf\u03b9\u03bc\u03ac\u03c3\u03b5\u03c4\u03b5 \u03c4\u03bf \u03c0\u03b5\u03c1\u03b9\u03b2\u03ac\u03bb\u03bb\u03bf\u03bd \u03b3\u03b9\u03b1 \u03c4\u03b1 \u03b5\u03c0\u03cc\u03bc\u03b5\u03bd\u03b1 \u03ba\u03b5\u03bb\u03b9\u03ac.\n",
                "\n",
                "**\u03a3\u03b7\u03bc\u03b1\u03bd\u03c4\u03b9\u03ba\u03ad\u03c2 \u039c\u03b5\u03c4\u03b1\u03b2\u03bb\u03b7\u03c4\u03ad\u03c2:**\n",
                "- `DO_FULL_RUN`: \u0395\u03bd\u03b5\u03c1\u03b3\u03bf\u03c0\u03bf\u03b9\u03b5\u03af \u03c0\u03bb\u03ae\u03c1\u03b7 \u03b5\u03ba\u03c4\u03ad\u03bb\u03b5\u03c3\u03b7 \u03c4\u03bf\u03c5 pipeline (\u03b1\u03c0\u03b1\u03b9\u03c4\u03b5\u03af GPU)\n",
                "- `SMOKE_RUN`: \u0393\u03c1\u03ae\u03b3\u03bf\u03c1\u03b7 \u03b4\u03bf\u03ba\u03b9\u03bc\u03b1\u03c3\u03c4\u03b9\u03ba\u03ae \u03b5\u03ba\u03c4\u03ad\u03bb\u03b5\u03c3\u03b7 \u03b3\u03b9\u03b1 \u03b5\u03c0\u03b1\u03bb\u03ae\u03b8\u03b5\u03c5\u03c3\u03b7 \u03bb\u03b5\u03b9\u03c4\u03bf\u03c5\u03c1\u03b3\u03af\u03b1\u03c2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "78474ab6",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "from pathlib import Path\n",
                "\n",
                "import numpy as np\n",
                "\n",
                "\n",
                "def _find_repo_root(start: Path) -> Path:\n",
                "    for p in [start] + list(start.parents):\n",
                "        if (p / 'PartD').exists() and (p / 'Datasets').exists():\n",
                "            return p\n",
                "    raise FileNotFoundError('Could not locate repo root (expected PartD/ and Datasets/).')\n",
                "\n",
                "\n",
                "root = _find_repo_root(Path.cwd())\n",
                "\n",
                "# ----------------------\n",
                "# Notebook-runner knobs\n",
                "# ----------------------\n",
                "# Set DO_FULL_RUN = True to run the full pipeline (may be long / require GPU).\n",
                "# By default the cell runs a *smoke* configuration that is quick and safe.\n",
                "DO_FULL_RUN = False\n",
                "SMOKE_RUN = True\n",
                "\n",
                "# Data loader (self-contained fallback to CSVs if project `src` loader is not importable)\n",
                "\n",
                "def load_data_local():\n",
                "    try:\n",
                "        # Prefer repository data loader if available\n",
                "        from src.data_loader import load_data as ld\n",
                "\n",
                "        X, y, X_test = ld()\n",
                "        return X, y, X_test\n",
                "    except Exception:\n",
                "        import pandas as pd\n",
                "\n",
                "        train_path = root / 'Datasets' / 'datasetTV.csv'\n",
                "        test_path = root / 'Datasets' / 'datasetTest.csv'\n",
                "        if not train_path.exists():\n",
                "            raise FileNotFoundError('Expected datasetTV.csv in Datasets/')\n",
                "        train_df = pd.read_csv(train_path, header=None)\n",
                "        test_df = pd.read_csv(test_path, header=None)\n",
                "        X = train_df.iloc[:, :-1].values\n",
                "        y = train_df.iloc[:, -1].values\n",
                "        X_test = test_df.values\n",
                "        return X, y, X_test\n",
                "\n",
                "\n",
                "# Minimal pipeline implementation embedded in the notebook (self-contained)\n",
                "\n",
                "def run_partd_in_notebook(do_full=False, smoke=True):\n",
                "    print('Part D runner (embedded in notebook). do_full=%r smoke=%r' % (do_full, smoke))\n",
                "\n",
                "    X, y, X_test = load_data_local()\n",
                "    print('Loaded data:', X.shape, 'labels:', np.unique(y).size, 'test:', X_test.shape)\n",
                "\n",
                "    # Quick feature transform\n",
                "    try:\n",
                "        from sklearn.preprocessing import QuantileTransformer\n",
                "    except Exception:\n",
                "        raise RuntimeError('scikit-learn is required for the notebook runner.')\n",
                "\n",
                "    qt = QuantileTransformer(output_distribution='normal', random_state=42)\n",
                "    X_q = qt.fit_transform(X)\n",
                "    X_test_q = qt.transform(X_test)\n",
                "\n",
                "    # Simple \"DAE\" placeholder for notebook: use PCA embedding for speed/safety\n",
                "    try:\n",
                "        from sklearn.decomposition import PCA\n",
                "\n",
                "        pca = PCA(n_components=min(32, X_q.shape[1]))\n",
                "        emb_tr = pca.fit_transform(X_q)\n",
                "        emb_te = pca.transform(X_test_q)\n",
                "    except Exception:\n",
                "        emb_tr = np.zeros((X_q.shape[0], 0))\n",
                "        emb_te = np.zeros((X_test_q.shape[0], 0))\n",
                "\n",
                "    X_final_tr = np.hstack([X_q, emb_tr])\n",
                "    X_final_te = np.hstack([X_test_q, emb_te])\n",
                "\n",
                "    # Razor: quick feature importance using CatBoost if available, else use simple variance filter\n",
                "    try:\n",
                "        from catboost import CatBoostClassifier\n",
                "\n",
                "        cb = CatBoostClassifier(iterations=50 if smoke else 800, verbose=False, task_type='GPU' if os.environ.get('CUDA_VISIBLE_DEVICES') else 'CPU')\n",
                "        cb.fit(X_final_tr, y)\n",
                "        imp = np.array(cb.get_feature_importance())\n",
                "        keep = imp > np.percentile(imp, 20)\n",
                "        print('CatBoost razor kept', keep.sum(), 'features')\n",
                "    except Exception:\n",
                "        print('CatBoost not available or failed \u2014 falling back to variance filter')\n",
                "        var = np.var(X_final_tr, axis=0)\n",
                "        keep = var > np.percentile(var, 20)\n",
                "        print('Variance razor kept', keep.sum(), 'features')\n",
                "\n",
                "    X_final_tr = X_final_tr[:, keep]\n",
                "    X_final_te = X_final_te[:, keep]\n",
                "\n",
                "    # Model: use CatBoost when available; else LogisticRegression\n",
                "    if not do_full:\n",
                "        # smoke training: small iterations / quick solver\n",
                "        try:\n",
                "            from catboost import CatBoostClassifier\n",
                "\n",
                "            clf = CatBoostClassifier(iterations=50, verbose=False, task_type='GPU' if os.environ.get('CUDA_VISIBLE_DEVICES') else 'CPU')\n",
                "            clf.fit(X_final_tr, y)\n",
                "            probs = clf.predict_proba(X_final_te)\n",
                "        except Exception:\n",
                "            from sklearn.linear_model import LogisticRegression\n",
                "\n",
                "            clf = LogisticRegression(max_iter=2000)\n",
                "            clf.fit(X_final_tr, y)\n",
                "            probs = clf.predict_proba(X_final_te)\n",
                "    else:\n",
                "        # Full pipeline (attempt to mirror sigma_omega): train ensemble and stacking (best-effort)\n",
                "        try:\n",
                "            from catboost import CatBoostClassifier\n",
                "            from sklearn.ensemble import RandomForestClassifier\n",
                "            from sklearn.linear_model import LogisticRegression\n",
                "            from sklearn.model_selection import StratifiedKFold\n",
                "\n",
                "            # simple ensemble of a CatBoost + RandomForest\n",
                "            cb = CatBoostClassifier(iterations=800, verbose=False, task_type='GPU' if os.environ.get('CUDA_VISIBLE_DEVICES') else 'CPU')\n",
                "            rf = RandomForestClassifier(n_estimators=200, max_depth=7, random_state=42)\n",
                "\n",
                "            cb.fit(X_final_tr, y)\n",
                "            rf.fit(X_final_tr, y)\n",
                "\n",
                "            p_cb = cb.predict_proba(X_final_te)\n",
                "            p_rf = rf.predict_proba(X_final_te)\n",
                "\n",
                "            probs = (p_cb + p_rf) / 2.0\n",
                "\n",
                "            # simple stacking to match prior behavior\n",
                "            oof = np.zeros((X_final_tr.shape[0], probs.shape[1]))\n",
                "            skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
                "            stack_features = []\n",
                "            for tr, va in skf.split(X_final_tr, y):\n",
                "                cb_local = CatBoostClassifier(iterations=200, verbose=False, task_type='GPU' if os.environ.get('CUDA_VISIBLE_DEVICES') else 'CPU')\n",
                "                cb_local.fit(X_final_tr[tr], y[tr])\n",
                "                oof[va] = cb_local.predict_proba(X_final_tr[va])\n",
                "            meta = LogisticRegression(max_iter=2000)\n",
                "            meta.fit(oof, y)\n",
                "            probs = meta.predict_proba(probs)\n",
                "\n",
                "        except Exception as e:\n",
                "            print('Full run fallback due to:', e)\n",
                "            from sklearn.linear_model import LogisticRegression\n",
                "\n",
                "            clf = LogisticRegression(max_iter=2000)\n",
                "            clf.fit(X_final_tr, y)\n",
                "            probs = clf.predict_proba(X_final_te)\n",
                "\n",
                "    preds = np.argmax(probs, axis=1)\n",
                "\n",
                "    # Save outputs (mirror legacy naming)\n",
                "    out_dir = root / 'PartD' / 'outputs'\n",
                "    out_dir.mkdir(parents=True, exist_ok=True)\n",
                "    out_path = out_dir / 'labelsX_grandmaster.npy'\n",
                "    np.save(out_path, preds)\n",
                "    print('Saved predictions to', out_path)\n",
                "\n",
                "    # also copy into Submission\n",
                "    sub_path = root / 'Submission' / 'labels1.npy'\n",
                "    np.save(sub_path, preds)\n",
                "    print('Saved submission copy to', sub_path)\n",
                "\n",
                "    return preds\n",
                "\n",
                "\n",
                "# Execute (safe defaults)\n",
                "if __name__ == '__main__':\n",
                "    if DO_FULL_RUN:\n",
                "        run_partd_in_notebook(do_full=True, smoke=False)\n",
                "    elif SMOKE_RUN:\n",
                "        run_partd_in_notebook(do_full=False, smoke=True)\n",
                "    else:\n",
                "        print('Notebook runner defined as run_partd_in_notebook(do_full=False, smoke=True). Set DO_FULL_RUN=True to run the full pipeline.')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0d1e21bd",
            "metadata": {},
            "source": [
                "# \u0394\u03b9\u03b1\u03b3\u03bd\u03c9\u03c3\u03c4\u03b9\u03ba\u03cc: \u0395\u03c0\u03b1\u03bb\u03ae\u03b8\u03b5\u03c5\u03c3\u03b7 \u03cc\u03c4\u03b9 \u03bf runner \u03b5\u03af\u03bd\u03b1\u03b9 \u03b1\u03c5\u03c4\u03cc\u03bd\u03bf\u03bc\u03bf\u03c2 (\u03b4\u03b5\u03bd \u03b1\u03c0\u03b1\u03b9\u03c4\u03bf\u03cd\u03bd\u03c4\u03b1\u03b9 \u03b5\u03be\u03c9\u03c4\u03b5\u03c1\u03b9\u03ba\u03ac imports)\n",
                "from pathlib import Path\n",
                "print('\u0391\u03c5\u03c4\u03cc\u03bd\u03bf\u03bc\u03bf\u03c2 runner \u03c5\u03c0\u03ac\u03c1\u03c7\u03b5\u03b9; ', 'run_partd_in_notebook' in globals())\n",
                "print('\u03a4\u03bf\u03c0\u03bf\u03b8\u03b5\u03c3\u03af\u03b1 \u03c0\u03b7\u03b3\u03b1\u03af\u03bf\u03c5 \u03ba\u03ce\u03b4\u03b9\u03ba\u03b1 runner: \u03b5\u03bd\u03c3\u03c9\u03bc\u03b1\u03c4\u03c9\u03bc\u03ad\u03bd\u03bf\u03c2 \u03c3\u03b5 \u03b1\u03c5\u03c4\u03cc \u03c4\u03bf notebook')\n",
                "print('\u03a3\u03b7\u03bc\u03b5\u03af\u03c9\u03c3\u03b7: \u0391\u03c5\u03c4\u03cc \u03c4\u03bf \u03b4\u03b9\u03b1\u03b3\u03bd\u03c9\u03c3\u03c4\u03b9\u03ba\u03cc \u03b4\u03b5\u03bd \u03b5\u03b9\u03c3\u03ac\u03b3\u03b5\u03b9 \u03c0\u03bb\u03ad\u03bf\u03bd \u03c4\u03bf sigma_omega\u00b7 \u03c4\u03bf notebook \u03b5\u03af\u03bd\u03b1\u03b9 \u03b1\u03bd\u03b5\u03be\u03ac\u03c1\u03c4\u03b7\u03c4\u03bf.')\n",
                "print('\\n\u0393\u03b9\u03b1 \u03b3\u03c1\u03ae\u03b3\u03bf\u03c1\u03b7 \u03b4\u03bf\u03ba\u03b9\u03bc\u03b1\u03c3\u03c4\u03b9\u03ba\u03ae \u03b5\u03ba\u03c4\u03ad\u03bb\u03b5\u03c3\u03b7: \u03bf\u03c1\u03af\u03c3\u03c4\u03b5 DO_FULL_RUN=False, SMOKE_RUN=True \u03c3\u03c4\u03bf \u03ba\u03b5\u03bb\u03af \u03c4\u03bf\u03c5 runner.')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bd2919a7",
            "metadata": {},
            "source": [
                "**\u039a\u03b5\u03bb\u03af 4: \u03a0\u03bb\u03b1\u03af\u03c3\u03b9\u03bf Notebook & \u039f\u03b4\u03b7\u03b3\u03af\u03b5\u03c2**\n",
                "\n",
                "\u03a3\u03cd\u03bd\u03c4\u03bf\u03bc\u03b5\u03c2 \u03c3\u03b7\u03bc\u03b5\u03b9\u03ce\u03c3\u03b5\u03b9\u03c2 \u03b3\u03b9\u03b1 \u03c4\u03b7 \u03b4\u03bf\u03bc\u03ae \u03c4\u03bf\u03c5 notebook \u03ba\u03b1\u03b9 \u03c4\u03bf\u03bd \u03c4\u03c1\u03cc\u03c0\u03bf \u03b1\u03c0\u03bf\u03b8\u03ae\u03ba\u03b5\u03c5\u03c3\u03b7\u03c2 \u03c4\u03c9\u03bd \u03b1\u03c0\u03bf\u03c4\u03b5\u03bb\u03b5\u03c3\u03bc\u03ac\u03c4\u03c9\u03bd.\n",
                "\n",
                "**\u0394\u03bf\u03bc\u03ae Notebook:**\n",
                "- \u039a\u03b5\u03bb\u03b9\u03ac \u03c1\u03cd\u03b8\u03bc\u03b9\u03c3\u03b7\u03c2 \u2192 \u0392\u03bf\u03b7\u03b8\u03b7\u03c4\u03b9\u03ba\u03ad\u03c2 \u03c3\u03c5\u03bd\u03b1\u03c1\u03c4\u03ae\u03c3\u03b5\u03b9\u03c2 \u2192 \u03a6\u03cc\u03c1\u03c4\u03c9\u03c3\u03b7 \u03b4\u03b5\u03b4\u03bf\u03bc\u03ad\u03bd\u03c9\u03bd \u2192 \u039c\u03b7\u03c7\u03b1\u03bd\u03b9\u03ba\u03ae \u03c7\u03b1\u03c1\u03b1\u03ba\u03c4\u03b7\u03c1\u03b9\u03c3\u03c4\u03b9\u03ba\u03ce\u03bd \u2192 \u039c\u03bf\u03bd\u03c4\u03ad\u03bb\u03b1 \u2192 Calibration \u2192 Stacking \u2192 \u0395\u03ba\u03c4\u03ad\u03bb\u03b5\u03c3\u03b7"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "63552efe",
            "metadata": {},
            "source": [
                "**\u039a\u03b5\u03bb\u03af 5: \u039f\u03bb\u03bf\u03ba\u03bb\u03b7\u03c1\u03c9\u03bc\u03ad\u03bd\u03bf Pipeline \u039c\u03ad\u03c1\u03bf\u03c5\u03c2 D - \u0395\u03c0\u03b9\u03c3\u03ba\u03cc\u03c0\u03b7\u03c3\u03b7**\n",
                "\n",
                "\u0391\u03c5\u03c4\u03cc \u03c4\u03bf \u03ba\u03b5\u03bb\u03af \u03b5\u03be\u03b7\u03b3\u03b5\u03af \u03cc\u03c4\u03b9 \u03cc\u03bb\u03b1 \u03c4\u03b1 \u03c3\u03c5\u03c3\u03c4\u03b1\u03c4\u03b9\u03ba\u03ac \u03b5\u03af\u03bd\u03b1\u03b9 \u03b5\u03bd\u03c3\u03c9\u03bc\u03b1\u03c4\u03c9\u03bc\u03ad\u03bd\u03b1 inline. \u0395\u03ba\u03c4\u03b5\u03bb\u03ad\u03c3\u03c4\u03b5 \u03c4\u03b1 \u03ba\u03b5\u03bb\u03b9\u03ac \u03bc\u03b5 \u03c3\u03b5\u03b9\u03c1\u03ac \u03b3\u03b9\u03b1 \u03bd\u03b1 \u03bf\u03c1\u03af\u03c3\u03b5\u03c4\u03b5:\n",
                "\n",
                "1. **\u03a1\u03cd\u03b8\u03bc\u03b9\u03c3\u03b7 (config)**: \u03a5\u03c0\u03b5\u03c1\u03c0\u03b1\u03c1\u03ac\u03bc\u03b5\u03c4\u03c1\u03bf\u03b9, device, seeds\n",
                "2. **\u0392\u03bf\u03b7\u03b8\u03b7\u03c4\u03b9\u03ba\u03ac (utilities)**: Seeding \u03b3\u03b9\u03b1 \u03b1\u03bd\u03b1\u03c0\u03b1\u03c1\u03b1\u03b3\u03c9\u03b3\u03b9\u03bc\u03cc\u03c4\u03b7\u03c4\u03b1\n",
                "3. **\u03a6\u03cc\u03c1\u03c4\u03c9\u03c3\u03b7 \u03b4\u03b5\u03b4\u03bf\u03bc\u03ad\u03bd\u03c9\u03bd**: CSV fallback \u03b1\u03bd \u03bf project loader \u03b4\u03b5\u03bd \u03b5\u03af\u03bd\u03b1\u03b9 \u03b4\u03b9\u03b1\u03b8\u03ad\u03c3\u03b9\u03bc\u03bf\u03c2\n",
                "4. **Pseudo-labeling**: \u0394\u03bf\u03bc\u03ad\u03c2 \u03b3\u03b9\u03b1 \u03b7\u03bc\u03b9-\u03b5\u03c0\u03bf\u03c0\u03c4\u03b5\u03c5\u03cc\u03bc\u03b5\u03bd\u03b7 \u03bc\u03ac\u03b8\u03b7\u03c3\u03b7\n",
                "5. **\u03a3\u03c5\u03bd\u03b1\u03c1\u03c4\u03ae\u03c3\u03b5\u03b9\u03c2 \u03b1\u03c0\u03ce\u03bb\u03b5\u03b9\u03b1\u03c2**: CE, Focal, class-balanced weights\n",
                "6. **Domain adaptation**: Adversarial reweighting, CORAL\n",
                "7. **Feature views & DAE**: \u039c\u03b5\u03c4\u03b1\u03c3\u03c7\u03b7\u03bc\u03b1\u03c4\u03b9\u03c3\u03bc\u03bf\u03af \u03c7\u03b1\u03c1\u03b1\u03ba\u03c4\u03b7\u03c1\u03b9\u03c3\u03c4\u03b9\u03ba\u03ce\u03bd \u03ba\u03b1\u03b9 Denoising Autoencoder\n",
                "8. **\u039c\u03bf\u03bd\u03c4\u03ad\u03bb\u03b1**: \u0394\u03b5\u03bd\u03b4\u03c1\u03b9\u03ba\u03ac (XGBoost, CatBoost) \u03ba\u03b1\u03b9 \u03bd\u03b5\u03c5\u03c1\u03c9\u03bd\u03b9\u03ba\u03ac (TabR, ThetaTabM)\n",
                "9. **Calibration & Stacking**: Isotonic calibration \u03ba\u03b1\u03b9 meta-learners"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "571da2cc",
            "metadata": {},
            "outputs": [],
            "source": [
                "# sigma_omega: config\n",
                "import os\n",
                "import warnings\n",
                "import numpy as np\n",
                "import torch\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "def _env_bool(name, default=False):\n",
                "    v = os.getenv(name)\n",
                "    if v is None:\n",
                "        return default\n",
                "    return v.strip().lower() in {'1', 'true', 'yes', 'y', 'on'}\n",
                "\n",
                "def _env_int(name, default):\n",
                "    v = os.getenv(name)\n",
                "    return int(v) if v is not None and v.strip() != '' else int(default)\n",
                "\n",
                "def _env_float(name, default):\n",
                "    v = os.getenv(name)\n",
                "    return float(v) if v is not None and v.strip() != '' else float(default)\n",
                "\n",
                "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "_seeds_env = os.getenv('SEEDS')\n",
                "if _seeds_env:\n",
                "    SEEDS = [int(s.strip()) for s in _seeds_env.split(',') if s.strip()]\n",
                "else:\n",
                "    _n_seeds = os.getenv('N_SEEDS')\n",
                "    if _n_seeds:\n",
                "        base = int(os.getenv('SEED_BASE', '42'))\n",
                "        n = int(_n_seeds)\n",
                "        SEEDS = [base + i for i in range(n)]\n",
                "    else:\n",
                "        SEEDS = [42, 43, 44, 45, 46]\n",
                "\n",
                "BATCH_SIZE = 2048\n",
                "LR_SCALE = 2e-3\n",
                "SAM_RHO = 0.08\n",
                "ALLOW_TRANSDUCTIVE = _env_bool('ALLOW_TRANSDUCTIVE', False)\n",
                "USE_STACKING = _env_bool('USE_STACKING', False)\n",
                "VIEWS = [v.strip().lower() for v in os.getenv('VIEWS', 'raw,quantile').split(',') if v.strip()]\n",
                "META_LEARNER = os.getenv('META_LEARNER', 'lr').strip().lower()\n",
                "USE_TABPFN = _env_bool('USE_TABPFN', False)\n",
                "TABPFN_N_ENSEMBLES = _env_int('TABPFN_N_ENSEMBLES', 32)\n",
                "LGBM_MAX_DEPTH = _env_int('LGBM_MAX_DEPTH', 3)\n",
                "LGBM_NUM_LEAVES = _env_int('LGBM_NUM_LEAVES', 31)\n",
                "LGBM_N_ESTIMATORS = _env_int('LGBM_N_ESTIMATORS', 400)\n",
                "ENABLE_ADV_REWEIGHT = _env_bool('ENABLE_ADV_REWEIGHT', False)\n",
                "ADV_MODEL = os.getenv('ADV_MODEL', 'lr').strip().lower()\n",
                "ADV_CLIP = _env_float('ADV_CLIP', 10.0)\n",
                "ADV_POWER = _env_float('ADV_POWER', 1.0)\n",
                "ENABLE_SWA = _env_bool('ENABLE_SWA', False)\n",
                "SWA_START_EPOCH = _env_int('SWA_START_EPOCH', 10)\n",
                "ENABLE_CORAL = _env_bool('ENABLE_CORAL', False)\n",
                "CORAL_REG = _env_float('CORAL_REG', 1e-3)\n",
                "ENABLE_SELF_TRAIN = _env_bool('ENABLE_SELF_TRAIN', False)\n",
                "SELF_TRAIN_ITERS = _env_int('SELF_TRAIN_ITERS', 0)\n",
                "SELF_TRAIN_CONF = _env_float('SELF_TRAIN_CONF', 0.92)\n",
                "SELF_TRAIN_AGREE = _env_float('SELF_TRAIN_AGREE', 1.0)\n",
                "SELF_TRAIN_VIEW_AGREE = _env_float('SELF_TRAIN_VIEW_AGREE', 0.66)\n",
                "SELF_TRAIN_MAX = _env_int('SELF_TRAIN_MAX', 10000)\n",
                "SELF_TRAIN_WEIGHT_POWER = _env_float('SELF_TRAIN_WEIGHT_POWER', 1.0)\n",
                "LOSS_NAME = os.getenv('LOSS', 'ce').strip().lower()\n",
                "LABEL_SMOOTHING = _env_float('LABEL_SMOOTHING', 0.0)\n",
                "FOCAL_GAMMA = _env_float('FOCAL_GAMMA', 2.0)\n",
                "USE_CLASS_BALANCED = _env_bool('CLASS_BALANCED', False)\n",
                "CB_BETA = _env_float('CB_BETA', 0.999)\n",
                "USE_MIXUP = _env_bool('USE_MIXUP', True)\n",
                "DAE_EPOCHS = _env_int('DAE_EPOCHS', 30)\n",
                "DAE_NOISE_STD = _env_float('DAE_NOISE_STD', 0.1)\n",
                "MANIFOLD_K = _env_int('MANIFOLD_K', 20)\n",
                "ENABLE_PAGERANK = _env_bool('ENABLE_PAGERANK', True)\n",
                "ENABLE_LID_SCALING = _env_bool('ENABLE_LID_SCALING', False)\n",
                "LID_T_MIN = _env_float('LID_T_MIN', 1.0)\n",
                "LID_T_MAX = _env_float('LID_T_MAX', 2.5)\n",
                "LID_T_POWER = _env_float('LID_T_POWER', 1.0)\n",
                "ENABLE_TTT = _env_bool('ENABLE_TTT', False)\n",
                "TTT_GAP_LOW = _env_float('TTT_GAP_LOW', 0.10)\n",
                "TTT_GAP_HIGH = _env_float('TTT_GAP_HIGH', 0.35)\n",
                "TTT_EPOCHS = _env_int('TTT_EPOCHS', 1)\n",
                "TTT_MAX_SAMPLES = _env_int('TTT_MAX_SAMPLES', 4096)\n",
                "TTT_LR_MULT = _env_float('TTT_LR_MULT', 0.2)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d440cb6e",
            "metadata": {},
            "source": [
                "**\u039a\u03b5\u03bb\u03af 6: \u03a1\u03cd\u03b8\u03bc\u03b9\u03c3\u03b7 \u03a5\u03c0\u03b5\u03c1\u03c0\u03b1\u03c1\u03b1\u03bc\u03ad\u03c4\u03c1\u03c9\u03bd (Configuration)**\n",
                "\n",
                "\u039f\u03c1\u03af\u03b6\u03b5\u03b9 \u03cc\u03bb\u03b5\u03c2 \u03c4\u03b9\u03c2 \u03c5\u03c0\u03b5\u03c1\u03c0\u03b1\u03c1\u03b1\u03bc\u03ad\u03c4\u03c1\u03bf\u03c5\u03c2 \u03c4\u03bf\u03c5 \u039c\u03ad\u03c1\u03bf\u03c5\u03c2 D, \u03c3\u03c5\u03bc\u03c0\u03b5\u03c1\u03b9\u03bb\u03b1\u03bc\u03b2\u03b1\u03bd\u03bf\u03bc\u03ad\u03bd\u03c9\u03bd:\n",
                "\n",
                "- **Feature Views**: raw, quantile, PCA, ICA, random projection, spectral\n",
                "- **Transductive Flags**: \u0395\u03c0\u03b9\u03c4\u03c1\u03ad\u03c0\u03b5\u03b9 \u03c7\u03c1\u03ae\u03c3\u03b7 test \u03b4\u03b5\u03b4\u03bf\u03bc\u03ad\u03bd\u03c9\u03bd \u03ba\u03b1\u03c4\u03ac \u03c4\u03b7\u03bd \u03b5\u03ba\u03c0\u03b1\u03af\u03b4\u03b5\u03c5\u03c3\u03b7\n",
                "- **Stacking Options**: Meta-learner \u03b5\u03c0\u03b9\u03bb\u03bf\u03b3\u03ad\u03c2 (LR, LGBM, MoE)\n",
                "- **Loss Settings**: Cross-entropy, Focal loss, label smoothing, class balancing\n",
                "- **Self-Training**: \u03a0\u03b1\u03c1\u03ac\u03bc\u03b5\u03c4\u03c1\u03bf\u03b9 \u03b7\u03bc\u03b9-\u03b5\u03c0\u03bf\u03c0\u03c4\u03b5\u03c5\u03cc\u03bc\u03b5\u03bd\u03b7\u03c2 \u03bc\u03ac\u03b8\u03b7\u03c3\u03b7\u03c2\n",
                "- **DAE Parameters**: Epochs, noise, learning rate \u03b3\u03b9\u03b1 \u03c4\u03bf\u03bd autoencoder"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ee896b06",
            "metadata": {},
            "outputs": [],
            "source": [
                "# sigma_omega: utils\n",
                "import os\n",
                "import numpy as np\n",
                "import torch\n",
                "\n",
                "def seed_everything(seed=42):\n",
                "    import random\n",
                "    random.seed(seed)\n",
                "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    if torch.cuda.is_available():\n",
                "        torch.cuda.manual_seed_all(seed)\n",
                "        torch.backends.cudnn.deterministic = True\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "697779fa",
            "metadata": {},
            "source": [
                "**\u039a\u03b5\u03bb\u03af 7: \u0392\u03bf\u03b7\u03b8\u03b7\u03c4\u03b9\u03ba\u03ad\u03c2 \u03a3\u03c5\u03bd\u03b1\u03c1\u03c4\u03ae\u03c3\u03b5\u03b9\u03c2 (Utilities)**\n",
                "\n",
                "\u0392\u03bf\u03b7\u03b8\u03b7\u03c4\u03b9\u03ba\u03ae \u03c3\u03c5\u03bd\u03ac\u03c1\u03c4\u03b7\u03c3\u03b7 `seed_everything()` \u03b3\u03b9\u03b1 \u03b4\u03b9\u03b1\u03c3\u03c6\u03ac\u03bb\u03b9\u03c3\u03b7 \u03b1\u03bd\u03b1\u03c0\u03b1\u03c1\u03b1\u03b3\u03c9\u03b3\u03b9\u03bc\u03cc\u03c4\u03b7\u03c4\u03b1\u03c2.\n",
                "\n",
                "\u0397 \u03c3\u03c5\u03bd\u03ac\u03c1\u03c4\u03b7\u03c3\u03b7 \u03bf\u03c1\u03af\u03b6\u03b5\u03b9 seeds \u03b3\u03b9\u03b1:\n",
                "- Python random module\n",
                "- NumPy random generator\n",
                "- PyTorch (CPU \u03ba\u03b1\u03b9 GPU)\n",
                "- CUDNN deterministic mode"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a52e838a",
            "metadata": {},
            "outputs": [],
            "source": [
                "# sigma_omega: data\n",
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "\n",
                "def load_data_safe():\n",
                "    try:\n",
                "        from src.data_loader import load_data\n",
                "        X, y, X_test = load_data()\n",
                "    except Exception:\n",
                "        root = Path.cwd()\n",
                "        train_path = root / 'Datasets' / 'datasetTV.csv'\n",
                "        test_path = root / 'Datasets' / 'datasetTest.csv'\n",
                "        if not train_path.exists():\n",
                "            raise FileNotFoundError('Expected datasetTV.csv in Datasets/')\n",
                "        train_df = pd.read_csv(train_path, header=None)\n",
                "        test_df = pd.read_csv(test_path, header=None)\n",
                "        X = train_df.iloc[:, :-1].values\n",
                "        y = train_df.iloc[:, -1].values\n",
                "        X_test = test_df.values\n",
                "    if X is None or y is None or X_test is None:\n",
                "        raise ValueError('load_data returned None(s)')\n",
                "    return X, y, X_test\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5bead861",
            "metadata": {},
            "source": [
                "**\u039a\u03b5\u03bb\u03af 8: \u03a6\u03cc\u03c1\u03c4\u03c9\u03c3\u03b7 \u0394\u03b5\u03b4\u03bf\u03bc\u03ad\u03bd\u03c9\u03bd (Data Loader)**\n",
                "\n",
                "\u03a6\u03bf\u03c1\u03c4\u03ce\u03bd\u03b5\u03b9 \u03c4\u03b1 \u03b4\u03b5\u03b4\u03bf\u03bc\u03ad\u03bd\u03b1 \u03b5\u03ba\u03c0\u03b1\u03af\u03b4\u03b5\u03c5\u03c3\u03b7\u03c2 \u03ba\u03b1\u03b9 \u03b5\u03bb\u03ad\u03b3\u03c7\u03bf\u03c5. \u03a7\u03c1\u03b7\u03c3\u03b9\u03bc\u03bf\u03c0\u03bf\u03b9\u03b5\u03af \u03c4\u03bf\u03bd project loader \u03b1\u03bd \u03b5\u03af\u03bd\u03b1\u03b9 \u03b4\u03b9\u03b1\u03b8\u03ad\u03c3\u03b9\u03bc\u03bf\u03c2, \u03b1\u03bb\u03bb\u03b9\u03ce\u03c2 \u03b5\u03c0\u03b9\u03c3\u03c4\u03c1\u03ad\u03c6\u03b5\u03b9 \u03c3\u03b5 \u03b1\u03c0\u03b5\u03c5\u03b8\u03b5\u03af\u03b1\u03c2 \u03b1\u03bd\u03ac\u03b3\u03bd\u03c9\u03c3\u03b7 CSV \u03b1\u03c0\u03cc \u03c4\u03bf\u03bd \u03c6\u03ac\u03ba\u03b5\u03bb\u03bf `Datasets/`.\n",
                "\n",
                "**\u0395\u03c0\u03b9\u03c3\u03c4\u03c1\u03ad\u03c6\u03b5\u03b9:**\n",
                "- `X`: \u03a7\u03b1\u03c1\u03b1\u03ba\u03c4\u03b7\u03c1\u03b9\u03c3\u03c4\u03b9\u03ba\u03ac \u03b5\u03ba\u03c0\u03b1\u03af\u03b4\u03b5\u03c5\u03c3\u03b7\u03c2\n",
                "- `y`: \u0395\u03c4\u03b9\u03ba\u03ad\u03c4\u03b5\u03c2 \u03b5\u03ba\u03c0\u03b1\u03af\u03b4\u03b5\u03c5\u03c3\u03b7\u03c2\n",
                "- `X_test`: \u03a7\u03b1\u03c1\u03b1\u03ba\u03c4\u03b7\u03c1\u03b9\u03c3\u03c4\u03b9\u03ba\u03ac \u03b5\u03bb\u03ad\u03b3\u03c7\u03bf\u03c5"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0e6e92a2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# sigma_omega: pseudo\n",
                "import numpy as np\n",
                "from dataclasses import dataclass\n",
                "\n",
                "@dataclass(frozen=True)\n",
                "class PseudoData:\n",
                "    idx: np.ndarray\n",
                "    y: np.ndarray\n",
                "    w: np.ndarray\n",
                "    @staticmethod\n",
                "    def empty():\n",
                "        return PseudoData(idx=np.array([], dtype=np.int64), y=np.array([], dtype=np.int64), w=np.array([], dtype=np.float32))\n",
                "    def active(self):\n",
                "        return self.idx is not None and self.y is not None and len(self.idx) > 0\n",
                "    def is_soft(self):\n",
                "        return self.y.ndim > 1 or np.issubdtype(self.y.dtype, np.floating)\n",
                "\n",
                "def normalize_pseudo(pseudo_idx=None, pseudo_y=None, pseudo_w=None):\n",
                "    if pseudo_idx is None or pseudo_y is None:\n",
                "        return PseudoData.empty()\n",
                "    idx = np.asarray(pseudo_idx, dtype=np.int64)\n",
                "    y = np.asarray(pseudo_y)\n",
                "    if y.ndim == 1:\n",
                "        y = y.astype(np.int64)\n",
                "    else:\n",
                "        y = y.astype(np.float32)\n",
                "    \n",
                "    if pseudo_w is None:\n",
                "        w = np.ones((len(idx),), dtype=np.float32)\n",
                "    else:\n",
                "        w = np.asarray(pseudo_w, dtype=np.float32)\n",
                "    if len(idx) == 0:\n",
                "        return PseudoData.empty()\n",
                "    return PseudoData(idx=idx, y=y, w=w)\n",
                "\n",
                "def vote_mode_and_agreement(votes_2d):\n",
                "    mode_pred = np.zeros((votes_2d.shape[1],), dtype=np.int64)\n",
                "    agree_frac = np.zeros((votes_2d.shape[1],), dtype=np.float64)\n",
                "    for j in range(votes_2d.shape[1]):\n",
                "        vals, counts = np.unique(votes_2d[:, j], return_counts=True)\n",
                "        k = int(np.argmax(counts))\n",
                "        mode_pred[j] = int(vals[k])\n",
                "        agree_frac[j] = float(np.max(counts)) / float(votes_2d.shape[0])\n",
                "    return mode_pred, agree_frac\n",
                "\n",
                "def view_agreement_fraction(preds_tensor_vs_n, mode_pred):\n",
                "    view_agree_frac = np.zeros((preds_tensor_vs_n.shape[2],), dtype=np.float64)\n",
                "    for vi in range(preds_tensor_vs_n.shape[0]):\n",
                "        view_votes = preds_tensor_vs_n[vi]\n",
                "        view_mode, _ = vote_mode_and_agreement(view_votes)\n",
                "        view_agree_frac += (view_mode == mode_pred).astype(np.float64)\n",
                "    view_agree_frac /= float(preds_tensor_vs_n.shape[0])\n",
                "    return view_agree_frac\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "73ef337b",
            "metadata": {},
            "source": [
                "**\u039a\u03b5\u03bb\u03af 9: Pseudo-Labels & \u03a8\u03b7\u03c6\u03bf\u03c6\u03bf\u03c1\u03af\u03b1**\n",
                "\n",
                "\u039f\u03c1\u03af\u03b6\u03b5\u03b9 \u03b4\u03bf\u03bc\u03ad\u03c2 \u03ba\u03b1\u03b9 \u03c3\u03c5\u03bd\u03b1\u03c1\u03c4\u03ae\u03c3\u03b5\u03b9\u03c2 \u03b3\u03b9\u03b1 \u03b7\u03bc\u03b9-\u03b5\u03c0\u03bf\u03c0\u03c4\u03b5\u03c5\u03cc\u03bc\u03b5\u03bd\u03b7 \u03bc\u03ac\u03b8\u03b7\u03c3\u03b7:\n",
                "\n",
                "- **`PseudoData`**: Dataclass \u03b3\u03b9\u03b1 \u03b1\u03c0\u03bf\u03b8\u03ae\u03ba\u03b5\u03c5\u03c3\u03b7 pseudo-labeled \u03b4\u03b5\u03b9\u03b3\u03bc\u03ac\u03c4\u03c9\u03bd \u03bc\u03b5 \u03b2\u03ac\u03c1\u03b7\n",
                "- **`normalize_pseudo()`**: \u039a\u03b1\u03bd\u03bf\u03bd\u03b9\u03ba\u03bf\u03c0\u03bf\u03af\u03b7\u03c3\u03b7 \u03ba\u03b1\u03b9 \u03b5\u03c0\u03b9\u03ba\u03cd\u03c1\u03c9\u03c3\u03b7 pseudo-labels\n",
                "- **`vote_mode_and_agreement()`**: \u03a5\u03c0\u03bf\u03bb\u03bf\u03b3\u03b9\u03c3\u03bc\u03cc\u03c2 \u03c0\u03bb\u03b5\u03b9\u03bf\u03c8\u03b7\u03c6\u03b9\u03ba\u03ae\u03c2 \u03c8\u03ae\u03c6\u03bf\u03c5 \u03ba\u03b1\u03b9 \u03c0\u03bf\u03c3\u03bf\u03c3\u03c4\u03bf\u03cd \u03c3\u03c5\u03bc\u03c6\u03c9\u03bd\u03af\u03b1\u03c2 \u03bc\u03b5\u03c4\u03b1\u03be\u03cd \u03bc\u03bf\u03bd\u03c4\u03ad\u03bb\u03c9\u03bd\n",
                "- **`view_agreement_fraction()`**: \u03a5\u03c0\u03bf\u03bb\u03bf\u03b3\u03b9\u03c3\u03bc\u03cc\u03c2 \u03c3\u03c5\u03bc\u03c6\u03c9\u03bd\u03af\u03b1\u03c2 \u03bc\u03b5\u03c4\u03b1\u03be\u03cd \u03b4\u03b9\u03b1\u03c6\u03bf\u03c1\u03b5\u03c4\u03b9\u03ba\u03ce\u03bd feature views"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8fbd46d8",
            "metadata": {},
            "outputs": [],
            "source": [
                "# sigma_omega: losses\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn.functional as F\n",
                "\n",
                "def compute_class_balanced_weights(y, num_classes, beta=0.999):\n",
                "    counts = np.bincount(np.asarray(y, dtype=np.int64), minlength=num_classes).astype(np.float64)\n",
                "    effective = 1.0 - np.power(beta, counts)\n",
                "    weights = (1.0 - beta) / (effective + 1e-12)\n",
                "    weights = weights / (weights.mean() + 1e-12)\n",
                "    return weights.astype(np.float32)\n",
                "\n",
                "def smooth_targets(targets, smoothing):\n",
                "    if smoothing <= 0:\n",
                "        return targets\n",
                "    n_classes = targets.shape[1]\n",
                "    return targets * (1.0 - smoothing) + (smoothing / n_classes)\n",
                "\n",
                "def soft_target_ce(logits, targets, class_weights=None):\n",
                "    log_probs = F.log_softmax(logits, dim=1)\n",
                "    if class_weights is not None:\n",
                "        w = class_weights.view(1, -1)\n",
                "        return -(targets * w * log_probs).sum(dim=1).mean()\n",
                "    return -(targets * log_probs).sum(dim=1).mean()\n",
                "\n",
                "def soft_target_focal(logits, targets, gamma=2.0, class_weights=None):\n",
                "    probs = torch.softmax(logits, dim=1).clamp(1e-8, 1.0 - 1e-8)\n",
                "    logp = torch.log(probs)\n",
                "    mod = torch.pow(1.0 - probs, gamma)\n",
                "    if class_weights is not None:\n",
                "        w = class_weights.view(1, -1)\n",
                "        loss = -(targets * w * mod * logp).sum(dim=1)\n",
                "    else:\n",
                "        loss = -(targets * mod * logp).sum(dim=1)\n",
                "    return loss.mean()\n",
                "\n",
                "def apply_lid_temperature_scaling(probs, lid_norm, t_min=1.0, t_max=2.5, power=1.0):\n",
                "    p = np.asarray(probs, dtype=np.float64)\n",
                "    lid = np.asarray(lid_norm, dtype=np.float64).reshape(-1)\n",
                "    lid = np.clip(lid, 0.0, 1.0)\n",
                "    T = float(t_min) + (float(t_max) - float(t_min)) * np.power(lid, float(power))\n",
                "    T = np.clip(T, 1e-3, 1e6).reshape(-1, 1)\n",
                "    logits = np.log(p + 1e-12)\n",
                "    logits = logits / T\n",
                "    logits = logits - logits.max(axis=1, keepdims=True)\n",
                "    exps = np.exp(logits)\n",
                "    return exps / (exps.sum(axis=1, keepdims=True) + 1e-12)\n",
                "\n",
                "def prob_meta_features(probs, lid=None):\n",
                "    p = np.asarray(probs, dtype=np.float64)\n",
                "    p = np.clip(p, 1e-12, 1.0)\n",
                "    p = p / (p.sum(axis=1, keepdims=True) + 1e-12)\n",
                "    part = np.partition(p, kth=(-1, -2), axis=1)\n",
                "    top1 = part[:, -1]\n",
                "    top2 = part[:, -2]\n",
                "    gap = top1 - top2\n",
                "    entropy = -(p * np.log(p)).sum(axis=1)\n",
                "    feats = np.column_stack([top1, gap, entropy])\n",
                "    if lid is not None:\n",
                "        feats = np.column_stack([feats, np.asarray(lid, dtype=np.float64).reshape(-1)])\n",
                "    return feats.astype(np.float32)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4b2850b0",
            "metadata": {},
            "source": [
                "**\u039a\u03b5\u03bb\u03af 10: \u03a3\u03c5\u03bd\u03b1\u03c1\u03c4\u03ae\u03c3\u03b5\u03b9\u03c2 \u0391\u03c0\u03ce\u03bb\u03b5\u03b9\u03b1\u03c2 & Meta-Features**\n",
                "\n",
                "\u03a5\u03bb\u03bf\u03c0\u03bf\u03b9\u03b5\u03af \u03c0\u03c1\u03bf\u03b7\u03b3\u03bc\u03ad\u03bd\u03b5\u03c2 \u03c4\u03b5\u03c7\u03bd\u03b9\u03ba\u03ad\u03c2 \u03b5\u03ba\u03c0\u03b1\u03af\u03b4\u03b5\u03c5\u03c3\u03b7\u03c2:\n",
                "\n",
                "- **Class-Balanced Weights**: \u0391\u03bd\u03c4\u03b9\u03c3\u03c4\u03ac\u03b8\u03bc\u03b9\u03c3\u03b7 \u03b3\u03b9\u03b1 \u03b1\u03bd\u03b9\u03c3\u03bf\u03c1\u03c1\u03bf\u03c0\u03b7\u03bc\u03ad\u03bd\u03b5\u03c2 \u03ba\u03bb\u03ac\u03c3\u03b5\u03b9\u03c2\n",
                "- **Label Smoothing**: \u039c\u03b5\u03af\u03c9\u03c3\u03b7 overfitting \u03bc\u03ad\u03c3\u03c9 \u03b5\u03be\u03bf\u03bc\u03ac\u03bb\u03c5\u03bd\u03c3\u03b7\u03c2 \u03c3\u03c4\u03cc\u03c7\u03c9\u03bd\n",
                "- **Soft-Target Cross-Entropy**: CE \u03bc\u03b5 \u03bc\u03b1\u03bb\u03b1\u03ba\u03bf\u03cd\u03c2 \u03c3\u03c4\u03cc\u03c7\u03bf\u03c5\u03c2\n",
                "- **Focal Loss**: \u0395\u03c3\u03c4\u03af\u03b1\u03c3\u03b7 \u03c3\u03b5 \u03b4\u03cd\u03c3\u03ba\u03bf\u03bb\u03b1 \u03b4\u03b5\u03af\u03b3\u03bc\u03b1\u03c4\u03b1 (\u03b3=2.0)\n",
                "- **LID Temperature Scaling**: \u03a0\u03c1\u03bf\u03c3\u03b1\u03c1\u03bc\u03bf\u03b3\u03ae \u03b8\u03b5\u03c1\u03bc\u03bf\u03ba\u03c1\u03b1\u03c3\u03af\u03b1\u03c2 \u03b2\u03ac\u03c3\u03b5\u03b9 Local Intrinsic Dimensionality\n",
                "- **Probability Meta-Features**: \u0395\u03be\u03b1\u03b3\u03c9\u03b3\u03ae \u03c7\u03b1\u03c1\u03b1\u03ba\u03c4\u03b7\u03c1\u03b9\u03c3\u03c4\u03b9\u03ba\u03ce\u03bd \u03b1\u03c0\u03cc \u03c0\u03b9\u03b8\u03b1\u03bd\u03cc\u03c4\u03b7\u03c4\u03b5\u03c2 (top1, gap, entropy)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bcb69a2a",
            "metadata": {},
            "outputs": [],
            "source": [
                "# sigma_omega: domain\n",
                "import numpy as np\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "\n",
                "\n",
                "def adversarial_weights(X_train, X_test, seed=42, model='lr', clip=10.0, power=1.0):\n",
                "    X_all = np.vstack([X_train, X_test])\n",
                "    y_dom = np.concatenate([np.zeros(len(X_train), dtype=np.int64), np.ones(len(X_test), dtype=np.int64)])\n",
                "    if model == 'xgb':\n",
                "        from xgboost import XGBClassifier\n",
                "        clf = XGBClassifier(\n",
                "            n_estimators=300,\n",
                "            max_depth=4,\n",
                "            learning_rate=0.05,\n",
                "            subsample=0.9,\n",
                "            colsample_bytree=0.9,\n",
                "            objective='binary:logistic',\n",
                "            eval_metric='logloss',\n",
                "            tree_method='hist',\n",
                "            random_state=int(seed),\n",
                "            verbosity=0,\n",
                "        )\n",
                "    else:\n",
                "        clf = LogisticRegression(max_iter=2000)\n",
                "    clf.fit(X_all, y_dom)\n",
                "    p_test = clf.predict_proba(X_train)[:, 1].astype(np.float64)\n",
                "    p_test = np.clip(p_test, 1e-6, 1.0 - 1e-6)\n",
                "    w = p_test / (1.0 - p_test)\n",
                "    w = np.power(w, float(power))\n",
                "    w = np.clip(w, 1.0 / float(clip), float(clip))\n",
                "    w = w / (np.mean(w) + 1e-12)\n",
                "    return w.astype(np.float32)\n",
                "\n",
                "\n",
                "def coral_align(X_train, X_test, reg=1e-3):\n",
                "    X_tr = np.asarray(X_train, dtype=np.float64)\n",
                "    X_te = np.asarray(X_test, dtype=np.float64)\n",
                "    X_trc = X_tr - X_tr.mean(axis=0, keepdims=True)\n",
                "    X_tec = X_te - X_te.mean(axis=0, keepdims=True)\n",
                "    cov_tr = (X_trc.T @ X_trc) / max(1, (len(X_trc) - 1))\n",
                "    cov_te = (X_tec.T @ X_tec) / max(1, (len(X_tec) - 1))\n",
                "    reg = float(reg)\n",
                "    cov_tr = cov_tr + reg * np.eye(cov_tr.shape[0])\n",
                "    cov_te = cov_te + reg * np.eye(cov_te.shape[0])\n",
                "    evals_tr, evecs_tr = np.linalg.eigh(cov_tr)\n",
                "    evals_tr = np.clip(evals_tr, 1e-12, None)\n",
                "    W_tr = evecs_tr @ np.diag(1.0 / np.sqrt(evals_tr)) @ evecs_tr.T\n",
                "    evals_te, evecs_te = np.linalg.eigh(cov_te)\n",
                "    evals_te = np.clip(evals_te, 1e-12, None)\n",
                "    C_te = evecs_te @ np.diag(np.sqrt(evals_te)) @ evecs_te.T\n",
                "    A = W_tr @ C_te\n",
                "    X_tr_a = X_trc @ A + X_tr.mean(axis=0, keepdims=True)\n",
                "    return X_tr_a.astype(np.float32), X_te.astype(np.float32)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2110849f",
            "metadata": {},
            "source": [
                "**\u039a\u03b5\u03bb\u03af 11: Domain Adaptation**\n",
                "\n",
                "\u03a4\u03b5\u03c7\u03bd\u03b9\u03ba\u03ad\u03c2 \u03c0\u03c1\u03bf\u03c3\u03b1\u03c1\u03bc\u03bf\u03b3\u03ae\u03c2 \u03c4\u03bf\u03bc\u03ad\u03b1 \u03b3\u03b9\u03b1 \u03b1\u03bd\u03c4\u03b9\u03bc\u03b5\u03c4\u03ce\u03c0\u03b9\u03c3\u03b7 distribution shift:\n",
                "\n",
                "- **Adversarial Reweighting**: \u0395\u03ba\u03c0\u03b1\u03af\u03b4\u03b5\u03c5\u03c3\u03b7 discriminator \u03b3\u03b9\u03b1 \u03b4\u03b9\u03ac\u03ba\u03c1\u03b9\u03c3\u03b7 train/test, \u03c7\u03c1\u03ae\u03c3\u03b7 \u03c0\u03b9\u03b8\u03b1\u03bd\u03bf\u03c4\u03ae\u03c4\u03c9\u03bd \u03c9\u03c2 \u03b2\u03ac\u03c1\u03b7 \u03b3\u03b9\u03b1 \u03c4\u03b1 training samples. \u0394\u03af\u03bd\u03b5\u03b9 \u03bc\u03b5\u03b3\u03b1\u03bb\u03cd\u03c4\u03b5\u03c1\u03b7 \u03c3\u03b7\u03bc\u03b1\u03c3\u03af\u03b1 \u03c3\u03b5 training samples \u03c0\u03bf\u03c5 \u03bc\u03bf\u03b9\u03ac\u03b6\u03bf\u03c5\u03bd \u03bc\u03b5 test.\n",
                "\n",
                "- **CORAL Alignment**: \u0395\u03c5\u03b8\u03c5\u03b3\u03c1\u03ac\u03bc\u03bc\u03b9\u03c3\u03b7 \u03b4\u03b5\u03cd\u03c4\u03b5\u03c1\u03b7\u03c2 \u03c4\u03ac\u03be\u03b7\u03c2 \u03c3\u03c4\u03b1\u03c4\u03b9\u03c3\u03c4\u03b9\u03ba\u03ce\u03bd (covariance matrices) \u03bc\u03b5\u03c4\u03b1\u03be\u03cd train \u03ba\u03b1\u03b9 test domains. \u039c\u03b5\u03c4\u03b1\u03c3\u03c7\u03b7\u03bc\u03b1\u03c4\u03af\u03b6\u03b5\u03b9 \u03c4\u03b1 training features \u03ce\u03c3\u03c4\u03b5 \u03bd\u03b1 \u03ad\u03c7\u03bf\u03c5\u03bd \u03c0\u03b1\u03c1\u03cc\u03bc\u03bf\u03b9\u03b1 \u03ba\u03b1\u03c4\u03b1\u03bd\u03bf\u03bc\u03ae \u03bc\u03b5 \u03c4\u03b1 test."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "df695fa9",
            "metadata": {},
            "outputs": [],
            "source": [
                "# sigma_omega: features\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from sklearn.decomposition import PCA, FastICA\n",
                "from sklearn.manifold import SpectralEmbedding\n",
                "from sklearn.neighbors import NearestNeighbors, kneighbors_graph\n",
                "from sklearn.preprocessing import QuantileTransformer\n",
                "from sklearn.random_projection import GaussianRandomProjection\n",
                "from torch.utils.data import DataLoader, TensorDataset\n",
                "\n",
                "from math import ceil\n",
                "\n",
                "# uses config, domain.coral_align\n",
                "\n",
                "def apply_feature_view(X_train, X_test, view, seed, allow_transductive=False, enable_coral=False, coral_reg=1e-3):\n",
                "    view = (view or 'raw').strip().lower()\n",
                "    def maybe_coral(a, b):\n",
                "        if enable_coral:\n",
                "            if not allow_transductive:\n",
                "                raise ValueError('CORAL requires ALLOW_TRANSDUCTIVE=1')\n",
                "            from math import isfinite\n",
                "            return coral_align(a, b, reg=coral_reg)\n",
                "        return a, b\n",
                "\n",
                "    if view == 'raw':\n",
                "        return maybe_coral(X_train, X_test)\n",
                "    if view == 'quantile':\n",
                "        qt = QuantileTransformer(output_distribution='normal', random_state=seed)\n",
                "        X_tr, X_te = qt.fit_transform(X_train), qt.transform(X_test)\n",
                "        return maybe_coral(X_tr, X_te)\n",
                "    if view.startswith('pca'):\n",
                "        n_components = min(50, X_train.shape[1], max(2, X_train.shape[0] - 1))\n",
                "        pca = PCA(n_components=n_components, random_state=seed)\n",
                "        X_tr, X_te = pca.fit_transform(X_train), pca.transform(X_test)\n",
                "        return maybe_coral(X_tr, X_te)\n",
                "    if view.startswith('ica'):\n",
                "        n_components = min(50, X_train.shape[1], max(2, X_train.shape[0] - 1))\n",
                "        ica = FastICA(n_components=n_components, random_state=seed, max_iter=500)\n",
                "        X_tr, X_te = ica.fit_transform(X_train), ica.transform(X_test)\n",
                "        return maybe_coral(X_tr, X_te)\n",
                "    if view.startswith('rp') or view.startswith('random'):\n",
                "        n_components = min(50, X_train.shape[1])\n",
                "        rp = GaussianRandomProjection(n_components=n_components, random_state=seed)\n",
                "        X_tr, X_te = rp.fit_transform(X_train), rp.transform(X_test)\n",
                "        return maybe_coral(X_tr, X_te)\n",
                "    if view.startswith('spectral'):\n",
                "        if not allow_transductive:\n",
                "            raise ValueError('spectral view needs ALLOW_TRANSDUCTIVE=1')\n",
                "        X_all = np.vstack([X_train, X_test])\n",
                "        n_components = min(30, X_all.shape[0] - 1)\n",
                "        se = SpectralEmbedding(n_components=n_components, random_state=seed)\n",
                "        Z = se.fit_transform(X_all)\n",
                "        return Z[: len(X_train)], Z[len(X_train):]\n",
                "    raise ValueError('Unknown view: %s' % view)\n",
                "\n",
                "\n",
                "class TransductiveDAE(nn.Module):\n",
                "    def __init__(self, input_dim):\n",
                "        super().__init__()\n",
                "        self.encoder = nn.Sequential(nn.Linear(input_dim, 512), nn.SiLU(), nn.Linear(512, 128))\n",
                "        self.decoder = nn.Sequential(nn.Linear(128, 512), nn.SiLU(), nn.Linear(512, input_dim))\n",
                "    def forward(self, x):\n",
                "        return self.decoder(self.encoder(x))\n",
                "\n",
                "\n",
                "class DataRefinery:\n",
                "    def __init__(self, input_dim, batch_size=2048, lr_scale=2e-3, noise_std=0.1, epochs=30, device=None):\n",
                "        self.dae = TransductiveDAE(input_dim).to(device or torch.device('cpu'))\n",
                "        self.batch_size = batch_size\n",
                "        self.lr_scale = lr_scale\n",
                "        self.noise_std = noise_std\n",
                "        self.epochs = epochs\n",
                "        self.device = device or torch.device('cpu')\n",
                "\n",
                "    def fit(self, X_all):\n",
                "        X_t = torch.tensor(X_all, dtype=torch.float32).to(self.device)\n",
                "        dl = DataLoader(TensorDataset(X_t), batch_size=self.batch_size, shuffle=True)\n",
                "        opt = optim.AdamW(self.dae.parameters(), lr=self.lr_scale)\n",
                "        crit = nn.MSELoss()\n",
                "        self.dae.train()\n",
                "        for _ in range(int(self.epochs)):\n",
                "            for (xb,) in dl:\n",
                "                noise = torch.randn_like(xb) * float(self.noise_std)\n",
                "                rec = self.dae(xb + noise)\n",
                "                loss = crit(rec, xb)\n",
                "                opt.zero_grad(); loss.backward(); opt.step()\n",
                "        return self\n",
                "\n",
                "    def transform(self, X):\n",
                "        self.dae.eval()\n",
                "        X_t = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
                "        emb, rec = [], []\n",
                "        for i in range(0, len(X), self.batch_size):\n",
                "            xb = X_t[i:i + self.batch_size]\n",
                "            with torch.no_grad():\n",
                "                z = self.dae.encoder(xb)\n",
                "                r = self.dae.decoder(z)\n",
                "            emb.append(z.cpu().numpy())\n",
                "            rec.append(r.cpu().numpy())\n",
                "        return np.vstack(emb), np.vstack(rec)\n",
                "\n",
                "\n",
                "def compute_manifold_features(X_train, X_test, allow_transductive=False, k=20, enable_pagerank=True, return_lid=False):\n",
                "    if allow_transductive:\n",
                "        X_all = np.vstack([X_train, X_test])\n",
                "        nbrs = NearestNeighbors(n_neighbors=k, n_jobs=-1).fit(X_all)\n",
                "        dists, _ = nbrs.kneighbors(X_all)\n",
                "        d_k = dists[:, -1]; d_j = dists[:, 1:]\n",
                "        lid = k / np.sum(np.log(d_k[:, None] / (d_j + 1e-10) + 1e-10), axis=1)\n",
                "        lid = (lid - lid.min()) / (lid.max() - lid.min() + 1e-12)\n",
                "        if enable_pagerank:\n",
                "            try:\n",
                "                import networkx as nx\n",
                "                A = kneighbors_graph(X_all, k, mode='connectivity', include_self=False)\n",
                "                G = nx.from_scipy_sparse_array(A)\n",
                "                pr = nx.pagerank(G, alpha=0.85, max_iter=50)\n",
                "                pagerank = np.array([pr[i] for i in range(len(X_all))], dtype=np.float64)\n",
                "                pagerank = (pagerank - pagerank.min()) / (pagerank.max() - pagerank.min() + 1e-12)\n",
                "            except Exception:\n",
                "                pagerank = np.zeros(len(X_all))\n",
                "        else:\n",
                "            pagerank = np.zeros(len(X_all))\n",
                "        feats = np.column_stack([lid, pagerank])\n",
                "        feats_tr, feats_te = feats[:len(X_train)], feats[len(X_train):]\n",
                "        if return_lid:\n",
                "            return feats_tr, feats_te, lid[:len(X_train)], lid[len(X_train):]\n",
                "        return feats_tr, feats_te\n",
                "\n",
                "    nbrs = NearestNeighbors(n_neighbors=min(k + 1, len(X_train)), n_jobs=-1).fit(X_train)\n",
                "    dists_tr, idx_tr = nbrs.kneighbors(X_train)\n",
                "    dists_tr = dists_tr[:, 1:]; idx_tr = idx_tr[:, 1:]; k_eff = dists_tr.shape[1]\n",
                "    d_k_tr = dists_tr[:, -1]; d_j_tr = dists_tr[:, :-1] if k_eff > 1 else dists_tr\n",
                "    lid_tr = k_eff / np.sum(np.log(d_k_tr[:, None] / (d_j_tr + 1e-10) + 1e-10), axis=1)\n",
                "    lid_tr_min, lid_tr_max = lid_tr.min(), lid_tr.max(); lid_tr_n = (lid_tr - lid_tr_min) / (lid_tr_max - lid_tr_min + 1e-12)\n",
                "    if enable_pagerank:\n",
                "        try:\n",
                "            import networkx as nx\n",
                "            A_tr = kneighbors_graph(X_train, min(k, len(X_train) - 1), mode='connectivity', include_self=False)\n",
                "            G_tr = nx.from_scipy_sparse_array(A_tr)\n",
                "            pr_tr_dict = nx.pagerank(G_tr, alpha=0.85, max_iter=50)\n",
                "            pr_tr = np.array([pr_tr_dict[i] for i in range(len(X_train))], dtype=np.float64)\n",
                "            pr_tr_min, pr_tr_max = pr_tr.min(), pr_tr.max(); pr_tr_n = (pr_tr - pr_tr_min) / (pr_tr_max - pr_tr_min + 1e-12)\n",
                "        except Exception:\n",
                "            pr_tr_n = np.zeros(len(X_train))\n",
                "    else:\n",
                "        pr_tr_n = np.zeros(len(X_train))\n",
                "    dists_te, idx_te = nbrs.kneighbors(X_test, n_neighbors=min(k, len(X_train)))\n",
                "    k_te = dists_te.shape[1]; d_k_te = dists_te[:, -1]; d_j_te = dists_te[:, :-1] if k_te > 1 else dists_te\n",
                "    lid_te = k_te / np.sum(np.log(d_k_te[:, None] / (d_j_te + 1e-10) + 1e-10), axis=1)\n",
                "    lid_te_n = (lid_te - lid_tr_min) / (lid_tr_max - lid_tr_min + 1e-12); lid_te_n = np.clip(lid_te_n, 0.0, 1.0)\n",
                "    pr_te_n = pr_tr_n[idx_te].mean(axis=1) if len(pr_tr_n) else np.zeros(len(X_test))\n",
                "    feats_tr = np.column_stack([lid_tr_n, pr_tr_n]); feats_te = np.column_stack([lid_te_n, pr_te_n])\n",
                "    if return_lid:\n",
                "        return feats_tr, feats_te, lid_tr_n, lid_te_n\n",
                "    return feats_tr, feats_te\n",
                "\n",
                "\n",
                "def build_streams(X_v, X_test_v, allow_transductive=False, dae_cfg=None, manifold_k=20, enable_pagerank=True, coral=False, coral_reg=1e-3, device=None, batch_size=2048, lr_scale=2e-3, noise_std=0.1, dae_epochs=30):\n",
                "    ref_fit_X = np.vstack([X_v, X_test_v]) if allow_transductive else X_v\n",
                "    ref = DataRefinery(\n",
                "        X_v.shape[1],\n",
                "        batch_size=batch_size,\n",
                "        lr_scale=lr_scale,\n",
                "        noise_std=noise_std,\n",
                "        epochs=dae_epochs,\n",
                "        device=device,\n",
                "    ).fit(ref_fit_X)\n",
                "    feats_tr, feats_te, lid_tr, lid_te = compute_manifold_features(\n",
                "        X_v,\n",
                "        X_test_v,\n",
                "        allow_transductive=allow_transductive,\n",
                "        k=manifold_k,\n",
                "        enable_pagerank=enable_pagerank,\n",
                "        return_lid=True,\n",
                "    )\n",
                "    emb_tr, rec_tr = ref.transform(X_v)\n",
                "    emb_te, rec_te = ref.transform(X_test_v)\n",
                "    X_neural_tr = np.hstack([X_v, feats_tr, emb_tr])\n",
                "    X_neural_te = np.hstack([X_test_v, feats_te, emb_te])\n",
                "    X_tree_tr = np.hstack([X_v, feats_tr, rec_tr])\n",
                "    X_tree_te = np.hstack([X_test_v, feats_te, rec_te])\n",
                "    return X_tree_tr, X_tree_te, X_neural_tr, X_neural_te, lid_tr, lid_te\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5c060a32",
            "metadata": {},
            "source": [
                "**\u039a\u03b5\u03bb\u03af 12: \u039c\u03b7\u03c7\u03b1\u03bd\u03b9\u03ba\u03ae \u03a7\u03b1\u03c1\u03b1\u03ba\u03c4\u03b7\u03c1\u03b9\u03c3\u03c4\u03b9\u03ba\u03ce\u03bd (Feature Engineering)**\n",
                "\n",
                "\u039f\u03c1\u03af\u03b6\u03b5\u03b9 \u03bc\u03b5\u03c4\u03b1\u03c3\u03c7\u03b7\u03bc\u03b1\u03c4\u03b9\u03c3\u03bc\u03bf\u03cd\u03c2 \u03c7\u03b1\u03c1\u03b1\u03ba\u03c4\u03b7\u03c1\u03b9\u03c3\u03c4\u03b9\u03ba\u03ce\u03bd \u03ba\u03b1\u03b9 manifold descriptors:\n",
                "\n",
                "**Feature Views:**\n",
                "- `raw`: \u0391\u03c1\u03c7\u03b9\u03ba\u03ac \u03c7\u03b1\u03c1\u03b1\u03ba\u03c4\u03b7\u03c1\u03b9\u03c3\u03c4\u03b9\u03ba\u03ac \u03c7\u03c9\u03c1\u03af\u03c2 \u03bc\u03b5\u03c4\u03b1\u03c3\u03c7\u03b7\u03bc\u03b1\u03c4\u03b9\u03c3\u03bc\u03cc\n",
                "- `quantile`: Gaussian Quantile Transformer \u03b3\u03b9\u03b1 \u03ba\u03b1\u03bd\u03bf\u03bd\u03b9\u03ba\u03bf\u03c0\u03bf\u03af\u03b7\u03c3\u03b7 \u03ba\u03b1\u03c4\u03b1\u03bd\u03bf\u03bc\u03ce\u03bd\n",
                "- `pca`: Principal Component Analysis \u03b3\u03b9\u03b1 \u03bc\u03b5\u03af\u03c9\u03c3\u03b7 \u03b4\u03b9\u03b1\u03c3\u03c4\u03ac\u03c3\u03b5\u03c9\u03bd\n",
                "- `ica`: Independent Component Analysis\n",
                "- `rp`: Gaussian Random Projection\n",
                "- `spectral`: Spectral Embedding (\u03b1\u03c0\u03b1\u03b9\u03c4\u03b5\u03af transductive mode)\n",
                "\n",
                "**Transductive DAE:**\n",
                "- Denoising Autoencoder \u03b5\u03ba\u03c0\u03b1\u03b9\u03b4\u03b5\u03c5\u03bc\u03ad\u03bd\u03bf\u03c2 \u03c3\u03b5 train+test \u03b3\u03b9\u03b1 \u03b5\u03be\u03b1\u03b3\u03c9\u03b3\u03ae latent representations\n",
                "\n",
                "**Manifold Descriptors:**\n",
                "- **LID (Local Intrinsic Dimensionality)**: \u039c\u03ad\u03c4\u03c1\u03b7\u03c3\u03b7 \u03c4\u03bf\u03c0\u03b9\u03ba\u03ae\u03c2 \u03c0\u03bf\u03bb\u03c5\u03c0\u03bb\u03bf\u03ba\u03cc\u03c4\u03b7\u03c4\u03b1\u03c2 \u03b3\u03cd\u03c1\u03c9 \u03b1\u03c0\u03cc \u03ba\u03ac\u03b8\u03b5 \u03c3\u03b7\u03bc\u03b5\u03af\u03bf\n",
                "- **PageRank**: \u039a\u03b5\u03bd\u03c4\u03c1\u03b9\u03ba\u03cc\u03c4\u03b7\u03c4\u03b1 \u03c3\u03b7\u03bc\u03b5\u03af\u03c9\u03bd \u03c3\u03c4\u03bf\u03bd KNN \u03b3\u03c1\u03ac\u03c6\u03bf"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fd8972d2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# sigma_omega: models_trees\n",
                "import torch\n",
                "\n",
                "def get_xgb_dart(n_c):\n",
                "    from xgboost import XGBClassifier\n",
                "\n",
                "    use_gpu = torch.cuda.is_available()\n",
                "    params = dict(\n",
                "        booster='dart',\n",
                "        rate_drop=0.1,\n",
                "        skip_drop=0.5,\n",
                "        n_estimators=500,\n",
                "        max_depth=6,\n",
                "        learning_rate=0.05,\n",
                "        objective='multi:softprob',\n",
                "        num_class=int(n_c),\n",
                "        eval_metric='mlogloss',\n",
                "        verbosity=0,\n",
                "    )\n",
                "    if use_gpu:\n",
                "        # XGBoost >= 2.0 uses device='cuda' and tree_method='hist'\n",
                "        params.update(tree_method='hist', device='cuda')\n",
                "    else:\n",
                "        params.update(tree_method='hist', device='cpu')\n",
                "    return XGBClassifier(**params)\n",
                "\n",
                "\n",
                "def get_cat_langevin(n_c):\n",
                "    from catboost import CatBoostClassifier\n",
                "\n",
                "    return CatBoostClassifier(\n",
                "        langevin=True,\n",
                "        diffusion_temperature=1000,\n",
                "        iterations=1000,\n",
                "        depth=8,\n",
                "        learning_rate=0.03,\n",
                "        loss_function='MultiClass',\n",
                "        eval_metric='MultiClass',\n",
                "        task_type='GPU' if torch.cuda.is_available() else 'CPU',\n",
                "        verbose=0,\n",
                "        allow_writing_files=False,\n",
                "    )\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ffbf7a42",
            "metadata": {},
            "source": [
                "**\u039a\u03b5\u03bb\u03af 13: \u0394\u03b5\u03bd\u03b4\u03c1\u03b9\u03ba\u03ac \u039c\u03bf\u03bd\u03c4\u03ad\u03bb\u03b1 (Tree Models)**\n",
                "\n",
                "Factory functions \u03b3\u03b9\u03b1 \u03b4\u03b7\u03bc\u03b9\u03bf\u03c5\u03c1\u03b3\u03af\u03b1 gradient boosting \u03bc\u03bf\u03bd\u03c4\u03ad\u03bb\u03c9\u03bd:\n",
                "\n",
                "**XGBoost DART:**\n",
                "- \u03a7\u03c1\u03b7\u03c3\u03b9\u03bc\u03bf\u03c0\u03bf\u03b9\u03b5\u03af DART (Dropouts meet Multiple Additive Regression Trees)\n",
                "- Rate drop: 10%, Skip drop: 50%\n",
                "- 500 estimators, max_depth=6\n",
                "- \u0391\u03c5\u03c4\u03cc\u03bc\u03b1\u03c4\u03b7 \u03c7\u03c1\u03ae\u03c3\u03b7 GPU \u03b1\u03bd \u03b4\u03b9\u03b1\u03b8\u03ad\u03c3\u03b9\u03bc\u03b7\n",
                "\n",
                "**CatBoost Langevin:**\n",
                "- \u03a7\u03c1\u03b7\u03c3\u03b9\u03bc\u03bf\u03c0\u03bf\u03b9\u03b5\u03af Langevin dynamics \u03b3\u03b9\u03b1 stochastic gradient descent\n",
                "- Diffusion temperature: 1000\n",
                "- 1000 iterations, depth=8\n",
                "- \u039a\u03b1\u03bb\u03cd\u03c4\u03b5\u03c1\u03b7 \u03b3\u03b5\u03bd\u03af\u03ba\u03b5\u03c5\u03c3\u03b7 \u03bc\u03ad\u03c3\u03c9 Bayesian sampling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ed7c7e0f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# sigma_omega: models_torch\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "from sklearn.base import BaseEstimator, ClassifierMixin\n",
                "from sklearn.neighbors import NearestNeighbors\n",
                "from torch.utils.data import DataLoader, TensorDataset\n",
                "\n",
                "# requires: config globals, losses helpers\n",
                "\n",
                "def is_torch_model(m):\n",
                "    return hasattr(m, 'finetune_on_pseudo') and callable(getattr(m, 'finetune_on_pseudo'))\n",
                "\n",
                "def select_silver_samples(probs, gap_low=0.10, gap_high=0.35, max_samples=4096, seed=42):\n",
                "    # Legacy method kept for optional usage, but TTT now prefers Entropy minimization.\n",
                "    p = np.asarray(probs, dtype=np.float64)\n",
                "    if p.ndim != 2 or p.shape[0] == 0:\n",
                "        return np.array([], dtype=np.int64), np.array([], dtype=np.int64)\n",
                "    part = np.partition(p, kth=(-1, -2), axis=1)\n",
                "    top1 = part[:, -1]; top2 = part[:, -2]; gap = top1 - top2\n",
                "    mask = (gap >= float(gap_low)) & (gap <= float(gap_high))\n",
                "    idx = np.nonzero(mask)[0]\n",
                "    if idx.size == 0:\n",
                "        return idx.astype(np.int64), np.array([], dtype=np.int64)\n",
                "    rng = np.random.default_rng(int(seed))\n",
                "    if idx.size > int(max_samples):\n",
                "        idx = rng.choice(idx, size=int(max_samples), replace=False)\n",
                "    y_pseudo = np.argmax(p[idx], axis=1).astype(np.int64)\n",
                "    return idx.astype(np.int64), y_pseudo\n",
                "\n",
                "class TopologyMixUpLoader:\n",
                "    def __init__(self, X, y, num_classes, batch_size):\n",
                "        self.X, self.y, self.num_classes, self.batch_size = X, y, int(num_classes), int(batch_size)\n",
                "        self.knn = NearestNeighbors(n_neighbors=5, n_jobs=-1).fit(X)\n",
                "        self.rng = np.random.default_rng()\n",
                "    def __iter__(self):\n",
                "        idxs = self.rng.permutation(len(self.X))\n",
                "        for i in range(0, len(self.X), self.batch_size):\n",
                "            b_idxs = idxs[i:i + self.batch_size]\n",
                "            X_b, y_b = self.X[b_idxs], self.y[b_idxs]\n",
                "            mn_idxs = self.knn.kneighbors(X_b, return_distance=False)\n",
                "            rand_n = self.rng.integers(1, 5, size=len(X_b))\n",
                "            target_idxs = mn_idxs[np.arange(len(X_b)), rand_n]\n",
                "            X_target = self.X[target_idxs]; y_target = self.y[target_idxs]\n",
                "            lam = self.rng.beta(0.4, 0.4, size=(len(X_b), 1)); lam = np.maximum(lam, 1 - lam)\n",
                "            X_mix = lam * X_b + (1 - lam) * X_target\n",
                "            y_b_oh = np.eye(self.num_classes, dtype=np.float32)[y_b]\n",
                "            y_t_oh = np.eye(self.num_classes, dtype=np.float32)[y_target]\n",
                "            y_mix = lam * y_b_oh + (1 - lam) * y_t_oh\n",
                "            yield (\n",
                "                torch.tensor(X_mix, dtype=torch.float32).to(DEVICE),\n",
                "                torch.tensor(y_mix, dtype=torch.float32).to(DEVICE),\n",
                "            )\n",
                "\n",
                "class SAM(torch.optim.Optimizer):\n",
                "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
                "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
                "        super().__init__(params, defaults)\n",
                "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
                "        self.param_groups = self.base_optimizer.param_groups\n",
                "        self.defaults.update(self.base_optimizer.defaults)\n",
                "    @torch.no_grad()\n",
                "    def first_step(self, zero_grad=False):\n",
                "        grad_norm = self._grad_norm()\n",
                "        for group in self.param_groups:\n",
                "            scale = group['rho'] / (grad_norm + 1e-12)\n",
                "            for p in group['params']:\n",
                "                if p.grad is None: continue\n",
                "                self.state[p]['old_p'] = p.data.clone()\n",
                "                p.add_((torch.pow(p, 2) if group['adaptive'] else 1.0) * p.grad * scale.to(p))\n",
                "        if zero_grad: self.zero_grad()\n",
                "    @torch.no_grad()\n",
                "    def second_step(self, zero_grad=False):\n",
                "        for group in self.param_groups:\n",
                "            for p in group['params']:\n",
                "                if p.grad is None: continue\n",
                "                p.data = self.state[p]['old_p']\n",
                "        if zero_grad: self.zero_grad()\n",
                "    def _grad_norm(self):\n",
                "        shared_device = self.param_groups[0]['params'][0].device\n",
                "        return torch.norm(torch.stack([\n",
                "                ((torch.abs(p) if group['adaptive'] else 1.0) * p.grad).norm(p=2).to(shared_device)\n",
                "                for group in self.param_groups for p in group['params'] if p.grad is not None\n",
                "            ]), p=2)\n",
                "    def step(self):\n",
                "        raise NotImplementedError\n",
                "\n",
                "class TabRModule(nn.Module):\n",
                "    def __init__(self, input_dim, num_classes, context_size=96):\n",
                "        super().__init__()\n",
                "        self.encoder = nn.Sequential(nn.Linear(input_dim, 128), nn.SiLU(), nn.Linear(128, context_size))\n",
                "        self.q_proj = nn.Linear(context_size, context_size)\n",
                "        self.k_proj = nn.Linear(context_size, context_size)\n",
                "        self.v_proj = nn.Linear(context_size, context_size)\n",
                "        self.head = nn.Sequential(nn.Linear(context_size, 64), nn.SiLU(), nn.Linear(64, num_classes))\n",
                "        self.scale = context_size ** -0.5\n",
                "    def forward(self, x, neighbors):\n",
                "        q = self.encoder(x).unsqueeze(1)\n",
                "        B, K, D = neighbors.shape\n",
                "        kv = self.encoder(neighbors.view(B * K, D)).view(B, K, -1)\n",
                "        scores = torch.bmm(self.q_proj(q), self.k_proj(kv).transpose(1, 2)) * self.scale\n",
                "        context = torch.bmm(F.softmax(scores, dim=-1), self.v_proj(kv)).squeeze(1)\n",
                "        return self.head(context + q.squeeze(1))\n",
                "\n",
                "class TrueTabR(BaseEstimator, ClassifierMixin):\n",
                "    def __init__(self, num_classes, n_neighbors=16):\n",
                "        self.num_classes, self.n_neighbors = num_classes, n_neighbors\n",
                "        self.model, self.knn, self.X_train_ = None, None, None\n",
                "    def fit(self, X, y, sample_weight=None):\n",
                "        self.X_train_ = np.array(X, dtype=np.float32)\n",
                "        self.knn = NearestNeighbors(n_neighbors=self.n_neighbors, n_jobs=-1).fit(self.X_train_)\n",
                "        train_neighbor_idx = self.knn.kneighbors(self.X_train_, return_distance=False)\n",
                "        self.model = TabRModule(X.shape[1], self.num_classes).to(DEVICE)\n",
                "        opt = optim.AdamW(self.model.parameters(), lr=LR_SCALE)\n",
                "        \n",
                "        y_is_soft = (y.ndim > 1) or np.issubdtype(y.dtype, np.floating)\n",
                "        class_w = None\n",
                "        if USE_CLASS_BALANCED:\n",
                "            y_eff = np.argmax(y, axis=1) if y_is_soft else y\n",
                "            cw = compute_class_balanced_weights(y_eff, self.num_classes, beta=CB_BETA)\n",
                "            class_w = torch.tensor(cw, dtype=torch.float32, device=DEVICE)\n",
                "        \n",
                "        X_t = torch.tensor(X, dtype=torch.float32)\n",
                "        y_t = torch.tensor(y, dtype=torch.float32 if y_is_soft else torch.long)\n",
                "        idx_t = torch.arange(len(X_t), dtype=torch.long)\n",
                "        \n",
                "        if sample_weight is None:\n",
                "            dl = DataLoader(TensorDataset(X_t, y_t, idx_t), batch_size=BATCH_SIZE, shuffle=True)\n",
                "        else:\n",
                "            w_t = torch.tensor(np.asarray(sample_weight, dtype=np.float32))\n",
                "            dl = DataLoader(TensorDataset(X_t, y_t, idx_t, w_t), batch_size=BATCH_SIZE, shuffle=True)\n",
                "            \n",
                "        self.model.train()\n",
                "        for _ in range(15):\n",
                "            for batch in dl:\n",
                "                if sample_weight is None: xb, yb, ib = batch; wb = None\n",
                "                else: xb, yb, ib, wb = batch\n",
                "                \n",
                "                nx = self.X_train_[train_neighbor_idx[ib.numpy()]]\n",
                "                logits = self.model(xb.to(DEVICE), torch.tensor(nx, dtype=torch.float32).to(DEVICE))\n",
                "                \n",
                "                if y_is_soft:\n",
                "                    if LOSS_NAME == 'focal':\n",
                "                         loss = soft_target_focal(logits, yb.to(DEVICE), gamma=FOCAL_GAMMA, class_weights=class_w)\n",
                "                    else:\n",
                "                         loss = soft_target_ce(logits, yb.to(DEVICE), class_weights=class_w)\n",
                "                else:\n",
                "                    if LOSS_NAME == 'focal':\n",
                "                        y_onehot = F.one_hot(yb.to(DEVICE), num_classes=self.num_classes).float()\n",
                "                        y_onehot = smooth_targets(y_onehot, LABEL_SMOOTHING)\n",
                "                        loss_vec = soft_target_focal(logits, y_onehot, gamma=FOCAL_GAMMA, class_weights=class_w)\n",
                "                        loss = loss_vec if wb is None else (loss_vec * wb.to(DEVICE)).mean()\n",
                "                    else:\n",
                "                        loss_vec = F.cross_entropy(logits, yb.to(DEVICE), reduction='none', weight=class_w, label_smoothing=LABEL_SMOOTHING)\n",
                "                        loss = loss_vec.mean() if wb is None else (loss_vec * wb.to(DEVICE)).mean()\n",
                "                        \n",
                "                opt.zero_grad(); loss.backward(); opt.step()\n",
                "        return self\n",
                "\n",
                "    def finetune_on_pseudo(self, X_pseudo, y_pseudo, epochs=1, lr_mult=0.2):\n",
                "        if X_pseudo is None or len(X_pseudo) == 0: return self\n",
                "        self.model.train(); lr = float(LR_SCALE) * float(lr_mult)\n",
                "        opt = optim.AdamW(self.model.parameters(), lr=lr)\n",
                "        y_is_soft = (y_pseudo.ndim > 1) or np.issubdtype(y_pseudo.dtype, np.floating)\n",
                "        \n",
                "        X_t = torch.tensor(np.asarray(X_pseudo, dtype=np.float32))\n",
                "        y_t = torch.tensor(np.asarray(y_pseudo, dtype=np.float32 if y_is_soft else np.int64))\n",
                "        \n",
                "        dl = DataLoader(TensorDataset(X_t, y_t), batch_size=BATCH_SIZE, shuffle=True)\n",
                "        for _ in range(int(epochs)):\n",
                "            for xb, yb in dl:\n",
                "                xb = xb.to(DEVICE); yb = yb.to(DEVICE)\n",
                "                nx = self.X_train_[self.knn.kneighbors(xb.detach().cpu().numpy(), return_distance=False)]\n",
                "                logits = self.model(xb, torch.tensor(nx, dtype=torch.float32).to(DEVICE))\n",
                "                \n",
                "                if y_is_soft:\n",
                "                    if LOSS_NAME == 'focal': loss = soft_target_focal(logits, yb, gamma=FOCAL_GAMMA)\n",
                "                    else: loss = soft_target_ce(logits, yb)\n",
                "                else:\n",
                "                    if LOSS_NAME == 'focal':\n",
                "                        y_oh = F.one_hot(yb, num_classes=self.num_classes).float()\n",
                "                        loss = soft_target_focal(logits, y_oh, gamma=FOCAL_GAMMA)\n",
                "                    else:\n",
                "                        loss = F.cross_entropy(logits, yb)\n",
                "                opt.zero_grad(); loss.backward(); opt.step()\n",
                "        return self\n",
                "    def predict_proba(self, X):\n",
                "        self.model.eval(); p = []\n",
                "        with torch.no_grad():\n",
                "            for i in range(0, len(X), BATCH_SIZE):\n",
                "                xb = X[i:i+BATCH_SIZE]\n",
                "                nx = self.X_train_[self.knn.kneighbors(xb, return_distance=False)]\n",
                "                p.append(torch.softmax(self.model(torch.tensor(xb, dtype=torch.float32).to(DEVICE), torch.tensor(nx).to(DEVICE)), dim=1).cpu().numpy())\n",
                "        return np.vstack(p)\n",
                "\n",
                "class ThetaTabM(BaseEstimator, ClassifierMixin):\n",
                "    def __init__(self, input_dim=None, num_classes=None):\n",
                "        self.input_dim = input_dim\n",
                "        self.num_classes = num_classes\n",
                "        self.model = None\n",
                "\n",
                "    def _build_model(self, dim):\n",
                "        return nn.Sequential(\n",
                "            nn.Linear(dim, 256), nn.LayerNorm(256), nn.SiLU(), nn.Dropout(0.2),\n",
                "            nn.Linear(256, 128), nn.LayerNorm(128), nn.SiLU(), nn.Linear(128, int(self.num_classes)),\n",
                "        ).to(DEVICE)\n",
                "\n",
                "    def fit(self, X, y, sample_weight=None):\n",
                "        if self.num_classes is None: self.num_classes = len(np.unique(y))\n",
                "        if self.model is None: self.model = self._build_model(X.shape[1])\n",
                "        \n",
                "        opt = SAM(self.model.parameters(), optim.AdamW, lr=LR_SCALE, rho=SAM_RHO)\n",
                "        y_is_soft = (y.ndim > 1) or np.issubdtype(y.dtype, np.floating)\n",
                "        class_w = None\n",
                "        if USE_CLASS_BALANCED:\n",
                "            y_eff = np.argmax(y, axis=1) if y_is_soft else y\n",
                "            cw = compute_class_balanced_weights(y_eff, self.num_classes, beta=CB_BETA)\n",
                "            class_w = torch.tensor(cw, dtype=torch.float32, device=DEVICE)\n",
                "                \n",
                "        self.model.train(); swa_model = None\n",
                "        if ENABLE_SWA:\n",
                "            try:\n",
                "                from torch.optim.swa_utils import AveragedModel\n",
                "                swa_model = AveragedModel(self.model)\n",
                "            except Exception: swa_model = None\n",
                "            \n",
                "        for ep in range(20):\n",
                "            use_mixup_local = USE_MIXUP and (sample_weight is None) and (not y_is_soft)\n",
                "            if use_mixup_local:\n",
                "                iterator = TopologyMixUpLoader(X, y, num_classes=self.num_classes, batch_size=BATCH_SIZE)\n",
                "                for xb, yb in iterator:\n",
                "                    yb = smooth_targets(yb, LABEL_SMOOTHING)\n",
                "                    opt.zero_grad(); logits = self.model(xb)\n",
                "                    loss = soft_target_ce(logits, yb, class_weights=class_w)\n",
                "                    loss.backward(); opt.first_step(zero_grad=True)\n",
                "                    logits2 = self.model(xb)\n",
                "                    loss2 = soft_target_ce(logits2, yb, class_weights=class_w)\n",
                "                    loss2.backward(); opt.second_step(zero_grad=True); opt.base_optimizer.step()\n",
                "            else:\n",
                "                w_arr = np.ones(len(X), dtype=np.float32) if sample_weight is None else np.asarray(sample_weight, dtype=np.float32)\n",
                "                X_t = torch.tensor(X, dtype=torch.float32).to(DEVICE)\n",
                "                w_t = torch.tensor(w_arr, dtype=torch.float32).to(DEVICE)\n",
                "                y_t = torch.tensor(y, dtype=torch.float32 if y_is_soft else torch.long).to(DEVICE)\n",
                "                dl = DataLoader(TensorDataset(X_t, y_t, w_t), batch_size=BATCH_SIZE, shuffle=True)\n",
                "                for xb, yb, wb in dl:\n",
                "                    opt.zero_grad(); logits = self.model(xb)\n",
                "                    if y_is_soft:\n",
                "                        loss = soft_target_focal(logits, yb, gamma=FOCAL_GAMMA, class_weights=class_w) if LOSS_NAME == 'focal' else soft_target_ce(logits, yb, class_weights=class_w)\n",
                "                    else:\n",
                "                        loss_vec = F.cross_entropy(logits, yb, reduction='none', weight=class_w, label_smoothing=LABEL_SMOOTHING)\n",
                "                        loss = (loss_vec * wb).mean()\n",
                "                    loss.backward(); opt.first_step(zero_grad=True)\n",
                "                    logits2 = self.model(xb)\n",
                "                    if y_is_soft:\n",
                "                        loss2 = soft_target_focal(logits2, yb, gamma=FOCAL_GAMMA, class_weights=class_w) if LOSS_NAME == 'focal' else soft_target_ce(logits2, yb, class_weights=class_w)\n",
                "                    else:\n",
                "                         loss2_vec = F.cross_entropy(logits2, yb, reduction='none', weight=class_w, label_smoothing=LABEL_SMOOTHING)\n",
                "                         loss2 = (loss2_vec * wb).mean()\n",
                "                    loss2.backward(); opt.second_step(zero_grad=True); opt.base_optimizer.step()\n",
                "            \n",
                "            if swa_model is not None and ep >= int(SWA_START_EPOCH):\n",
                "                swa_model.update_parameters(self.model)\n",
                "                \n",
                "        if swa_model is not None and int(SWA_START_EPOCH) < 20:\n",
                "            self.model.load_state_dict(swa_model.module.state_dict())\n",
                "        return self\n",
                "\n",
                "    def finetune_on_pseudo(self, X_pseudo, y_pseudo, epochs=1, lr_mult=0.2):\n",
                "        if X_pseudo is None or len(X_pseudo) == 0: return self\n",
                "        self.model.train(); lr = float(LR_SCALE) * float(lr_mult)\n",
                "        opt = optim.AdamW(self.model.parameters(), lr=lr)\n",
                "        y_is_soft = (y_pseudo.ndim > 1) or np.issubdtype(y_pseudo.dtype, np.floating)\n",
                "        X_t = torch.tensor(np.asarray(X_pseudo, dtype=np.float32))\n",
                "        y_t = torch.tensor(np.asarray(y_pseudo, dtype=np.float32 if y_is_soft else np.int64))\n",
                "        dl = DataLoader(TensorDataset(X_t, y_t), batch_size=BATCH_SIZE, shuffle=True)\n",
                "        for _ in range(int(epochs)):\n",
                "            for xb, yb in dl:\n",
                "                xb = xb.to(DEVICE); yb = yb.to(DEVICE)\n",
                "                logits = self.model(xb)\n",
                "                if y_is_soft:\n",
                "                    loss = soft_target_focal(logits, yb, gamma=FOCAL_GAMMA) if LOSS_NAME == 'focal' else soft_target_ce(logits, yb)\n",
                "                else:\n",
                "                    loss = F.cross_entropy(logits, yb)\n",
                "                opt.zero_grad(); loss.backward(); opt.step()\n",
                "        return self\n",
                "\n",
                "    def predict_proba(self, X):\n",
                "        self.model.eval(); p = []\n",
                "        with torch.no_grad():\n",
                "            for i in range(0, len(X), BATCH_SIZE):\n",
                "                p.append(torch.softmax(self.model(torch.tensor(X[i:i+BATCH_SIZE], dtype=torch.float32).to(DEVICE)), dim=1).cpu().numpy())\n",
                "        return np.vstack(p)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3de730be",
            "metadata": {},
            "source": [
                "**\u039a\u03b5\u03bb\u03af 14: \u039d\u03b5\u03c5\u03c1\u03c9\u03bd\u03b9\u03ba\u03ac \u039c\u03bf\u03bd\u03c4\u03ad\u03bb\u03b1 (Neural Models)**\n",
                "\n",
                "\u03a5\u03bb\u03bf\u03c0\u03bf\u03b9\u03b5\u03af \u03c0\u03c1\u03bf\u03b7\u03b3\u03bc\u03ad\u03bd\u03b1 \u03bd\u03b5\u03c5\u03c1\u03c9\u03bd\u03b9\u03ba\u03ac \u03b4\u03af\u03ba\u03c4\u03c5\u03b1 \u03b3\u03b9\u03b1 tabular \u03b4\u03b5\u03b4\u03bf\u03bc\u03ad\u03bd\u03b1:\n",
                "\n",
                "**TrueTabR (Retrieval-Augmented):**\n",
                "- \u03a7\u03c1\u03b7\u03c3\u03b9\u03bc\u03bf\u03c0\u03bf\u03b9\u03b5\u03af attention \u03c0\u03ac\u03bd\u03c9 \u03c3\u03b5 K-nearest neighbors\n",
                "- Query: encoding \u03c4\u03bf\u03c5 \u03c4\u03c1\u03ad\u03c7\u03bf\u03bd\u03c4\u03bf\u03c2 sample\n",
                "- Keys/Values: encodings \u03c4\u03c9\u03bd \u03b3\u03b5\u03b9\u03c4\u03cc\u03bd\u03c9\u03bd\n",
                "- \u0391\u03be\u03b9\u03bf\u03c0\u03bf\u03b9\u03b5\u03af \u03c4\u03bf\u03c0\u03b9\u03ba\u03ae \u03b4\u03bf\u03bc\u03ae \u03c4\u03c9\u03bd \u03b4\u03b5\u03b4\u03bf\u03bc\u03ad\u03bd\u03c9\u03bd\n",
                "\n",
                "**ThetaTabM (MLP with SAM):**\n",
                "- Sharpness-Aware Minimization \u03b3\u03b9\u03b1 \u03ba\u03b1\u03bb\u03cd\u03c4\u03b5\u03c1\u03b7 \u03b3\u03b5\u03bd\u03af\u03ba\u03b5\u03c5\u03c3\u03b7\n",
                "- Topology-aware MixUp \u03b2\u03b1\u03c3\u03b9\u03c3\u03bc\u03ad\u03bd\u03bf \u03c3\u03b5 KNN\n",
                "- LayerNorm \u03ba\u03b1\u03b9 SiLU activations\n",
                "- \u03a5\u03c0\u03bf\u03c3\u03c4\u03b7\u03c1\u03af\u03b6\u03b5\u03b9 Focal loss \u03ba\u03b1\u03b9 label smoothing\n",
                "\n",
                "**\u039a\u03bf\u03b9\u03bd\u03ac \u03c7\u03b1\u03c1\u03b1\u03ba\u03c4\u03b7\u03c1\u03b9\u03c3\u03c4\u03b9\u03ba\u03ac:**\n",
                "- Optional class-balanced weights\n",
                "- Finetune on pseudo-labels \u03b3\u03b9\u03b1 TTT (Test-Time Training)\n",
                "- Optional SWA (Stochastic Weight Averaging)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**\u039a\u03b5\u03bb\u03af 15a: Test-Time Training (Entropy Minimization)**\n",
                "\n",
                "\u03a5\u03bb\u03bf\u03c0\u03bf\u03b9\u03b5\u03af \u03c4\u03b7\u03bd \u03ba\u03bb\u03ac\u03c3\u03b7 `EntropyMinimizationTTT`:\n",
                "- \u03a0\u03c1\u03bf\u03c3\u03b1\u03c1\u03bc\u03cc\u03b6\u03b5\u03b9 \u03c4\u03bf \u03bc\u03bf\u03bd\u03c4\u03ad\u03bb\u03bf \u03c3\u03c4\u03b1 \u03b4\u03b5\u03b4\u03bf\u03bc\u03ad\u03bd\u03b1 \u03b5\u03bb\u03ad\u03b3\u03c7\u03bf\u03c5 (test data) \u03b5\u03bb\u03b1\u03c7\u03b9\u03c3\u03c4\u03bf\u03c0\u03bf\u03b9\u03ce\u03bd\u03c4\u03b1\u03c2 \u03c4\u03b7\u03bd \u03b5\u03bd\u03c4\u03c1\u03bf\u03c0\u03af\u03b1 \u03c4\u03c9\u03bd \u03c0\u03c1\u03bf\u03b2\u03bb\u03ad\u03c8\u03b5\u03c9\u03bd\n",
                "- \u0391\u03c5\u03be\u03ac\u03bd\u03b5\u03b9 \u03c4\u03b7\u03bd \u03b1\u03c5\u03c4\u03bf\u03c0\u03b5\u03c0\u03bf\u03af\u03b8\u03b7\u03c3\u03b7 \u03c4\u03bf\u03c5 \u03bc\u03bf\u03bd\u03c4\u03ad\u03bb\u03bf\u03c5 \u03c3\u03b5 unlabeled \u03b4\u03b5\u03b4\u03bf\u03bc\u03ad\u03bd\u03b1\n",
                "- \u03a7\u03c1\u03b7\u03c3\u03b9\u03bc\u03bf\u03c0\u03bf\u03b9\u03b5\u03af\u03c4\u03b1\u03b9 \u03b4\u03c5\u03bd\u03b1\u03bc\u03b9\u03ba\u03ac \u03ba\u03b1\u03c4\u03ac \u03c4\u03b7\u03bd \u03c0\u03c1\u03cc\u03b2\u03bb\u03b5\u03c8\u03b7"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# sigma_omega: ttt\n",
                "import copy\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "\n",
                "class EntropyMinimizationTTT:\n",
                "    def __init__(self, steps=1, lr=1e-4, optimizer_type='adam'):\n",
                "        self.steps = steps\n",
                "        self.lr = lr\n",
                "        self.optimizer_type = optimizer_type\n",
                "\n",
                "    def adapt(self, model, x_test_batch):\n",
                "        adapted_model = copy.deepcopy(model)\n",
                "        adapted_model.train()\n",
                "        if self.optimizer_type == 'adam':\n",
                "            optimizer = optim.Adam(adapted_model.parameters(), lr=self.lr)\n",
                "        else:\n",
                "            optimizer = optim.SGD(adapted_model.parameters(), lr=self.lr, momentum=0.9)\n",
                "        for _ in range(self.steps):\n",
                "            optimizer.zero_grad()\n",
                "            outputs = adapted_model(x_test_batch)\n",
                "            log_probs = torch.log_softmax(outputs, dim=1)\n",
                "            probs = torch.exp(log_probs)\n",
                "            entropy = -(probs * log_probs).sum(dim=1).mean()\n",
                "            entropy.backward()\n",
                "            optimizer.step()\n",
                "        return adapted_model\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a1a2207e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# sigma_omega: calibration\n",
                "import copy\n",
                "import numpy as np\n",
                "from sklearn.isotonic import IsotonicRegression\n",
                "from sklearn.model_selection import StratifiedKFold\n",
                "\n",
                "class CalibratedModel:\n",
                "    def __init__(self, base_model, name):\n",
                "        self.base, self.name, self.ir = base_model, name, None\n",
                "\n",
                "    def fit(self, X, y, sample_weight=None, pseudo_X=None, pseudo_y=None, pseudo_w=None):\n",
                "        skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
                "        self.models = []\n",
                "        self.calibrators = []\n",
                "        for tr_idx, val_idx in skf.split(X, y):\n",
                "            X_tr, X_val = X[tr_idx], X[val_idx]\n",
                "            y_tr, y_val = y[tr_idx], y[val_idx]\n",
                "            sw_tr = sample_weight[tr_idx] if sample_weight is not None else None\n",
                "            \n",
                "            if pseudo_X is not None and len(pseudo_X) > 0:\n",
                "                is_pseudo_soft = (pseudo_y.ndim > 1) or np.issubdtype(pseudo_y.dtype, np.floating)\n",
                "                is_torch = hasattr(self.base, 'finetune_on_pseudo')\n",
                "                \n",
                "                # Align types\n",
                "                p_X_eff = pseudo_X\n",
                "                p_y_eff = pseudo_y\n",
                "                \n",
                "                if is_pseudo_soft and not is_torch:\n",
                "                    # Tree -> Hard\n",
                "                    p_y_eff = np.argmax(pseudo_y, axis=1).astype(np.int64) if pseudo_y.ndim > 1 else pseudo_y.astype(np.int64)\n",
                "                elif is_pseudo_soft and is_torch:\n",
                "                    # Torch -> Soft\n",
                "                    # Also need training data to be soft one-hot if not already\n",
                "                    if y_tr.ndim == 1:\n",
                "                        num_classes = pseudo_y.shape[1]\n",
                "                        y_tr = np.eye(num_classes, dtype=np.float32)[y_tr]\n",
                "                        \n",
                "                X_tr = np.vstack([X_tr, p_X_eff])\n",
                "                y_tr = np.concatenate([y_tr, p_y_eff]) if y_tr.ndim == p_y_eff.ndim else np.vstack([y_tr, p_y_eff])\n",
                "                \n",
                "                if sw_tr is None and pseudo_w is not None:\n",
                "                    sw_tr = np.ones(len(y_tr) - len(p_y_eff), dtype=np.float32)\n",
                "                    sw_tr = np.concatenate([sw_tr, np.asarray(pseudo_w, dtype=np.float32)])\n",
                "                elif sw_tr is not None and pseudo_w is not None:\n",
                "                    sw_tr = np.concatenate([sw_tr, np.asarray(pseudo_w, dtype=np.float32)])\n",
                "\n",
                "            model = copy.deepcopy(self.base)\n",
                "            try:\n",
                "                model.fit(X_tr, y_tr, sample_weight=sw_tr)\n",
                "            except TypeError:\n",
                "                model.fit(X_tr, y_tr)\n",
                "                \n",
                "            val_probs = model.predict_proba(X_val).astype(np.float32)\n",
                "            c_list = []\n",
                "            # Calibrate per class using Isotonic Regression\n",
                "            # Note: Isotonic expects 1D arrays. If y_val is soft (for torch model training compatibility), we need hard labels for calibration truth.\n",
                "            y_val_hard = np.argmax(y_val, axis=1) if (y_val.ndim > 1) else y_val\n",
                "            \n",
                "            for c in range(val_probs.shape[1]):\n",
                "                iso = IsotonicRegression(out_of_bounds='clip')\n",
                "                iso.fit(val_probs[:, c], (y_val_hard == c).astype(int))\n",
                "                c_list.append(iso)\n",
                "            self.models.append(model)\n",
                "            self.calibrators.append(c_list)\n",
                "        return self\n",
                "\n",
                "    def predict_proba(self, X):\n",
                "        total_probs = np.zeros((len(X), len(self.calibrators[0])))\n",
                "        for model, calib_list in zip(self.models, self.calibrators):\n",
                "            raw_p = model.predict_proba(X)\n",
                "            cal_p = np.zeros_like(raw_p)\n",
                "            for c in range(raw_p.shape[1]):\n",
                "                cal_p[:, c] = calib_list[c].predict(raw_p[:, c])\n",
                "            cal_p /= (cal_p.sum(axis=1, keepdims=True) + 1e-10)\n",
                "            total_probs += cal_p\n",
                "        return total_probs / len(self.models)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1e087d93",
            "metadata": {},
            "source": [
                "**\u039a\u03b5\u03bb\u03af 15: Calibration (\u0392\u03b1\u03b8\u03bc\u03bf\u03bd\u03cc\u03bc\u03b7\u03c3\u03b7 \u03a0\u03b9\u03b8\u03b1\u03bd\u03bf\u03c4\u03ae\u03c4\u03c9\u03bd)**\n",
                "\n",
                "Wrapper \u03ba\u03bb\u03ac\u03c3\u03b7 `CalibratedModel` \u03b3\u03b9\u03b1 \u03b2\u03b1\u03b8\u03bc\u03bf\u03bd\u03cc\u03bc\u03b7\u03c3\u03b7 \u03c0\u03b9\u03b8\u03b1\u03bd\u03bf\u03c4\u03ae\u03c4\u03c9\u03bd:\n",
                "\n",
                "**\u039c\u03b5\u03b8\u03bf\u03b4\u03bf\u03bb\u03bf\u03b3\u03af\u03b1:**\n",
                "- 10-fold stratified cross-validation\n",
                "- \u0395\u03ba\u03c0\u03b1\u03af\u03b4\u03b5\u03c5\u03c3\u03b7 Isotonic Regression \u03b1\u03bd\u03ac \u03ba\u03bb\u03ac\u03c3\u03b7 \u03c3\u03c4\u03b9\u03c2 OOF \u03c0\u03b9\u03b8\u03b1\u03bd\u03cc\u03c4\u03b7\u03c4\u03b5\u03c2\n",
                "- \u039c\u03ad\u03c3\u03bf\u03c2 \u03cc\u03c1\u03bf\u03c2 calibrated \u03c0\u03b9\u03b8\u03b1\u03bd\u03bf\u03c4\u03ae\u03c4\u03c9\u03bd \u03b1\u03c0\u03cc \u03cc\u03bb\u03b1 \u03c4\u03b1 folds\n",
                "\n",
                "**\u03a0\u03bb\u03b5\u03bf\u03bd\u03b5\u03ba\u03c4\u03ae\u03bc\u03b1\u03c4\u03b1:**\n",
                "- \u0394\u03b9\u03bf\u03c1\u03b8\u03ce\u03bd\u03b5\u03b9 overconfident \u03ae underconfident \u03c0\u03c1\u03bf\u03b2\u03bb\u03ad\u03c8\u03b5\u03b9\u03c2\n",
                "- \u0392\u03b5\u03bb\u03c4\u03b9\u03ce\u03bd\u03b5\u03b9 \u03b1\u03be\u03b9\u03bf\u03c0\u03b9\u03c3\u03c4\u03af\u03b1 \u03c0\u03b9\u03b8\u03b1\u03bd\u03bf\u03c4\u03ae\u03c4\u03c9\u03bd \u03b3\u03b9\u03b1 downstream tasks\n",
                "- \u03a5\u03c0\u03bf\u03c3\u03c4\u03b7\u03c1\u03af\u03b6\u03b5\u03b9 \u03b5\u03bd\u03c3\u03c9\u03bc\u03ac\u03c4\u03c9\u03c3\u03b7 pseudo-labeled \u03b4\u03b5\u03b9\u03b3\u03bc\u03ac\u03c4\u03c9\u03bd"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f78cff74",
            "metadata": {},
            "outputs": [],
            "source": [
                "# sigma_omega: stacking\n",
                "import copy\n",
                "import numpy as np\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.model_selection import StratifiedKFold\n",
                "# helpers from losses\n",
                "# requires prob_meta_features\n",
                "\n",
                "def tabpfn_predict_proba(X_train, y_train, X_eval, n_ensembles=32, seed=42, device='cpu'):\n",
                "    try:\n",
                "        from tabpfn import TabPFNClassifier\n",
                "    except Exception: raise RuntimeError(\"TabPFN missing\")\n",
                "    model = TabPFNClassifier(device=str(device), N_ensemble_configurations=int(n_ensembles), seed=int(seed))\n",
                "    model.fit(X_train, y_train)\n",
                "    return model.predict_proba(X_eval).astype(np.float32)\n",
                "\n",
                "def fit_predict_stacking(\n",
                "    names_models,\n",
                "    view_name,\n",
                "    X_train_base,\n",
                "    X_test_base,\n",
                "    y,\n",
                "    num_classes,\n",
                "    cv_splits=10,\n",
                "    seed=42,\n",
                "    sample_weight=None,\n",
                "    pseudo_idx=None,\n",
                "    pseudo_y=None,\n",
                "    pseudo_w=None,\n",
                "):\n",
                "    skf = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=seed)\n",
                "    n_models = len(names_models) + (1 if USE_TABPFN else 0)\n",
                "    oof_preds = [np.zeros((len(X_train_base), num_classes), dtype=np.float32) for _ in range(n_models)]\n",
                "    test_preds_running = [np.zeros((len(X_test_base), num_classes), dtype=np.float32) for _ in range(n_models)]\n",
                "    \n",
                "    model_map = {name: i for i, (name, _) in enumerate(names_models)}\n",
                "    if USE_TABPFN: model_map['TabPFN'] = len(names_models)\n",
                "\n",
                "    for fold_i, (tr_idx, val_idx) in enumerate(skf.split(X_train_base, y)):\n",
                "        X_tr_raw, X_val_raw = X_train_base[tr_idx], X_train_base[val_idx]\n",
                "        y_tr, y_val = y[tr_idx], y[val_idx]\n",
                "        sw_tr = sample_weight[tr_idx] if sample_weight is not None else None\n",
                "        \n",
                "        # Cross-Fit: Transform inside fold\n",
                "        X_tr_view, X_val_view = apply_feature_view(X_tr_raw, X_val_raw, view=view_name, seed=seed, allow_transductive=ALLOW_TRANSDUCTIVE)\n",
                "        _, X_test_view_fold = apply_feature_view(X_tr_raw, X_test_base, view=view_name, seed=seed, allow_transductive=ALLOW_TRANSDUCTIVE)\n",
                "        \n",
                "        X_tree_tr, X_tree_val, X_neural_tr, X_neural_val, _, _ = build_streams(X_tr_view, X_val_view)\n",
                "        _, X_tree_te_fold, _, X_neural_te_fold, _, _ = build_streams(X_tr_view, X_test_view_fold)\n",
                "        \n",
                "        pX_tree = None; pX_neural = None; py = pseudo_y; pw = pseudo_w\n",
                "        if pseudo_idx is not None and len(pseudo_idx) > 0:\n",
                "            pX_tree = X_tree_te_fold[pseudo_idx]\n",
                "            pX_neural = X_neural_te_fold[pseudo_idx]\n",
                "\n",
                "        for name, base_template in names_models:\n",
                "            idx_m = model_map[name]\n",
                "            model = copy.deepcopy(base_template)\n",
                "            is_tree = ('XGB' in name or 'Cat' in name)\n",
                "            X_f_tr = X_tree_tr if is_tree else X_neural_tr\n",
                "            X_f_val = X_tree_val if is_tree else X_neural_val\n",
                "            X_f_te = X_tree_te_fold if is_tree else X_neural_te_fold\n",
                "            pX_f = pX_tree if is_tree else pX_neural\n",
                "            \n",
                "            X_train_final = X_f_tr; y_train_final = y_tr; w_train_final = sw_tr\n",
                "            \n",
                "            if pX_f is not None:\n",
                "                is_pseudo_soft = (py.ndim > 1) or np.issubdtype(py.dtype, np.floating)\n",
                "                is_torch = hasattr(model, 'finetune_on_pseudo')\n",
                "                y_tr_eff = y_train_final; py_eff = py\n",
                "                \n",
                "                if is_pseudo_soft and not is_torch:\n",
                "                    py_eff = np.argmax(py, axis=1).astype(np.int64) if py.ndim > 1 else py.astype(np.int64)\n",
                "                elif is_pseudo_soft and is_torch:\n",
                "                    if y_tr_eff.ndim == 1: y_tr_eff = np.eye(num_classes, dtype=np.float32)[y_tr_eff]\n",
                "                    \n",
                "                X_train_final = np.vstack([X_f_tr, pX_f])\n",
                "                y_train_final = np.concatenate([y_tr_eff, py_eff]) if y_tr_eff.ndim == py_eff.ndim else np.vstack([y_tr_eff, py_eff])\n",
                "                \n",
                "                w_tr_base = w_train_final if w_train_final is not None else np.ones(len(y_tr), dtype=np.float32)\n",
                "                w_p_base = pw if pw is not None else np.ones(len(py), dtype=np.float32)\n",
                "                w_train_final = np.concatenate([w_tr_base, w_p_base])\n",
                "\n",
                "            try: model.fit(X_train_final, y_train_final, sample_weight=w_train_final)\n",
                "            except TypeError: model.fit(X_train_final, y_train_final)\n",
                "            \n",
                "            oof_preds[idx_m][val_idx] = model.predict_proba(X_f_val).astype(np.float32)\n",
                "            test_preds_running[idx_m] += model.predict_proba(X_f_te).astype(np.float32)\n",
                "\n",
                "        if USE_TABPFN:\n",
                "            idx_m = model_map['TabPFN']\n",
                "            # TabPFN on cleaned fold, no pseudo\n",
                "            p_oof = tabpfn_predict_proba(X_tr_view, y_tr, X_val_view, n_ensembles=TABPFN_N_ENSEMBLES, device=DEVICE)\n",
                "            oof_preds[idx_m][val_idx] = p_oof\n",
                "            p_te = tabpfn_predict_proba(X_tr_view, y_tr, X_test_view_fold, n_ensembles=TABPFN_N_ENSEMBLES, device=DEVICE)\n",
                "            test_preds_running[idx_m] += p_te\n",
                "\n",
                "    for i in range(n_models): test_preds_running[i] /= cv_splits\n",
                "    \n",
                "    meta_X = np.hstack(oof_preds)\n",
                "    meta_te = np.hstack(test_preds_running)\n",
                "    meta_feat_oof = [prob_meta_features(oof) for oof in oof_preds]\n",
                "    meta_feat_te = [prob_meta_features(p) for p in test_preds_running]\n",
                "    meta_X = np.hstack([meta_X] + meta_feat_oof)\n",
                "    meta_te = np.hstack([meta_te] + meta_feat_te)\n",
                "\n",
                "    if META_LEARNER == 'lgbm':\n",
                "        try:\n",
                "             from lightgbm import LGBMClassifier\n",
                "             meta = LGBMClassifier(objective='multiclass', num_class=int(num_classes), n_estimators=100, verbosity=-1)\n",
                "        except: meta = LogisticRegression(max_iter=2000, multi_class='multinomial')\n",
                "    else:\n",
                "        meta = LogisticRegression(max_iter=2000, multi_class='multinomial')\n",
                "        \n",
                "    meta.fit(meta_X, y, sample_weight=sample_weight)\n",
                "    return meta.predict_proba(meta_te)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "aafde68f",
            "metadata": {},
            "source": [
                "**\u039a\u03b5\u03bb\u03af 16: Stacking & Meta-Learning**\n",
                "\n",
                "\u03a5\u03bb\u03bf\u03c0\u03bf\u03b9\u03b5\u03af stacking ensemble \u03bc\u03b5 \u03b4\u03b9\u03ac\u03c6\u03bf\u03c1\u03b5\u03c2 \u03b5\u03c0\u03b9\u03bb\u03bf\u03b3\u03ad\u03c2 meta-learner:\n",
                "\n",
                "**\u0394\u03b9\u03b1\u03b4\u03b9\u03ba\u03b1\u03c3\u03af\u03b1:**\n",
                "1. \u0394\u03b7\u03bc\u03b9\u03bf\u03c5\u03c1\u03b3\u03af\u03b1 OOF (Out-of-Fold) predictions \u03b3\u03b9\u03b1 \u03ba\u03ac\u03b8\u03b5 base model\n",
                "2. \u0395\u03be\u03b1\u03b3\u03c9\u03b3\u03ae meta-features \u03b1\u03c0\u03cc \u03c0\u03b9\u03b8\u03b1\u03bd\u03cc\u03c4\u03b7\u03c4\u03b5\u03c2 (top1, gap, entropy)\n",
                "3. \u0395\u03ba\u03c0\u03b1\u03af\u03b4\u03b5\u03c5\u03c3\u03b7 meta-learner \u03c3\u03c4\u03b1 OOF\n",
                "4. \u03a0\u03c1\u03cc\u03b2\u03bb\u03b5\u03c8\u03b7 \u03c3\u03c4\u03bf test set\n",
                "\n",
                "**Meta-Learner \u0395\u03c0\u03b9\u03bb\u03bf\u03b3\u03ad\u03c2:**\n",
                "- `lr`: Logistic Regression (default)\n",
                "- `lgbm`: LightGBM Classifier\n",
                "- `moe`: Mixture of Experts (gating network \u03b5\u03c0\u03b9\u03bb\u03ad\u03b3\u03b5\u03b9 \u03c4\u03bf\u03bd \u03ba\u03b1\u03bb\u03cd\u03c4\u03b5\u03c1\u03bf expert)\n",
                "\n",
                "**Optional TabPFN:**\n",
                "- In-context learning model \u03b3\u03b9\u03b1 tabular \u03b4\u03b5\u03b4\u03bf\u03bc\u03ad\u03bd\u03b1\n",
                "- \u0394\u03b5\u03bd \u03b1\u03c0\u03b1\u03b9\u03c4\u03b5\u03af hyperparameter tuning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2ef808c6",
            "metadata": {},
            "outputs": [],
            "source": [
                "# sigma_omega: pipeline\n",
                "import numpy as np\n",
                "\n",
                "def predict_probs_for_view(view, seed, X_train_base, X_test_base, y_enc, num_classes, pseudo_idx=None, pseudo_y=None, pseudo_w=None,\n",
                "                           allow_transductive=False, enable_coral=False, coral_reg=1e-3,\n",
                "                           enable_adv=False, adv_model='lr', adv_clip=10.0, adv_power=1.0,\n",
                "                           use_stacking=False, enable_lid_scaling=False, lid_t_min=1.0, lid_t_max=2.5, lid_t_power=1.0,\n",
                "                           enable_ttt=False, ttt_gap_low=0.10, ttt_gap_high=0.35, ttt_max_samples=4096, ttt_epochs=1, ttt_lr_mult=0.2,\n",
                "                           manifold_k=20, enable_pagerank=True, dae_batch=2048, dae_lr=2e-3, dae_noise=0.1, dae_epochs=30):\n",
                "    \n",
                "    pseudo = normalize_pseudo(pseudo_idx=pseudo_idx, pseudo_y=pseudo_y, pseudo_w=pseudo_w)\n",
                "    sample_weight = None\n",
                "    if enable_adv:\n",
                "        # Note: adversarial weights computed on features. Which features? \n",
                "        # Usually raw or view-transformed. Let's use view transformed inside loop or raw here?\n",
                "        # Domain.py uses raw-ish features usually. Let's allow raw for simplicity.\n",
                "        sample_weight = adversarial_weights(X_train_base, X_test_base, seed=seed, model=adv_model, clip=adv_clip, power=adv_power)\n",
                "\n",
                "    names_models = [\n",
                "        ('XGB_DART', get_xgb_dart(num_classes)),\n",
                "        ('Cat_Langevin', get_cat_langevin(num_classes)),\n",
                "        ('ThetaTabM', ThetaTabM(None, num_classes)), # Lazy init\n",
                "        ('TrueTabR', TrueTabR(num_classes)),\n",
                "    ]\n",
                "\n",
                "    if use_stacking:\n",
                "        # Cross-Fit Stacking\n",
                "        p = fit_predict_stacking(\n",
                "            names_models=names_models,\n",
                "            view_name=view,\n",
                "            X_train_base=X_train_base,\n",
                "            X_test_base=X_test_base,\n",
                "            y=y_enc,\n",
                "            num_classes=num_classes,\n",
                "            cv_splits=10,\n",
                "            seed=seed,\n",
                "            sample_weight=sample_weight,\n",
                "            pseudo_idx=pseudo.idx if pseudo.active() else None,\n",
                "            pseudo_y=pseudo.y if pseudo.active() else None,\n",
                "            pseudo_w=pseudo.w if pseudo.active() else None,\n",
                "        )\n",
                "        return p\n",
                "\n",
                "    # Standard Ensemble\n",
                "    X_v, X_test_v = apply_feature_view(X_train_base, X_test_base, view=view, seed=seed, allow_transductive=allow_transductive, enable_coral=enable_coral, coral_reg=coral_reg)\n",
                "    X_tree_tr, X_tree_te, X_neural_tr, X_neural_te, lid_tr, lid_te = build_streams(\n",
                "        X_v, X_test_v, allow_transductive=allow_transductive, manifold_k=manifold_k, enable_pagerank=enable_pagerank,\n",
                "        dae_epochs=dae_epochs, batch_size=dae_batch, lr_scale=dae_lr, noise_std=dae_noise\n",
                "    )\n",
                "    \n",
                "    pseudo_X_tree = X_tree_te[pseudo.idx] if pseudo.active() else None\n",
                "    pseudo_X_neural = X_neural_te[pseudo.idx] if pseudo.active() else None\n",
                "    \n",
                "    view_probs = 0\n",
                "    for name, base in names_models:\n",
                "        print(f'  > Calibrating {name} (10-Fold)...')\n",
                "        data_tr = X_tree_tr if ('XGB' in name or 'Cat' in name) else X_neural_tr\n",
                "        data_te = X_tree_te if ('XGB' in name or 'Cat' in name) else X_neural_te\n",
                "        \n",
                "        calibrated = CalibratedModel(base, name)\n",
                "        calibrated.fit(\n",
                "            data_tr, y_enc,\n",
                "            sample_weight=sample_weight,\n",
                "            pseudo_X=pseudo_X_tree if ('XGB' in name or 'Cat' in name) else pseudo_X_neural,\n",
                "            pseudo_y=pseudo.y if pseudo.active() else None,\n",
                "            pseudo_w=pseudo.w if pseudo.active() else None,\n",
                "        )\n",
                "        p = calibrated.predict_proba(data_te)\n",
                "        \n",
                "        if enable_ttt and is_torch_model(base):\n",
                "            # Reflexion Core Upgrade: Entropy Minimization TTT\n",
                "             if not allow_transductive:\n",
                "                 raise RuntimeError(\"ENABLE_TTT requires ALLOW_TRANSDUCTIVE=1\")\n",
                "             \n",
                "             ttt_solver = EntropyMinimizationTTT(steps=ttt_epochs, lr=ttt_lr_mult * 1e-4)\n",
                "             device = next(base.model.parameters()).device\n",
                "             X_test_tensor = torch.tensor(data_te, dtype=torch.float32, device=device)\n",
                "             \n",
                "             adapted = ttt_solver.adapt(base.model, X_test_tensor)\n",
                "             adapted.eval()\n",
                "             with torch.no_grad():\n",
                "                 logits = adapted(X_test_tensor)\n",
                "                 p = torch.softmax(logits, dim=1).cpu().numpy()\n",
                "\n",
                "        if enable_lid_scaling:\n",
                "            p = apply_lid_temperature_scaling(p, lid_te, t_min=lid_t_min, t_max=lid_t_max, power=lid_t_power)\n",
                "        view_probs += p\n",
                "    return view_probs / len(names_models)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "be1217f1",
            "metadata": {},
            "source": [
                "**\u039a\u03b5\u03bb\u03af 17: Per-View Pipeline**\n",
                "\n",
                "\u039a\u03b5\u03bd\u03c4\u03c1\u03b9\u03ba\u03ae \u03c3\u03c5\u03bd\u03ac\u03c1\u03c4\u03b7\u03c3\u03b7 `predict_probs_for_view()` \u03c0\u03bf\u03c5 \u03b5\u03bd\u03bf\u03c1\u03c7\u03b7\u03c3\u03c4\u03c1\u03ce\u03bd\u03b5\u03b9:\n",
                "\n",
                "1. **Feature Transform**: \u0395\u03c6\u03b1\u03c1\u03bc\u03bf\u03b3\u03ae \u03c4\u03bf\u03c5 \u03b5\u03c0\u03b9\u03bb\u03b5\u03b3\u03bc\u03ad\u03bd\u03bf\u03c5 view (raw, quantile, \u03ba\u03bb\u03c0.)\n",
                "2. **Stream Building**: \u0394\u03b7\u03bc\u03b9\u03bf\u03c5\u03c1\u03b3\u03af\u03b1 tree/neural streams \u03bc\u03b5 DAE embeddings \u03ba\u03b1\u03b9 manifold features\n",
                "3. **Adversarial Weights**: \u03a5\u03c0\u03bf\u03bb\u03bf\u03b3\u03b9\u03c3\u03bc\u03cc\u03c2 \u03b2\u03b1\u03c1\u03ce\u03bd \u03b1\u03c0\u03cc domain discriminator\n",
                "4. **Model Training**: \u0395\u03ba\u03c0\u03b1\u03af\u03b4\u03b5\u03c5\u03c3\u03b7 XGBoost, CatBoost, ThetaTabM, TabR\n",
                "5. **Calibration**: \u0392\u03b1\u03b8\u03bc\u03bf\u03bd\u03cc\u03bc\u03b7\u03c3\u03b7 \u03c0\u03b9\u03b8\u03b1\u03bd\u03bf\u03c4\u03ae\u03c4\u03c9\u03bd \u03bc\u03b5 isotonic regression\n",
                "6. **Optional Stacking**: Meta-learner \u03c0\u03ac\u03bd\u03c9 \u03c3\u03c4\u03b1 OOF\n",
                "7. **Optional TTT**: Test-Time Training \u03c3\u03b5 uncertain samples\n",
                "8. **LID Scaling**: Temperature scaling \u03b2\u03ac\u03c3\u03b5\u03b9 \u03c4\u03bf\u03c0\u03b9\u03ba\u03ae\u03c2 \u03c0\u03bf\u03bb\u03c5\u03c0\u03bb\u03bf\u03ba\u03cc\u03c4\u03b7\u03c4\u03b1\u03c2\n",
                "\n",
                "**\u0395\u03c0\u03b9\u03c3\u03c4\u03c1\u03ad\u03c6\u03b5\u03b9:** \u039c\u03ad\u03c3\u03bf \u03cc\u03c1\u03bf \u03c0\u03b9\u03b8\u03b1\u03bd\u03bf\u03c4\u03ae\u03c4\u03c9\u03bd \u03b1\u03c0\u03cc \u03cc\u03bb\u03b1 \u03c4\u03b1 \u03bc\u03bf\u03bd\u03c4\u03ad\u03bb\u03b1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3b65f1ec",
            "metadata": {},
            "outputs": [],
            "source": [
                "# main entrypoint\n",
                "import os\n",
                "import numpy as np\n",
                "import torch\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "\n",
                "def partd_main():\n",
                "    print('>>> INITIATING PART D GRANDMASTER PROTOCOL <<<')\n",
                "    X, y, X_test = load_data_safe()\n",
                "    le = LabelEncoder(); y_enc = le.fit_transform(y); num_classes = len(le.classes_)\n",
                "\n",
                "    # Razor\n",
                "    print('[RAZOR] Scanning for noise features...')\n",
                "    from catboost import CatBoostClassifier\n",
                "    scout = CatBoostClassifier(iterations=500, verbose=0, task_type='GPU' if torch.cuda.is_available() else 'CPU')\n",
                "    scout.fit(X, y_enc)\n",
                "    imps = scout.get_feature_importance(); thresh = np.percentile(imps, 20)\n",
                "    keep_mask = imps > thresh\n",
                "    X_razor = X[:, keep_mask]; X_test_razor = X_test[:, keep_mask]\n",
                "    print(f'  > Dropped {np.sum(~keep_mask)} features. New Dim: {X_razor.shape[1]}')\n",
                "\n",
                "    final_ensemble_probs = 0\n",
                "    if ENABLE_SELF_TRAIN and SELF_TRAIN_ITERS > 0:\n",
                "        if not ALLOW_TRANSDUCTIVE: raise RuntimeError('ENABLE_SELF_TRAIN requires ALLOW_TRANSDUCTIVE=1')\n",
                "        pseudo = PseudoData.empty(); last_avg_probs = None\n",
                "        for it in range(int(SELF_TRAIN_ITERS) + 1):\n",
                "            print(f'\\n>>> SELF-TRAIN ITERATION {it} (pseudo={len(pseudo.idx)}) <<<')\n",
                "            probs_per_view = {v: [] for v in VIEWS}; preds_per_view = {v: [] for v in VIEWS}\n",
                "            for seed in SEEDS:\n",
                "                seed_everything(seed)\n",
                "                for view in VIEWS:\n",
                "                    p = predict_probs_for_view(\n",
                "                        view, seed, X_razor, X_test_razor, y_enc, num_classes,\n",
                "                        pseudo_idx=pseudo.idx, pseudo_y=pseudo.y, pseudo_w=pseudo.w,\n",
                "                        allow_transductive=ALLOW_TRANSDUCTIVE, enable_coral=ENABLE_CORAL, coral_reg=CORAL_REG,\n",
                "                        enable_adv=ENABLE_ADV_REWEIGHT, adv_model=ADV_MODEL, adv_clip=ADV_CLIP, adv_power=ADV_POWER,\n",
                "                        use_stacking=USE_STACKING, enable_lid_scaling=ENABLE_LID_SCALING,\n",
                "                        lid_t_min=LID_T_MIN, lid_t_max=LID_T_MAX, lid_t_power=LID_T_POWER,\n",
                "                        enable_ttt=ENABLE_TTT, ttt_gap_low=TTT_GAP_LOW, ttt_gap_high=TTT_GAP_HIGH, ttt_max_samples=TTT_MAX_SAMPLES,\n",
                "                        ttt_epochs=TTT_EPOCHS, ttt_lr_mult=TTT_LR_MULT,\n",
                "                        manifold_k=MANIFOLD_K, enable_pagerank=ENABLE_PAGERANK,\n",
                "                        dae_batch=BATCH_SIZE, dae_lr=LR_SCALE, dae_noise=DAE_NOISE_STD, dae_epochs=DAE_EPOCHS,\n",
                "                    )\n",
                "                    probs_per_view[view].append(p); preds_per_view[view].append(np.argmax(p, axis=1))\n",
                "            \n",
                "            probs_tensor = np.stack([np.stack(probs_per_view[v], axis=0) for v in VIEWS], axis=0)\n",
                "            preds_tensor = np.stack([np.stack(preds_per_view[v], axis=0) for v in VIEWS], axis=0)\n",
                "            avg_probs = probs_tensor.mean(axis=(0, 1)); last_avg_probs = avg_probs\n",
                "            \n",
                "            if it < int(SELF_TRAIN_ITERS):\n",
                "                votes = preds_tensor.reshape(preds_tensor.shape[0] * preds_tensor.shape[1], preds_tensor.shape[2])\n",
                "                mode_pred, agree_frac_votes = vote_mode_and_agreement(votes)\n",
                "                view_agree_frac = view_agreement_fraction(preds_tensor, mode_pred)\n",
                "                conf = np.max(avg_probs, axis=1)\n",
                "                mask = (conf >= float(SELF_TRAIN_CONF)) & (agree_frac_votes >= float(SELF_TRAIN_AGREE)) & (view_agree_frac >= float(SELF_TRAIN_VIEW_AGREE))\n",
                "                idx = np.nonzero(mask)[0]\n",
                "                if idx.size > int(SELF_TRAIN_MAX):\n",
                "                    top = np.argsort(conf[idx])[::-1][: int(SELF_TRAIN_MAX)]; idx = idx[top]\n",
                "                \n",
                "                # Soft Pseudo-Labels\n",
                "                pseudo_idx = idx.astype(np.int64)\n",
                "                pseudo_y = avg_probs[pseudo_idx] # Soft targets\n",
                "                pseudo_w = np.power(conf[pseudo_idx].astype(np.float32), float(SELF_TRAIN_WEIGHT_POWER))\n",
                "                pseudo = PseudoData(idx=pseudo_idx, y=pseudo_y, w=pseudo_w)\n",
                "                print(f'  [SELF-TRAIN] mined {len(pseudo_idx)} pseudo')\n",
                "        final_ensemble_probs = last_avg_probs\n",
                "    else:\n",
                "        for seed in SEEDS:\n",
                "            print(f'\\n>>> SEQUENCE START: SEED {seed} <<<')\n",
                "            seed_everything(seed)\n",
                "            view_probs_total = 0; view_count = 0\n",
                "            for view in VIEWS:\n",
                "                print(f'  [VIEW] {view}')\n",
                "                if USE_STACKING: print('  > Stacking meta-learner (OOF -> meta)...')\n",
                "                view_probs_total += predict_probs_for_view(\n",
                "                    view, seed, X_razor, X_test_razor, y_enc, num_classes,\n",
                "                    allow_transductive=ALLOW_TRANSDUCTIVE, enable_coral=ENABLE_CORAL, coral_reg=CORAL_REG,\n",
                "                    enable_adv=ENABLE_ADV_REWEIGHT, adv_model=ADV_MODEL, adv_clip=ADV_CLIP, adv_power=ADV_POWER,\n",
                "                    use_stacking=USE_STACKING, enable_lid_scaling=ENABLE_LID_SCALING,\n",
                "                    lid_t_min=LID_T_MIN, lid_t_max=LID_T_MAX, lid_t_power=LID_T_POWER,\n",
                "                    enable_ttt=ENABLE_TTT, ttt_gap_low=TTT_GAP_LOW, ttt_gap_high=TTT_GAP_HIGH, ttt_max_samples=TTT_MAX_SAMPLES,\n",
                "                    ttt_epochs=TTT_EPOCHS, ttt_lr_mult=TTT_LR_MULT,\n",
                "                    manifold_k=MANIFOLD_K, enable_pagerank=ENABLE_PAGERANK,\n",
                "                    dae_batch=BATCH_SIZE, dae_lr=LR_SCALE, dae_noise=DAE_NOISE_STD, dae_epochs=DAE_EPOCHS,\n",
                "                )\n",
                "                view_count += 1\n",
                "            final_ensemble_probs += (view_probs_total / max(1, view_count))\n",
                "        final_ensemble_probs /= len(SEEDS)\n",
                "        \n",
                "    preds = np.argmax(final_ensemble_probs, axis=1); labels = le.inverse_transform(preds)\n",
                "    os.makedirs('PartD/outputs', exist_ok=True)\n",
                "    np.save('PartD/outputs/labelsX_grandmaster.npy', labels)\n",
                "    print('\\\\n>>> PART D PROTOCOL COMPLETE <<<')\n",
                "    return labels\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d2cadce4",
            "metadata": {},
            "source": [
                "**\u039a\u03b5\u03bb\u03af 18: \u039a\u03cd\u03c1\u03b9\u03b1 \u0395\u03bd\u03bf\u03c1\u03c7\u03ae\u03c3\u03c4\u03c1\u03c9\u03c3\u03b7 (Main Orchestration)**\n",
                "\n",
                "\u0397 \u03c3\u03c5\u03bd\u03ac\u03c1\u03c4\u03b7\u03c3\u03b7 `partd_main()` \u03b5\u03ba\u03c4\u03b5\u03bb\u03b5\u03af \u03c4\u03bf \u03c0\u03bb\u03ae\u03c1\u03b5\u03c2 pipeline:\n",
                "\n",
                "**\u0392\u03ae\u03bc\u03b1\u03c4\u03b1:**\n",
                "1. **\u03a6\u03cc\u03c1\u03c4\u03c9\u03c3\u03b7 \u0394\u03b5\u03b4\u03bf\u03bc\u03ad\u03bd\u03c9\u03bd**: \u03a7\u03c1\u03ae\u03c3\u03b7 `load_data_safe()`\n",
                "2. **Razor Feature Selection**: \u0391\u03c6\u03b1\u03af\u03c1\u03b5\u03c3\u03b7 \u03c7\u03b1\u03bc\u03b7\u03bb\u03ae\u03c2 \u03c3\u03b7\u03bc\u03b1\u03c3\u03af\u03b1\u03c2 features \u03bc\u03b5 CatBoost importance\n",
                "3. **Multi-Seed Ensembling**: \u0395\u03c0\u03b1\u03bd\u03ac\u03bb\u03b7\u03c8\u03b7 \u03b3\u03b9\u03b1 \u03ba\u03ac\u03b8\u03b5 seed \u03c3\u03c4\u03bf SEEDS\n",
                "4. **Multi-View Ensembling**: \u0395\u03c0\u03b1\u03bd\u03ac\u03bb\u03b7\u03c8\u03b7 \u03b3\u03b9\u03b1 \u03ba\u03ac\u03b8\u03b5 view \u03c3\u03c4\u03bf VIEWS\n",
                "5. **Optional Self-Training**: \u0395\u03c0\u03b1\u03bd\u03b1\u03bb\u03b7\u03c0\u03c4\u03b9\u03ba\u03ae \u03c0\u03c1\u03bf\u03c3\u03b8\u03ae\u03ba\u03b7 confident pseudo-labels\n",
                "6. **Final Prediction**: \u039c\u03ad\u03c3\u03bf\u03c2 \u03cc\u03c1\u03bf\u03c2 \u03c0\u03b9\u03b8\u03b1\u03bd\u03bf\u03c4\u03ae\u03c4\u03c9\u03bd, argmax \u03b3\u03b9\u03b1 \u03ba\u03bb\u03ac\u03c3\u03b7\n",
                "7. **Save Outputs**: \u0391\u03c0\u03bf\u03b8\u03ae\u03ba\u03b5\u03c5\u03c3\u03b7 \u03c3\u03b5 `PartD/outputs/labelsX_grandmaster.npy`\n",
                "\n",
                "**Self-Training Mode:**\n",
                "- \u0391\u03c0\u03b1\u03b9\u03c4\u03b5\u03af `ENABLE_SELF_TRAIN=1` \u03ba\u03b1\u03b9 `ALLOW_TRANSDUCTIVE=1`\n",
                "- \u0395\u03c0\u03b9\u03bb\u03ad\u03b3\u03b5\u03b9 samples \u03bc\u03b5 \u03c5\u03c8\u03b7\u03bb\u03ae confidence \u03ba\u03b1\u03b9 agreement \u03bc\u03b5\u03c4\u03b1\u03be\u03cd \u03bc\u03bf\u03bd\u03c4\u03ad\u03bb\u03c9\u03bd/views\n",
                "- \u0395\u03c0\u03b1\u03bd\u03b1\u03bb\u03b1\u03bc\u03b2\u03ac\u03bd\u03b5\u03b9 \u03bc\u03b5 \u03b5\u03c0\u03b1\u03c5\u03be\u03b7\u03bc\u03ad\u03bd\u03b1 \u03b4\u03b5\u03b4\u03bf\u03bc\u03ad\u03bd\u03b1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ad3dd9bc",
            "metadata": {},
            "outputs": [],
            "source": [
                "**\u039a\u03b5\u03bb\u03af 19: \u0395\u03ba\u03c4\u03ad\u03bb\u03b5\u03c3\u03b7 (Runner)**\n",
                "\n",
                "\u0395\u03ba\u03c4\u03b5\u03bb\u03b5\u03af \u03c4\u03bf \u03b5\u03bd\u03c3\u03c9\u03bc\u03b1\u03c4\u03c9\u03bc\u03ad\u03bd\u03bf pipeline \u03ba\u03b1\u03b9 \u03b1\u03c0\u03bf\u03b8\u03b7\u03ba\u03b5\u03cd\u03b5\u03b9 \u03c4\u03b1 \u03b1\u03c0\u03bf\u03c4\u03b5\u03bb\u03ad\u03c3\u03bc\u03b1\u03c4\u03b1.\n",
                "\n",
                "**\u0391\u03c1\u03c7\u03b5\u03af\u03b1 \u0395\u03be\u03cc\u03b4\u03bf\u03c5:**\n",
                "- `PartD/outputs/labelsX_grandmaster.npy`: \u039a\u03cd\u03c1\u03b9\u03bf \u03b1\u03c1\u03c7\u03b5\u03af\u03bf predictions\n",
                "- `Submission/labels1.npy`: \u0391\u03bd\u03c4\u03af\u03b3\u03c1\u03b1\u03c6\u03bf \u03b3\u03b9\u03b1 submission\n",
                "\n",
                "**\u039f\u03b4\u03b7\u03b3\u03af\u03b5\u03c2 \u03a7\u03c1\u03ae\u03c3\u03b7\u03c2:**\n",
                "- \u0393\u03b9\u03b1 smoke test: `DO_FULL_RUN=False`, `SMOKE_RUN=True`\n",
                "- \u0393\u03b9\u03b1 \u03c0\u03bb\u03ae\u03c1\u03b7 \u03b5\u03ba\u03c4\u03ad\u03bb\u03b5\u03c3\u03b7: `DO_FULL_RUN=True` (\u03b1\u03c0\u03b1\u03b9\u03c4\u03b5\u03af GPU)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}