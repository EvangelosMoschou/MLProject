{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "fbfdd82f",
            "metadata": {},
            "source": [
                "# Machine Learning Project - Part D\n",
                "**Team 1**\n",
                "* Name: Evangelos Moschou\n",
                "* AEM: 10986\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dde9a654",
            "metadata": {},
            "source": [
                "## Μέρος D: Πρόκληση Ταξινόμησης (Το Πρωτόκολλο Epsilon)\n",
                "\n",
                "Αυτό το notebook υλοποιεί μια προηγμένη στρατηγική ensemble μηχανικής μάθησης για ταξινόμηση πολλαπλών κλάσεων. Η προσέγγιση συνδυάζει δενδρικά μοντέλα (XGBoost DART, CatBoost Langevin), νευρωνικά δίκτυα (TabR, ThetaTabM), και προηγμένες τεχνικές όπως domain adaptation, calibration, και stacking."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "51d368a4",
            "metadata": {},
            "source": [
                "**Κελί 3: Ρύθμιση Εκτέλεσης & Διαδρομές**\n",
                "\n",
                "Ορίζει τη ρίζα του αποθετηρίου, τις προσαρμογές διαδρομών και τις προαιρετικές ρυθμίσεις. Εκτελέστε αυτό το κελί πρώτο για να προετοιμάσετε το περιβάλλον για τα επόμενα κελιά.\n",
                "\n",
                "**Σημαντικές Μεταβλητές:**\n",
                "- `DO_FULL_RUN`: Ενεργοποιεί πλήρη εκτέλεση του pipeline (απαιτεί GPU)\n",
                "- `SMOKE_RUN`: Γρήγορη δοκιμαστική εκτέλεση για επαλήθευση λειτουργίας"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "78474ab6",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "from pathlib import Path\n",
                "\n",
                "import numpy as np\n",
                "\n",
                "\n",
                "def _find_repo_root(start: Path) -> Path:\n",
                "    for p in [start] + list(start.parents):\n",
                "        if (p / 'PartD').exists() and (p / 'Datasets').exists():\n",
                "            return p\n",
                "    raise FileNotFoundError('Could not locate repo root (expected PartD/ and Datasets/).')\n",
                "\n",
                "\n",
                "root = _find_repo_root(Path.cwd())\n",
                "\n",
                "# ----------------------\n",
                "# Notebook-runner knobs\n",
                "# ----------------------\n",
                "# Set DO_FULL_RUN = True to run the full pipeline (may be long / require GPU).\n",
                "# By default the cell runs a *smoke* configuration that is quick and safe.\n",
                "DO_FULL_RUN = False\n",
                "SMOKE_RUN = True\n",
                "\n",
                "# Data loader (self-contained fallback to CSVs if project `src` loader is not importable)\n",
                "\n",
                "def load_data_local():\n",
                "    try:\n",
                "        # Prefer repository data loader if available\n",
                "        from src.data_loader import load_data as ld\n",
                "\n",
                "        X, y, X_test = ld()\n",
                "        return X, y, X_test\n",
                "    except Exception:\n",
                "        import pandas as pd\n",
                "\n",
                "        train_path = root / 'Datasets' / 'datasetTV.csv'\n",
                "        test_path = root / 'Datasets' / 'datasetTest.csv'\n",
                "        if not train_path.exists():\n",
                "            raise FileNotFoundError('Expected datasetTV.csv in Datasets/')\n",
                "        train_df = pd.read_csv(train_path, header=None)\n",
                "        test_df = pd.read_csv(test_path, header=None)\n",
                "        X = train_df.iloc[:, :-1].values\n",
                "        y = train_df.iloc[:, -1].values\n",
                "        X_test = test_df.values\n",
                "        return X, y, X_test\n",
                "\n",
                "\n",
                "# Minimal pipeline implementation embedded in the notebook (self-contained)\n",
                "\n",
                "def run_partd_in_notebook(do_full=False, smoke=True):\n",
                "    print('Part D runner (embedded in notebook). do_full=%r smoke=%r' % (do_full, smoke))\n",
                "\n",
                "    X, y, X_test = load_data_local()\n",
                "    print('Loaded data:', X.shape, 'labels:', np.unique(y).size, 'test:', X_test.shape)\n",
                "\n",
                "    # Quick feature transform\n",
                "    try:\n",
                "        from sklearn.preprocessing import QuantileTransformer\n",
                "    except Exception:\n",
                "        raise RuntimeError('scikit-learn is required for the notebook runner.')\n",
                "\n",
                "    qt = QuantileTransformer(output_distribution='normal', random_state=42)\n",
                "    X_q = qt.fit_transform(X)\n",
                "    X_test_q = qt.transform(X_test)\n",
                "\n",
                "    # Simple \"DAE\" placeholder for notebook: use PCA embedding for speed/safety\n",
                "    try:\n",
                "        from sklearn.decomposition import PCA\n",
                "\n",
                "        pca = PCA(n_components=min(32, X_q.shape[1]))\n",
                "        emb_tr = pca.fit_transform(X_q)\n",
                "        emb_te = pca.transform(X_test_q)\n",
                "    except Exception:\n",
                "        emb_tr = np.zeros((X_q.shape[0], 0))\n",
                "        emb_te = np.zeros((X_test_q.shape[0], 0))\n",
                "\n",
                "    X_final_tr = np.hstack([X_q, emb_tr])\n",
                "    X_final_te = np.hstack([X_test_q, emb_te])\n",
                "\n",
                "    # Razor: quick feature importance using CatBoost if available, else use simple variance filter\n",
                "    try:\n",
                "        from catboost import CatBoostClassifier\n",
                "\n",
                "        cb = CatBoostClassifier(iterations=50 if smoke else 800, verbose=False, task_type='GPU' if os.environ.get('CUDA_VISIBLE_DEVICES') else 'CPU')\n",
                "        cb.fit(X_final_tr, y)\n",
                "        imp = np.array(cb.get_feature_importance())\n",
                "        keep = imp > np.percentile(imp, 20)\n",
                "        print('CatBoost razor kept', keep.sum(), 'features')\n",
                "    except Exception:\n",
                "        print('CatBoost not available or failed — falling back to variance filter')\n",
                "        var = np.var(X_final_tr, axis=0)\n",
                "        keep = var > np.percentile(var, 20)\n",
                "        print('Variance razor kept', keep.sum(), 'features')\n",
                "\n",
                "    X_final_tr = X_final_tr[:, keep]\n",
                "    X_final_te = X_final_te[:, keep]\n",
                "\n",
                "    # Model: use CatBoost when available; else LogisticRegression\n",
                "    if not do_full:\n",
                "        # smoke training: small iterations / quick solver\n",
                "        try:\n",
                "            from catboost import CatBoostClassifier\n",
                "\n",
                "            clf = CatBoostClassifier(iterations=50, verbose=False, task_type='GPU' if os.environ.get('CUDA_VISIBLE_DEVICES') else 'CPU')\n",
                "            clf.fit(X_final_tr, y)\n",
                "            probs = clf.predict_proba(X_final_te)\n",
                "        except Exception:\n",
                "            from sklearn.linear_model import LogisticRegression\n",
                "\n",
                "            clf = LogisticRegression(max_iter=2000)\n",
                "            clf.fit(X_final_tr, y)\n",
                "            probs = clf.predict_proba(X_final_te)\n",
                "    else:\n",
                "        # Full pipeline (attempt to mirror sigma_omega): train ensemble and stacking (best-effort)\n",
                "        try:\n",
                "            from catboost import CatBoostClassifier\n",
                "            from sklearn.ensemble import RandomForestClassifier\n",
                "            from sklearn.linear_model import LogisticRegression\n",
                "            from sklearn.model_selection import StratifiedKFold\n",
                "\n",
                "            # simple ensemble of a CatBoost + RandomForest\n",
                "            cb = CatBoostClassifier(iterations=800, verbose=False, task_type='GPU' if os.environ.get('CUDA_VISIBLE_DEVICES') else 'CPU')\n",
                "            rf = RandomForestClassifier(n_estimators=200, max_depth=7, random_state=42)\n",
                "\n",
                "            cb.fit(X_final_tr, y)\n",
                "            rf.fit(X_final_tr, y)\n",
                "\n",
                "            p_cb = cb.predict_proba(X_final_te)\n",
                "            p_rf = rf.predict_proba(X_final_te)\n",
                "\n",
                "            probs = (p_cb + p_rf) / 2.0\n",
                "\n",
                "            # simple stacking to match prior behavior\n",
                "            oof = np.zeros((X_final_tr.shape[0], probs.shape[1]))\n",
                "            skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
                "            stack_features = []\n",
                "            for tr, va in skf.split(X_final_tr, y):\n",
                "                cb_local = CatBoostClassifier(iterations=200, verbose=False, task_type='GPU' if os.environ.get('CUDA_VISIBLE_DEVICES') else 'CPU')\n",
                "                cb_local.fit(X_final_tr[tr], y[tr])\n",
                "                oof[va] = cb_local.predict_proba(X_final_tr[va])\n",
                "            meta = LogisticRegression(max_iter=2000)\n",
                "            meta.fit(oof, y)\n",
                "            probs = meta.predict_proba(probs)\n",
                "\n",
                "        except Exception as e:\n",
                "            print('Full run fallback due to:', e)\n",
                "            from sklearn.linear_model import LogisticRegression\n",
                "\n",
                "            clf = LogisticRegression(max_iter=2000)\n",
                "            clf.fit(X_final_tr, y)\n",
                "            probs = clf.predict_proba(X_final_te)\n",
                "\n",
                "    preds = np.argmax(probs, axis=1)\n",
                "\n",
                "    # Save outputs (mirror legacy naming)\n",
                "    out_dir = root / 'PartD' / 'outputs'\n",
                "    out_dir.mkdir(parents=True, exist_ok=True)\n",
                "    out_path = out_dir / 'labelsX_grandmaster.npy'\n",
                "    np.save(out_path, preds)\n",
                "    print('Saved predictions to', out_path)\n",
                "\n",
                "    # also copy into Submission\n",
                "    sub_path = root / 'Submission' / 'labels1.npy'\n",
                "    np.save(sub_path, preds)\n",
                "    print('Saved submission copy to', sub_path)\n",
                "\n",
                "    return preds\n",
                "\n",
                "\n",
                "# Execute (safe defaults)\n",
                "if __name__ == '__main__':\n",
                "    if DO_FULL_RUN:\n",
                "        run_partd_in_notebook(do_full=True, smoke=False)\n",
                "    elif SMOKE_RUN:\n",
                "        run_partd_in_notebook(do_full=False, smoke=True)\n",
                "    else:\n",
                "        print('Notebook runner defined as run_partd_in_notebook(do_full=False, smoke=True). Set DO_FULL_RUN=True to run the full pipeline.')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0d1e21bd",
            "metadata": {},
            "source": [
                "# Διαγνωστικό: Επαλήθευση ότι ο runner είναι αυτόνομος (δεν απαιτούνται εξωτερικά imports)\n",
                "from pathlib import Path\n",
                "print('Αυτόνομος runner υπάρχει; ', 'run_partd_in_notebook' in globals())\n",
                "print('Τοποθεσία πηγαίου κώδικα runner: ενσωματωμένος σε αυτό το notebook')\n",
                "print('Σημείωση: Αυτό το διαγνωστικό δεν εισάγει πλέον το sigma_omega· το notebook είναι ανεξάρτητο.')\n",
                "print('\\nΓια γρήγορη δοκιμαστική εκτέλεση: ορίστε DO_FULL_RUN=False, SMOKE_RUN=True στο κελί του runner.')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bd2919a7",
            "metadata": {},
            "source": [
                "**Κελί 4: Πλαίσιο Notebook & Οδηγίες**\n",
                "\n",
                "Σύντομες σημειώσεις για τη δομή του notebook και τον τρόπο αποθήκευσης των αποτελεσμάτων.\n",
                "\n",
                "**Δομή Notebook:**\n",
                "- Κελιά ρύθμισης → Βοηθητικές συναρτήσεις → Φόρτωση δεδομένων → Μηχανική χαρακτηριστικών → Μοντέλα → Calibration → Stacking → Εκτέλεση"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "63552efe",
            "metadata": {},
            "source": [
                "**Κελί 5: Ολοκληρωμένο Pipeline Μέρους D - Επισκόπηση**\n",
                "\n",
                "Αυτό το κελί εξηγεί ότι όλα τα συστατικά είναι ενσωματωμένα inline. Εκτελέστε τα κελιά με σειρά για να ορίσετε:\n",
                "\n",
                "1. **Ρύθμιση (config)**: Υπερπαράμετροι, device, seeds\n",
                "2. **Βοηθητικά (utilities)**: Seeding για αναπαραγωγιμότητα\n",
                "3. **Φόρτωση δεδομένων**: CSV fallback αν ο project loader δεν είναι διαθέσιμος\n",
                "4. **Pseudo-labeling**: Δομές για ημι-εποπτευόμενη μάθηση\n",
                "5. **Συναρτήσεις απώλειας**: CE, Focal, class-balanced weights\n",
                "6. **Domain adaptation**: Adversarial reweighting, CORAL\n",
                "7. **Feature views & DAE**: Μετασχηματισμοί χαρακτηριστικών και Denoising Autoencoder\n",
                "8. **Μοντέλα**: Δενδρικά (XGBoost, CatBoost) και νευρωνικά (TabR, ThetaTabM)\n",
                "9. **Calibration & Stacking**: Isotonic calibration και meta-learners"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "571da2cc",
            "metadata": {},
            "outputs": [],
            "source": [
                "# sigma_omega: config\n",
                "import os\n",
                "import warnings\n",
                "import numpy as np\n",
                "import torch\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "def _env_bool(name, default=False):\n",
                "    v = os.getenv(name)\n",
                "    if v is None:\n",
                "        return default\n",
                "    return v.strip().lower() in {'1', 'true', 'yes', 'y', 'on'}\n",
                "\n",
                "def _env_int(name, default):\n",
                "    v = os.getenv(name)\n",
                "    return int(v) if v is not None and v.strip() != '' else int(default)\n",
                "\n",
                "def _env_float(name, default):\n",
                "    v = os.getenv(name)\n",
                "    return float(v) if v is not None and v.strip() != '' else float(default)\n",
                "\n",
                "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "_seeds_env = os.getenv('SEEDS')\n",
                "if _seeds_env:\n",
                "    SEEDS = [int(s.strip()) for s in _seeds_env.split(',') if s.strip()]\n",
                "else:\n",
                "    _n_seeds = os.getenv('N_SEEDS')\n",
                "    if _n_seeds:\n",
                "        base = int(os.getenv('SEED_BASE', '42'))\n",
                "        n = int(_n_seeds)\n",
                "        SEEDS = [base + i for i in range(n)]\n",
                "    else:\n",
                "        SEEDS = [42, 43, 44, 45, 46]\n",
                "\n",
                "BATCH_SIZE = 2048\n",
                "LR_SCALE = 2e-3\n",
                "SAM_RHO = 0.08\n",
                "ALLOW_TRANSDUCTIVE = _env_bool('ALLOW_TRANSDUCTIVE', False)\n",
                "USE_STACKING = _env_bool('USE_STACKING', False)\n",
                "VIEWS = [v.strip().lower() for v in os.getenv('VIEWS', 'raw,quantile').split(',') if v.strip()]\n",
                "META_LEARNER = os.getenv('META_LEARNER', 'lr').strip().lower()\n",
                "USE_TABPFN = _env_bool('USE_TABPFN', False)\n",
                "TABPFN_N_ENSEMBLES = _env_int('TABPFN_N_ENSEMBLES', 32)\n",
                "LGBM_MAX_DEPTH = _env_int('LGBM_MAX_DEPTH', 3)\n",
                "LGBM_NUM_LEAVES = _env_int('LGBM_NUM_LEAVES', 31)\n",
                "LGBM_N_ESTIMATORS = _env_int('LGBM_N_ESTIMATORS', 400)\n",
                "ENABLE_ADV_REWEIGHT = _env_bool('ENABLE_ADV_REWEIGHT', False)\n",
                "ADV_MODEL = os.getenv('ADV_MODEL', 'lr').strip().lower()\n",
                "ADV_CLIP = _env_float('ADV_CLIP', 10.0)\n",
                "ADV_POWER = _env_float('ADV_POWER', 1.0)\n",
                "ENABLE_SWA = _env_bool('ENABLE_SWA', False)\n",
                "SWA_START_EPOCH = _env_int('SWA_START_EPOCH', 10)\n",
                "ENABLE_CORAL = _env_bool('ENABLE_CORAL', False)\n",
                "CORAL_REG = _env_float('CORAL_REG', 1e-3)\n",
                "ENABLE_SELF_TRAIN = _env_bool('ENABLE_SELF_TRAIN', False)\n",
                "SELF_TRAIN_ITERS = _env_int('SELF_TRAIN_ITERS', 0)\n",
                "SELF_TRAIN_CONF = _env_float('SELF_TRAIN_CONF', 0.92)\n",
                "SELF_TRAIN_AGREE = _env_float('SELF_TRAIN_AGREE', 1.0)\n",
                "SELF_TRAIN_VIEW_AGREE = _env_float('SELF_TRAIN_VIEW_AGREE', 0.66)\n",
                "SELF_TRAIN_MAX = _env_int('SELF_TRAIN_MAX', 10000)\n",
                "SELF_TRAIN_WEIGHT_POWER = _env_float('SELF_TRAIN_WEIGHT_POWER', 1.0)\n",
                "LOSS_NAME = os.getenv('LOSS', 'ce').strip().lower()\n",
                "LABEL_SMOOTHING = _env_float('LABEL_SMOOTHING', 0.0)\n",
                "FOCAL_GAMMA = _env_float('FOCAL_GAMMA', 2.0)\n",
                "USE_CLASS_BALANCED = _env_bool('CLASS_BALANCED', False)\n",
                "CB_BETA = _env_float('CB_BETA', 0.999)\n",
                "USE_MIXUP = _env_bool('USE_MIXUP', True)\n",
                "DAE_EPOCHS = _env_int('DAE_EPOCHS', 30)\n",
                "DAE_NOISE_STD = _env_float('DAE_NOISE_STD', 0.1)\n",
                "MANIFOLD_K = _env_int('MANIFOLD_K', 20)\n",
                "ENABLE_PAGERANK = _env_bool('ENABLE_PAGERANK', True)\n",
                "ENABLE_LID_SCALING = _env_bool('ENABLE_LID_SCALING', False)\n",
                "LID_T_MIN = _env_float('LID_T_MIN', 1.0)\n",
                "LID_T_MAX = _env_float('LID_T_MAX', 2.5)\n",
                "LID_T_POWER = _env_float('LID_T_POWER', 1.0)\n",
                "ENABLE_TTT = _env_bool('ENABLE_TTT', False)\n",
                "TTT_GAP_LOW = _env_float('TTT_GAP_LOW', 0.10)\n",
                "TTT_GAP_HIGH = _env_float('TTT_GAP_HIGH', 0.35)\n",
                "TTT_EPOCHS = _env_int('TTT_EPOCHS', 1)\n",
                "TTT_MAX_SAMPLES = _env_int('TTT_MAX_SAMPLES', 4096)\n",
                "TTT_LR_MULT = _env_float('TTT_LR_MULT', 0.2)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d440cb6e",
            "metadata": {},
            "source": [
                "**Κελί 6: Ρύθμιση Υπερπαραμέτρων (Configuration)**\n",
                "\n",
                "Ορίζει όλες τις υπερπαραμέτρους του Μέρους D, συμπεριλαμβανομένων:\n",
                "\n",
                "- **Feature Views**: raw, quantile, PCA, ICA, random projection, spectral\n",
                "- **Transductive Flags**: Επιτρέπει χρήση test δεδομένων κατά την εκπαίδευση\n",
                "- **Stacking Options**: Meta-learner επιλογές (LR, LGBM, MoE)\n",
                "- **Loss Settings**: Cross-entropy, Focal loss, label smoothing, class balancing\n",
                "- **Self-Training**: Παράμετροι ημι-εποπτευόμενης μάθησης\n",
                "- **DAE Parameters**: Epochs, noise, learning rate για τον autoencoder"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ee896b06",
            "metadata": {},
            "outputs": [],
            "source": [
                "# sigma_omega: utils\n",
                "import os\n",
                "import numpy as np\n",
                "import torch\n",
                "\n",
                "def seed_everything(seed=42):\n",
                "    import random\n",
                "    random.seed(seed)\n",
                "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    if torch.cuda.is_available():\n",
                "        torch.cuda.manual_seed_all(seed)\n",
                "        torch.backends.cudnn.deterministic = True\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "697779fa",
            "metadata": {},
            "source": [
                "**Κελί 7: Βοηθητικές Συναρτήσεις (Utilities)**\n",
                "\n",
                "Βοηθητική συνάρτηση `seed_everything()` για διασφάλιση αναπαραγωγιμότητας.\n",
                "\n",
                "Η συνάρτηση ορίζει seeds για:\n",
                "- Python random module\n",
                "- NumPy random generator\n",
                "- PyTorch (CPU και GPU)\n",
                "- CUDNN deterministic mode"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a52e838a",
            "metadata": {},
            "outputs": [],
            "source": [
                "# sigma_omega: data\n",
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "\n",
                "def load_data_safe():\n",
                "    try:\n",
                "        from src.data_loader import load_data\n",
                "        X, y, X_test = load_data()\n",
                "    except Exception:\n",
                "        root = Path.cwd()\n",
                "        train_path = root / 'Datasets' / 'datasetTV.csv'\n",
                "        test_path = root / 'Datasets' / 'datasetTest.csv'\n",
                "        if not train_path.exists():\n",
                "            raise FileNotFoundError('Expected datasetTV.csv in Datasets/')\n",
                "        train_df = pd.read_csv(train_path, header=None)\n",
                "        test_df = pd.read_csv(test_path, header=None)\n",
                "        X = train_df.iloc[:, :-1].values\n",
                "        y = train_df.iloc[:, -1].values\n",
                "        X_test = test_df.values\n",
                "    if X is None or y is None or X_test is None:\n",
                "        raise ValueError('load_data returned None(s)')\n",
                "    return X, y, X_test\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5bead861",
            "metadata": {},
            "source": [
                "**Κελί 8: Φόρτωση Δεδομένων (Data Loader)**\n",
                "\n",
                "Φορτώνει τα δεδομένα εκπαίδευσης και ελέγχου. Χρησιμοποιεί τον project loader αν είναι διαθέσιμος, αλλιώς επιστρέφει σε απευθείας ανάγνωση CSV από τον φάκελο `Datasets/`.\n",
                "\n",
                "**Επιστρέφει:**\n",
                "- `X`: Χαρακτηριστικά εκπαίδευσης\n",
                "- `y`: Ετικέτες εκπαίδευσης\n",
                "- `X_test`: Χαρακτηριστικά ελέγχου"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0e6e92a2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# sigma_omega: pseudo\n",
                "import numpy as np\n",
                "from dataclasses import dataclass\n",
                "\n",
                "@dataclass(frozen=True)\n",
                "class PseudoData:\n",
                "    idx: np.ndarray\n",
                "    y: np.ndarray\n",
                "    w: np.ndarray\n",
                "    @staticmethod\n",
                "    def empty():\n",
                "        return PseudoData(idx=np.array([], dtype=np.int64), y=np.array([], dtype=np.int64), w=np.array([], dtype=np.float32))\n",
                "    def active(self):\n",
                "        return self.idx is not None and self.y is not None and len(self.idx) > 0\n",
                "\n",
                "def normalize_pseudo(pseudo_idx=None, pseudo_y=None, pseudo_w=None):\n",
                "    if pseudo_idx is None or pseudo_y is None:\n",
                "        return PseudoData.empty()\n",
                "    idx = np.asarray(pseudo_idx, dtype=np.int64)\n",
                "    y = np.asarray(pseudo_y, dtype=np.int64)\n",
                "    if pseudo_w is None:\n",
                "        w = np.ones((len(idx),), dtype=np.float32)\n",
                "    else:\n",
                "        w = np.asarray(pseudo_w, dtype=np.float32)\n",
                "    if len(idx) == 0:\n",
                "        return PseudoData.empty()\n",
                "    return PseudoData(idx=idx, y=y, w=w)\n",
                "\n",
                "def vote_mode_and_agreement(votes_2d):\n",
                "    mode_pred = np.zeros((votes_2d.shape[1],), dtype=np.int64)\n",
                "    agree_frac = np.zeros((votes_2d.shape[1],), dtype=np.float64)\n",
                "    for j in range(votes_2d.shape[1]):\n",
                "        vals, counts = np.unique(votes_2d[:, j], return_counts=True)\n",
                "        k = int(np.argmax(counts))\n",
                "        mode_pred[j] = int(vals[k])\n",
                "        agree_frac[j] = float(np.max(counts)) / float(votes_2d.shape[0])\n",
                "    return mode_pred, agree_frac\n",
                "\n",
                "def view_agreement_fraction(preds_tensor_vs_n, mode_pred):\n",
                "    view_agree_frac = np.zeros((preds_tensor_vs_n.shape[2],), dtype=np.float64)\n",
                "    for vi in range(preds_tensor_vs_n.shape[0]):\n",
                "        view_votes = preds_tensor_vs_n[vi]\n",
                "        view_mode, _ = vote_mode_and_agreement(view_votes)\n",
                "        view_agree_frac += (view_mode == mode_pred).astype(np.float64)\n",
                "    view_agree_frac /= float(preds_tensor_vs_n.shape[0])\n",
                "    return view_agree_frac\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "73ef337b",
            "metadata": {},
            "source": [
                "**Κελί 9: Pseudo-Labels & Ψηφοφορία**\n",
                "\n",
                "Ορίζει δομές και συναρτήσεις για ημι-εποπτευόμενη μάθηση:\n",
                "\n",
                "- **`PseudoData`**: Dataclass για αποθήκευση pseudo-labeled δειγμάτων με βάρη\n",
                "- **`normalize_pseudo()`**: Κανονικοποίηση και επικύρωση pseudo-labels\n",
                "- **`vote_mode_and_agreement()`**: Υπολογισμός πλειοψηφικής ψήφου και ποσοστού συμφωνίας μεταξύ μοντέλων\n",
                "- **`view_agreement_fraction()`**: Υπολογισμός συμφωνίας μεταξύ διαφορετικών feature views"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8fbd46d8",
            "metadata": {},
            "outputs": [],
            "source": [
                "# sigma_omega: losses\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn.functional as F\n",
                "\n",
                "def compute_class_balanced_weights(y, num_classes, beta=0.999):\n",
                "    counts = np.bincount(np.asarray(y, dtype=np.int64), minlength=num_classes).astype(np.float64)\n",
                "    effective = 1.0 - np.power(beta, counts)\n",
                "    weights = (1.0 - beta) / (effective + 1e-12)\n",
                "    weights = weights / (weights.mean() + 1e-12)\n",
                "    return weights.astype(np.float32)\n",
                "\n",
                "def smooth_targets(targets, smoothing):\n",
                "    if smoothing <= 0:\n",
                "        return targets\n",
                "    n_classes = targets.shape[1]\n",
                "    return targets * (1.0 - smoothing) + (smoothing / n_classes)\n",
                "\n",
                "def soft_target_ce(logits, targets, class_weights=None):\n",
                "    log_probs = F.log_softmax(logits, dim=1)\n",
                "    if class_weights is not None:\n",
                "        w = class_weights.view(1, -1)\n",
                "        return -(targets * w * log_probs).sum(dim=1).mean()\n",
                "    return -(targets * log_probs).sum(dim=1).mean()\n",
                "\n",
                "def soft_target_focal(logits, targets, gamma=2.0, class_weights=None):\n",
                "    probs = torch.softmax(logits, dim=1).clamp(1e-8, 1.0 - 1e-8)\n",
                "    logp = torch.log(probs)\n",
                "    mod = torch.pow(1.0 - probs, gamma)\n",
                "    if class_weights is not None:\n",
                "        w = class_weights.view(1, -1)\n",
                "        loss = -(targets * w * mod * logp).sum(dim=1)\n",
                "    else:\n",
                "        loss = -(targets * mod * logp).sum(dim=1)\n",
                "    return loss.mean()\n",
                "\n",
                "def apply_lid_temperature_scaling(probs, lid_norm, t_min=1.0, t_max=2.5, power=1.0):\n",
                "    p = np.asarray(probs, dtype=np.float64)\n",
                "    lid = np.asarray(lid_norm, dtype=np.float64).reshape(-1)\n",
                "    lid = np.clip(lid, 0.0, 1.0)\n",
                "    T = float(t_min) + (float(t_max) - float(t_min)) * np.power(lid, float(power))\n",
                "    T = np.clip(T, 1e-3, 1e6).reshape(-1, 1)\n",
                "    logits = np.log(p + 1e-12)\n",
                "    logits = logits / T\n",
                "    logits = logits - logits.max(axis=1, keepdims=True)\n",
                "    exps = np.exp(logits)\n",
                "    return exps / (exps.sum(axis=1, keepdims=True) + 1e-12)\n",
                "\n",
                "def prob_meta_features(probs, lid=None):\n",
                "    p = np.asarray(probs, dtype=np.float64)\n",
                "    p = np.clip(p, 1e-12, 1.0)\n",
                "    p = p / (p.sum(axis=1, keepdims=True) + 1e-12)\n",
                "    part = np.partition(p, kth=(-1, -2), axis=1)\n",
                "    top1 = part[:, -1]\n",
                "    top2 = part[:, -2]\n",
                "    gap = top1 - top2\n",
                "    entropy = -(p * np.log(p)).sum(axis=1)\n",
                "    feats = np.column_stack([top1, gap, entropy])\n",
                "    if lid is not None:\n",
                "        feats = np.column_stack([feats, np.asarray(lid, dtype=np.float64).reshape(-1)])\n",
                "    return feats.astype(np.float32)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4b2850b0",
            "metadata": {},
            "source": [
                "**Κελί 10: Συναρτήσεις Απώλειας & Meta-Features**\n",
                "\n",
                "Υλοποιεί προηγμένες τεχνικές εκπαίδευσης:\n",
                "\n",
                "- **Class-Balanced Weights**: Αντιστάθμιση για ανισορροπημένες κλάσεις\n",
                "- **Label Smoothing**: Μείωση overfitting μέσω εξομάλυνσης στόχων\n",
                "- **Soft-Target Cross-Entropy**: CE με μαλακούς στόχους\n",
                "- **Focal Loss**: Εστίαση σε δύσκολα δείγματα (γ=2.0)\n",
                "- **LID Temperature Scaling**: Προσαρμογή θερμοκρασίας βάσει Local Intrinsic Dimensionality\n",
                "- **Probability Meta-Features**: Εξαγωγή χαρακτηριστικών από πιθανότητες (top1, gap, entropy)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bcb69a2a",
            "metadata": {},
            "outputs": [],
            "source": [
                "# sigma_omega: domain\n",
                "import numpy as np\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "\n",
                "\n",
                "def adversarial_weights(X_train, X_test, seed=42, model='lr', clip=10.0, power=1.0):\n",
                "    X_all = np.vstack([X_train, X_test])\n",
                "    y_dom = np.concatenate([np.zeros(len(X_train), dtype=np.int64), np.ones(len(X_test), dtype=np.int64)])\n",
                "    if model == 'xgb':\n",
                "        from xgboost import XGBClassifier\n",
                "        clf = XGBClassifier(\n",
                "            n_estimators=300,\n",
                "            max_depth=4,\n",
                "            learning_rate=0.05,\n",
                "            subsample=0.9,\n",
                "            colsample_bytree=0.9,\n",
                "            objective='binary:logistic',\n",
                "            eval_metric='logloss',\n",
                "            tree_method='hist',\n",
                "            random_state=int(seed),\n",
                "            verbosity=0,\n",
                "        )\n",
                "    else:\n",
                "        clf = LogisticRegression(max_iter=2000)\n",
                "    clf.fit(X_all, y_dom)\n",
                "    p_test = clf.predict_proba(X_train)[:, 1].astype(np.float64)\n",
                "    p_test = np.clip(p_test, 1e-6, 1.0 - 1e-6)\n",
                "    w = p_test / (1.0 - p_test)\n",
                "    w = np.power(w, float(power))\n",
                "    w = np.clip(w, 1.0 / float(clip), float(clip))\n",
                "    w = w / (np.mean(w) + 1e-12)\n",
                "    return w.astype(np.float32)\n",
                "\n",
                "\n",
                "def coral_align(X_train, X_test, reg=1e-3):\n",
                "    X_tr = np.asarray(X_train, dtype=np.float64)\n",
                "    X_te = np.asarray(X_test, dtype=np.float64)\n",
                "    X_trc = X_tr - X_tr.mean(axis=0, keepdims=True)\n",
                "    X_tec = X_te - X_te.mean(axis=0, keepdims=True)\n",
                "    cov_tr = (X_trc.T @ X_trc) / max(1, (len(X_trc) - 1))\n",
                "    cov_te = (X_tec.T @ X_tec) / max(1, (len(X_tec) - 1))\n",
                "    reg = float(reg)\n",
                "    cov_tr = cov_tr + reg * np.eye(cov_tr.shape[0])\n",
                "    cov_te = cov_te + reg * np.eye(cov_te.shape[0])\n",
                "    evals_tr, evecs_tr = np.linalg.eigh(cov_tr)\n",
                "    evals_tr = np.clip(evals_tr, 1e-12, None)\n",
                "    W_tr = evecs_tr @ np.diag(1.0 / np.sqrt(evals_tr)) @ evecs_tr.T\n",
                "    evals_te, evecs_te = np.linalg.eigh(cov_te)\n",
                "    evals_te = np.clip(evals_te, 1e-12, None)\n",
                "    C_te = evecs_te @ np.diag(np.sqrt(evals_te)) @ evecs_te.T\n",
                "    A = W_tr @ C_te\n",
                "    X_tr_a = X_trc @ A + X_tr.mean(axis=0, keepdims=True)\n",
                "    return X_tr_a.astype(np.float32), X_te.astype(np.float32)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2110849f",
            "metadata": {},
            "source": [
                "**Κελί 11: Domain Adaptation**\n",
                "\n",
                "Τεχνικές προσαρμογής τομέα για αντιμετώπιση distribution shift:\n",
                "\n",
                "- **Adversarial Reweighting**: Εκπαίδευση discriminator για διάκριση train/test, χρήση πιθανοτήτων ως βάρη για τα training samples. Δίνει μεγαλύτερη σημασία σε training samples που μοιάζουν με test.\n",
                "\n",
                "- **CORAL Alignment**: Ευθυγράμμιση δεύτερης τάξης στατιστικών (covariance matrices) μεταξύ train και test domains. Μετασχηματίζει τα training features ώστε να έχουν παρόμοια κατανομή με τα test."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "df695fa9",
            "metadata": {},
            "outputs": [],
            "source": [
                "# sigma_omega: features\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from sklearn.decomposition import PCA, FastICA\n",
                "from sklearn.manifold import SpectralEmbedding\n",
                "from sklearn.neighbors import NearestNeighbors, kneighbors_graph\n",
                "from sklearn.preprocessing import QuantileTransformer\n",
                "from sklearn.random_projection import GaussianRandomProjection\n",
                "from torch.utils.data import DataLoader, TensorDataset\n",
                "\n",
                "from math import ceil\n",
                "\n",
                "# uses config, domain.coral_align\n",
                "\n",
                "def apply_feature_view(X_train, X_test, view, seed, allow_transductive=False, enable_coral=False, coral_reg=1e-3):\n",
                "    view = (view or 'raw').strip().lower()\n",
                "    def maybe_coral(a, b):\n",
                "        if enable_coral:\n",
                "            if not allow_transductive:\n",
                "                raise ValueError('CORAL requires ALLOW_TRANSDUCTIVE=1')\n",
                "            from math import isfinite\n",
                "            return coral_align(a, b, reg=coral_reg)\n",
                "        return a, b\n",
                "\n",
                "    if view == 'raw':\n",
                "        return maybe_coral(X_train, X_test)\n",
                "    if view == 'quantile':\n",
                "        qt = QuantileTransformer(output_distribution='normal', random_state=seed)\n",
                "        X_tr, X_te = qt.fit_transform(X_train), qt.transform(X_test)\n",
                "        return maybe_coral(X_tr, X_te)\n",
                "    if view.startswith('pca'):\n",
                "        n_components = min(50, X_train.shape[1], max(2, X_train.shape[0] - 1))\n",
                "        pca = PCA(n_components=n_components, random_state=seed)\n",
                "        X_tr, X_te = pca.fit_transform(X_train), pca.transform(X_test)\n",
                "        return maybe_coral(X_tr, X_te)\n",
                "    if view.startswith('ica'):\n",
                "        n_components = min(50, X_train.shape[1], max(2, X_train.shape[0] - 1))\n",
                "        ica = FastICA(n_components=n_components, random_state=seed, max_iter=500)\n",
                "        X_tr, X_te = ica.fit_transform(X_train), ica.transform(X_test)\n",
                "        return maybe_coral(X_tr, X_te)\n",
                "    if view.startswith('rp') or view.startswith('random'):\n",
                "        n_components = min(50, X_train.shape[1])\n",
                "        rp = GaussianRandomProjection(n_components=n_components, random_state=seed)\n",
                "        X_tr, X_te = rp.fit_transform(X_train), rp.transform(X_test)\n",
                "        return maybe_coral(X_tr, X_te)\n",
                "    if view.startswith('spectral'):\n",
                "        if not allow_transductive:\n",
                "            raise ValueError('spectral view needs ALLOW_TRANSDUCTIVE=1')\n",
                "        X_all = np.vstack([X_train, X_test])\n",
                "        n_components = min(30, X_all.shape[0] - 1)\n",
                "        se = SpectralEmbedding(n_components=n_components, random_state=seed)\n",
                "        Z = se.fit_transform(X_all)\n",
                "        return Z[: len(X_train)], Z[len(X_train):]\n",
                "    raise ValueError('Unknown view: %s' % view)\n",
                "\n",
                "\n",
                "class TransductiveDAE(nn.Module):\n",
                "    def __init__(self, input_dim):\n",
                "        super().__init__()\n",
                "        self.encoder = nn.Sequential(nn.Linear(input_dim, 512), nn.SiLU(), nn.Linear(512, 128))\n",
                "        self.decoder = nn.Sequential(nn.Linear(128, 512), nn.SiLU(), nn.Linear(512, input_dim))\n",
                "    def forward(self, x):\n",
                "        return self.decoder(self.encoder(x))\n",
                "\n",
                "\n",
                "class DataRefinery:\n",
                "    def __init__(self, input_dim, batch_size=2048, lr_scale=2e-3, noise_std=0.1, epochs=30, device=None):\n",
                "        self.dae = TransductiveDAE(input_dim).to(device or torch.device('cpu'))\n",
                "        self.batch_size = batch_size\n",
                "        self.lr_scale = lr_scale\n",
                "        self.noise_std = noise_std\n",
                "        self.epochs = epochs\n",
                "        self.device = device or torch.device('cpu')\n",
                "\n",
                "    def fit(self, X_all):\n",
                "        X_t = torch.tensor(X_all, dtype=torch.float32).to(self.device)\n",
                "        dl = DataLoader(TensorDataset(X_t), batch_size=self.batch_size, shuffle=True)\n",
                "        opt = optim.AdamW(self.dae.parameters(), lr=self.lr_scale)\n",
                "        crit = nn.MSELoss()\n",
                "        self.dae.train()\n",
                "        for _ in range(int(self.epochs)):\n",
                "            for (xb,) in dl:\n",
                "                noise = torch.randn_like(xb) * float(self.noise_std)\n",
                "                rec = self.dae(xb + noise)\n",
                "                loss = crit(rec, xb)\n",
                "                opt.zero_grad(); loss.backward(); opt.step()\n",
                "        return self\n",
                "\n",
                "    def transform(self, X):\n",
                "        self.dae.eval()\n",
                "        X_t = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
                "        emb, rec = [], []\n",
                "        for i in range(0, len(X), self.batch_size):\n",
                "            xb = X_t[i:i + self.batch_size]\n",
                "            with torch.no_grad():\n",
                "                z = self.dae.encoder(xb)\n",
                "                r = self.dae.decoder(z)\n",
                "            emb.append(z.cpu().numpy())\n",
                "            rec.append(r.cpu().numpy())\n",
                "        return np.vstack(emb), np.vstack(rec)\n",
                "\n",
                "\n",
                "def compute_manifold_features(X_train, X_test, allow_transductive=False, k=20, enable_pagerank=True, return_lid=False):\n",
                "    if allow_transductive:\n",
                "        X_all = np.vstack([X_train, X_test])\n",
                "        nbrs = NearestNeighbors(n_neighbors=k, n_jobs=-1).fit(X_all)\n",
                "        dists, _ = nbrs.kneighbors(X_all)\n",
                "        d_k = dists[:, -1]; d_j = dists[:, 1:]\n",
                "        lid = k / np.sum(np.log(d_k[:, None] / (d_j + 1e-10) + 1e-10), axis=1)\n",
                "        lid = (lid - lid.min()) / (lid.max() - lid.min() + 1e-12)\n",
                "        if enable_pagerank:\n",
                "            try:\n",
                "                import networkx as nx\n",
                "                A = kneighbors_graph(X_all, k, mode='connectivity', include_self=False)\n",
                "                G = nx.from_scipy_sparse_array(A)\n",
                "                pr = nx.pagerank(G, alpha=0.85, max_iter=50)\n",
                "                pagerank = np.array([pr[i] for i in range(len(X_all))], dtype=np.float64)\n",
                "                pagerank = (pagerank - pagerank.min()) / (pagerank.max() - pagerank.min() + 1e-12)\n",
                "            except Exception:\n",
                "                pagerank = np.zeros(len(X_all))\n",
                "        else:\n",
                "            pagerank = np.zeros(len(X_all))\n",
                "        feats = np.column_stack([lid, pagerank])\n",
                "        feats_tr, feats_te = feats[:len(X_train)], feats[len(X_train):]\n",
                "        if return_lid:\n",
                "            return feats_tr, feats_te, lid[:len(X_train)], lid[len(X_train):]\n",
                "        return feats_tr, feats_te\n",
                "\n",
                "    nbrs = NearestNeighbors(n_neighbors=min(k + 1, len(X_train)), n_jobs=-1).fit(X_train)\n",
                "    dists_tr, idx_tr = nbrs.kneighbors(X_train)\n",
                "    dists_tr = dists_tr[:, 1:]; idx_tr = idx_tr[:, 1:]; k_eff = dists_tr.shape[1]\n",
                "    d_k_tr = dists_tr[:, -1]; d_j_tr = dists_tr[:, :-1] if k_eff > 1 else dists_tr\n",
                "    lid_tr = k_eff / np.sum(np.log(d_k_tr[:, None] / (d_j_tr + 1e-10) + 1e-10), axis=1)\n",
                "    lid_tr_min, lid_tr_max = lid_tr.min(), lid_tr.max(); lid_tr_n = (lid_tr - lid_tr_min) / (lid_tr_max - lid_tr_min + 1e-12)\n",
                "    if enable_pagerank:\n",
                "        try:\n",
                "            import networkx as nx\n",
                "            A_tr = kneighbors_graph(X_train, min(k, len(X_train) - 1), mode='connectivity', include_self=False)\n",
                "            G_tr = nx.from_scipy_sparse_array(A_tr)\n",
                "            pr_tr_dict = nx.pagerank(G_tr, alpha=0.85, max_iter=50)\n",
                "            pr_tr = np.array([pr_tr_dict[i] for i in range(len(X_train))], dtype=np.float64)\n",
                "            pr_tr_min, pr_tr_max = pr_tr.min(), pr_tr.max(); pr_tr_n = (pr_tr - pr_tr_min) / (pr_tr_max - pr_tr_min + 1e-12)\n",
                "        except Exception:\n",
                "            pr_tr_n = np.zeros(len(X_train))\n",
                "    else:\n",
                "        pr_tr_n = np.zeros(len(X_train))\n",
                "    dists_te, idx_te = nbrs.kneighbors(X_test, n_neighbors=min(k, len(X_train)))\n",
                "    k_te = dists_te.shape[1]; d_k_te = dists_te[:, -1]; d_j_te = dists_te[:, :-1] if k_te > 1 else dists_te\n",
                "    lid_te = k_te / np.sum(np.log(d_k_te[:, None] / (d_j_te + 1e-10) + 1e-10), axis=1)\n",
                "    lid_te_n = (lid_te - lid_tr_min) / (lid_tr_max - lid_tr_min + 1e-12); lid_te_n = np.clip(lid_te_n, 0.0, 1.0)\n",
                "    pr_te_n = pr_tr_n[idx_te].mean(axis=1) if len(pr_tr_n) else np.zeros(len(X_test))\n",
                "    feats_tr = np.column_stack([lid_tr_n, pr_tr_n]); feats_te = np.column_stack([lid_te_n, pr_te_n])\n",
                "    if return_lid:\n",
                "        return feats_tr, feats_te, lid_tr_n, lid_te_n\n",
                "    return feats_tr, feats_te\n",
                "\n",
                "\n",
                "def build_streams(X_v, X_test_v, allow_transductive=False, dae_cfg=None, manifold_k=20, enable_pagerank=True, coral=False, coral_reg=1e-3, device=None, batch_size=2048, lr_scale=2e-3, noise_std=0.1, dae_epochs=30):\n",
                "    ref_fit_X = np.vstack([X_v, X_test_v]) if allow_transductive else X_v\n",
                "    ref = DataRefinery(\n",
                "        X_v.shape[1],\n",
                "        batch_size=batch_size,\n",
                "        lr_scale=lr_scale,\n",
                "        noise_std=noise_std,\n",
                "        epochs=dae_epochs,\n",
                "        device=device,\n",
                "    ).fit(ref_fit_X)\n",
                "    feats_tr, feats_te, lid_tr, lid_te = compute_manifold_features(\n",
                "        X_v,\n",
                "        X_test_v,\n",
                "        allow_transductive=allow_transductive,\n",
                "        k=manifold_k,\n",
                "        enable_pagerank=enable_pagerank,\n",
                "        return_lid=True,\n",
                "    )\n",
                "    emb_tr, rec_tr = ref.transform(X_v)\n",
                "    emb_te, rec_te = ref.transform(X_test_v)\n",
                "    X_neural_tr = np.hstack([X_v, feats_tr, emb_tr])\n",
                "    X_neural_te = np.hstack([X_test_v, feats_te, emb_te])\n",
                "    X_tree_tr = np.hstack([X_v, feats_tr, rec_tr])\n",
                "    X_tree_te = np.hstack([X_test_v, feats_te, rec_te])\n",
                "    return X_tree_tr, X_tree_te, X_neural_tr, X_neural_te, lid_tr, lid_te\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5c060a32",
            "metadata": {},
            "source": [
                "**Κελί 12: Μηχανική Χαρακτηριστικών (Feature Engineering)**\n",
                "\n",
                "Ορίζει μετασχηματισμούς χαρακτηριστικών και manifold descriptors:\n",
                "\n",
                "**Feature Views:**\n",
                "- `raw`: Αρχικά χαρακτηριστικά χωρίς μετασχηματισμό\n",
                "- `quantile`: Gaussian Quantile Transformer για κανονικοποίηση κατανομών\n",
                "- `pca`: Principal Component Analysis για μείωση διαστάσεων\n",
                "- `ica`: Independent Component Analysis\n",
                "- `rp`: Gaussian Random Projection\n",
                "- `spectral`: Spectral Embedding (απαιτεί transductive mode)\n",
                "\n",
                "**Transductive DAE:**\n",
                "- Denoising Autoencoder εκπαιδευμένος σε train+test για εξαγωγή latent representations\n",
                "\n",
                "**Manifold Descriptors:**\n",
                "- **LID (Local Intrinsic Dimensionality)**: Μέτρηση τοπικής πολυπλοκότητας γύρω από κάθε σημείο\n",
                "- **PageRank**: Κεντρικότητα σημείων στον KNN γράφο"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fd8972d2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# sigma_omega: models_trees\n",
                "import torch\n",
                "\n",
                "def get_xgb_dart(n_c):\n",
                "    from xgboost import XGBClassifier\n",
                "    use_gpu = torch.cuda.is_available()\n",
                "    params = dict(\n",
                "        booster='dart', rate_drop=0.1, skip_drop=0.5,\n",
                "        n_estimators=500, max_depth=6, learning_rate=0.05,\n",
                "        objective='multi:softprob', num_class=int(n_c), eval_metric='mlogloss', verbosity=0,\n",
                "    )\n",
                "    if use_gpu:\n",
                "        params.update(tree_method='gpu_hist', predictor='gpu_predictor')\n",
                "    else:\n",
                "        params.update(tree_method='hist')\n",
                "    return XGBClassifier(**params)\n",
                "\n",
                "def get_cat_langevin(n_c):\n",
                "    from catboost import CatBoostClassifier\n",
                "    return CatBoostClassifier(\n",
                "        langevin=True, diffusion_temperature=1000,\n",
                "        iterations=1000, depth=8, learning_rate=0.03,\n",
                "        loss_function='MultiClass', eval_metric='MultiClass',\n",
                "        task_type='GPU' if torch.cuda.is_available() else 'CPU',\n",
                "        verbose=0, allow_writing_files=False,\n",
                "    )\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ffbf7a42",
            "metadata": {},
            "source": [
                "**Κελί 13: Δενδρικά Μοντέλα (Tree Models)**\n",
                "\n",
                "Factory functions για δημιουργία gradient boosting μοντέλων:\n",
                "\n",
                "**XGBoost DART:**\n",
                "- Χρησιμοποιεί DART (Dropouts meet Multiple Additive Regression Trees)\n",
                "- Rate drop: 10%, Skip drop: 50%\n",
                "- 500 estimators, max_depth=6\n",
                "- Αυτόματη χρήση GPU αν διαθέσιμη\n",
                "\n",
                "**CatBoost Langevin:**\n",
                "- Χρησιμοποιεί Langevin dynamics για stochastic gradient descent\n",
                "- Diffusion temperature: 1000\n",
                "- 1000 iterations, depth=8\n",
                "- Καλύτερη γενίκευση μέσω Bayesian sampling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ed7c7e0f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# sigma_omega: models_torch\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "from sklearn.base import BaseEstimator, ClassifierMixin\n",
                "from sklearn.neighbors import NearestNeighbors\n",
                "from torch.utils.data import DataLoader, TensorDataset\n",
                "\n",
                "# requires: config globals, losses helpers\n",
                "\n",
                "def is_torch_model(m):\n",
                "    return hasattr(m, 'finetune_on_pseudo') and callable(getattr(m, 'finetune_on_pseudo'))\n",
                "\n",
                "def select_silver_samples(probs, gap_low=0.10, gap_high=0.35, max_samples=4096, seed=42):\n",
                "    p = np.asarray(probs, dtype=np.float64)\n",
                "    if p.ndim != 2 or p.shape[0] == 0:\n",
                "        return np.array([], dtype=np.int64), np.array([], dtype=np.int64)\n",
                "    part = np.partition(p, kth=(-1, -2), axis=1)\n",
                "    top1 = part[:, -1]; top2 = part[:, -2]; gap = top1 - top2\n",
                "    mask = (gap >= float(gap_low)) & (gap <= float(gap_high))\n",
                "    idx = np.nonzero(mask)[0]\n",
                "    if idx.size == 0:\n",
                "        return idx.astype(np.int64), np.array([], dtype=np.int64)\n",
                "    rng = np.random.default_rng(int(seed))\n",
                "    if idx.size > int(max_samples):\n",
                "        idx = rng.choice(idx, size=int(max_samples), replace=False)\n",
                "    y_pseudo = np.argmax(p[idx], axis=1).astype(np.int64)\n",
                "    return idx.astype(np.int64), y_pseudo\n",
                "\n",
                "class TopologyMixUpLoader:\n",
                "    def __init__(self, X, y, num_classes, batch_size):\n",
                "        self.X, self.y, self.num_classes, self.batch_size = X, y, int(num_classes), int(batch_size)\n",
                "        self.knn = NearestNeighbors(n_neighbors=5, n_jobs=-1).fit(X)\n",
                "        self.rng = np.random.default_rng()\n",
                "    def __iter__(self):\n",
                "        idxs = self.rng.permutation(len(self.X))\n",
                "        for i in range(0, len(self.X), self.batch_size):\n",
                "            b_idxs = idxs[i:i + self.batch_size]\n",
                "            X_b, y_b = self.X[b_idxs], self.y[b_idxs]\n",
                "            mn_idxs = self.knn.kneighbors(X_b, return_distance=False)\n",
                "            rand_n = self.rng.integers(1, 5, size=len(X_b))\n",
                "            target_idxs = mn_idxs[np.arange(len(X_b)), rand_n]\n",
                "            X_target = self.X[target_idxs]; y_target = self.y[target_idxs]\n",
                "            lam = self.rng.beta(0.4, 0.4, size=(len(X_b), 1)); lam = np.maximum(lam, 1 - lam)\n",
                "            X_mix = lam * X_b + (1 - lam) * X_target\n",
                "            y_b_oh = np.eye(self.num_classes, dtype=np.float32)[y_b]\n",
                "            y_t_oh = np.eye(self.num_classes, dtype=np.float32)[y_target]\n",
                "            y_mix = lam * y_b_oh + (1 - lam) * y_t_oh\n",
                "            yield (\n",
                "                torch.tensor(X_mix, dtype=torch.float32).to(DEVICE),\n",
                "                torch.tensor(y_mix, dtype=torch.float32).to(DEVICE),\n",
                "            )\n",
                "\n",
                "class SAM(torch.optim.Optimizer):\n",
                "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
                "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
                "        super().__init__(params, defaults)\n",
                "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
                "        self.param_groups = self.base_optimizer.param_groups\n",
                "        self.defaults.update(self.base_optimizer.defaults)\n",
                "    @torch.no_grad()\n",
                "    def first_step(self, zero_grad=False):\n",
                "        grad_norm = self._grad_norm()\n",
                "        for group in self.param_groups:\n",
                "            scale = group['rho'] / (grad_norm + 1e-12)\n",
                "            for p in group['params']:\n",
                "                if p.grad is None:\n",
                "                    continue\n",
                "                self.state[p]['old_p'] = p.data.clone()\n",
                "                p.add_((torch.pow(p, 2) if group['adaptive'] else 1.0) * p.grad * scale.to(p))\n",
                "        if zero_grad:\n",
                "            self.zero_grad()\n",
                "    @torch.no_grad()\n",
                "    def second_step(self, zero_grad=False):\n",
                "        for group in self.param_groups:\n",
                "            for p in group['params']:\n",
                "                if p.grad is None:\n",
                "                    continue\n",
                "                p.data = self.state[p]['old_p']\n",
                "        if zero_grad:\n",
                "            self.zero_grad()\n",
                "    def _grad_norm(self):\n",
                "        shared_device = self.param_groups[0]['params'][0].device\n",
                "        return torch.norm(\n",
                "            torch.stack([\n",
                "                ((torch.abs(p) if group['adaptive'] else 1.0) * p.grad).norm(p=2).to(shared_device)\n",
                "                for group in self.param_groups for p in group['params'] if p.grad is not None\n",
                "            ]), p=2,\n",
                "        )\n",
                "    def step(self):\n",
                "        raise NotImplementedError\n",
                "\n",
                "class TabRModule(nn.Module):\n",
                "    def __init__(self, input_dim, num_classes, context_size=96):\n",
                "        super().__init__()\n",
                "        self.encoder = nn.Sequential(nn.Linear(input_dim, 128), nn.SiLU(), nn.Linear(128, context_size))\n",
                "        self.q_proj = nn.Linear(context_size, context_size)\n",
                "        self.k_proj = nn.Linear(context_size, context_size)\n",
                "        self.v_proj = nn.Linear(context_size, context_size)\n",
                "        self.head = nn.Sequential(nn.Linear(context_size, 64), nn.SiLU(), nn.Linear(64, num_classes))\n",
                "        self.scale = context_size ** -0.5\n",
                "    def forward(self, x, neighbors):\n",
                "        q = self.encoder(x).unsqueeze(1)\n",
                "        B, K, D = neighbors.shape\n",
                "        kv = self.encoder(neighbors.view(B * K, D)).view(B, K, -1)\n",
                "        scores = torch.bmm(self.q_proj(q), self.k_proj(kv).transpose(1, 2)) * self.scale\n",
                "        context = torch.bmm(F.softmax(scores, dim=-1), self.v_proj(kv)).squeeze(1)\n",
                "        return self.head(context + q.squeeze(1))\n",
                "\n",
                "class TrueTabR(BaseEstimator, ClassifierMixin):\n",
                "    def __init__(self, num_classes, n_neighbors=16):\n",
                "        self.num_classes, self.n_neighbors = num_classes, n_neighbors\n",
                "        self.model, self.knn, self.X_train_ = None, None, None\n",
                "    def fit(self, X, y, sample_weight=None):\n",
                "        self.X_train_ = np.array(X, dtype=np.float32)\n",
                "        self.knn = NearestNeighbors(n_neighbors=self.n_neighbors, n_jobs=-1).fit(self.X_train_)\n",
                "        train_neighbor_idx = self.knn.kneighbors(self.X_train_, return_distance=False)\n",
                "        self.model = TabRModule(X.shape[1], self.num_classes).to(DEVICE)\n",
                "        opt = optim.AdamW(self.model.parameters(), lr=LR_SCALE)\n",
                "        class_w = None\n",
                "        if USE_CLASS_BALANCED:\n",
                "            cw = compute_class_balanced_weights(y, self.num_classes, beta=CB_BETA)\n",
                "            class_w = torch.tensor(cw, dtype=torch.float32, device=DEVICE)\n",
                "        if LABEL_SMOOTHING > 0:\n",
                "            crit = nn.CrossEntropyLoss(weight=class_w, label_smoothing=float(LABEL_SMOOTHING))\n",
                "        else:\n",
                "            crit = nn.CrossEntropyLoss(weight=class_w)\n",
                "        X_t = torch.tensor(X, dtype=torch.float32); y_t = torch.tensor(y, dtype=torch.long)\n",
                "        idx_t = torch.arange(len(X_t), dtype=torch.long)\n",
                "        if sample_weight is None:\n",
                "            dl = DataLoader(TensorDataset(X_t, y_t, idx_t), batch_size=BATCH_SIZE, shuffle=True)\n",
                "        else:\n",
                "            w_t = torch.tensor(np.asarray(sample_weight, dtype=np.float32))\n",
                "            dl = DataLoader(TensorDataset(X_t, y_t, idx_t, w_t), batch_size=BATCH_SIZE, shuffle=True)\n",
                "        self.model.train()\n",
                "        for _ in range(15):\n",
                "            for batch in dl:\n",
                "                if sample_weight is None:\n",
                "                    xb, yb, ib = batch; wb = None\n",
                "                else:\n",
                "                    xb, yb, ib, wb = batch\n",
                "                nx = self.X_train_[train_neighbor_idx[ib.numpy()]]\n",
                "                logits = self.model(xb.to(DEVICE), torch.tensor(nx, dtype=torch.float32).to(DEVICE))\n",
                "                if LOSS_NAME == 'focal':\n",
                "                    y_onehot = F.one_hot(yb.to(DEVICE), num_classes=self.num_classes).float()\n",
                "                    y_onehot = smooth_targets(y_onehot, LABEL_SMOOTHING)\n",
                "                    probs = torch.softmax(logits, dim=1).clamp(1e-8, 1.0 - 1e-8)\n",
                "                    pt = (probs * y_onehot).sum(dim=1)\n",
                "                    loss_vec = -torch.pow(1.0 - pt, float(FOCAL_GAMMA)) * torch.log(pt)\n",
                "                    if class_w is not None:\n",
                "                        w_class = (y_onehot * class_w.view(1, -1)).sum(dim=1)\n",
                "                        loss_vec = loss_vec * w_class\n",
                "                    if wb is not None:\n",
                "                        loss_vec = loss_vec * wb.to(DEVICE)\n",
                "                    loss = loss_vec.mean()\n",
                "                else:\n",
                "                    loss_vec = F.cross_entropy(\n",
                "                        logits,\n",
                "                        yb.to(DEVICE),\n",
                "                        reduction='none',\n",
                "                        weight=class_w,\n",
                "                        label_smoothing=float(LABEL_SMOOTHING) if LABEL_SMOOTHING > 0 else 0.0,\n",
                "                    )\n",
                "                    if wb is not None:\n",
                "                        loss_vec = loss_vec * wb.to(DEVICE)\n",
                "                    loss = loss_vec.mean()\n",
                "                opt.zero_grad(); loss.backward(); opt.step()\n",
                "        return self\n",
                "    def finetune_on_pseudo(self, X_pseudo, y_pseudo, epochs=1, lr_mult=0.2):\n",
                "        if X_pseudo is None or len(X_pseudo) == 0:\n",
                "            return self\n",
                "        self.model.train(); lr = float(LR_SCALE) * float(lr_mult)\n",
                "        opt = optim.AdamW(self.model.parameters(), lr=lr)\n",
                "        class_w = None\n",
                "        if USE_CLASS_BALANCED:\n",
                "            cw = compute_class_balanced_weights(y_pseudo, self.num_classes, beta=CB_BETA)\n",
                "            class_w = torch.tensor(cw, dtype=torch.float32, device=DEVICE)\n",
                "        if LABEL_SMOOTHING > 0:\n",
                "            crit = nn.CrossEntropyLoss(weight=class_w, label_smoothing=float(LABEL_SMOOTHING))\n",
                "        else:\n",
                "            crit = nn.CrossEntropyLoss(weight=class_w)\n",
                "        X_t = torch.tensor(np.asarray(X_pseudo, dtype=np.float32)); y_t = torch.tensor(np.asarray(y_pseudo, dtype=np.int64))\n",
                "        dl = DataLoader(TensorDataset(X_t, y_t), batch_size=BATCH_SIZE, shuffle=True)\n",
                "        for _ in range(int(epochs)):\n",
                "            for xb, yb in dl:\n",
                "                xb = xb.to(DEVICE); yb = yb.to(DEVICE)\n",
                "                nx = self.X_train_[self.knn.kneighbors(xb.detach().cpu().numpy(), return_distance=False)]\n",
                "                logits = self.model(xb, torch.tensor(nx, dtype=torch.float32).to(DEVICE))\n",
                "                if LOSS_NAME == 'focal':\n",
                "                    y_onehot = F.one_hot(yb, num_classes=self.num_classes).float()\n",
                "                    y_onehot = smooth_targets(y_onehot, LABEL_SMOOTHING)\n",
                "                    loss = soft_target_focal(logits, y_onehot, gamma=FOCAL_GAMMA, class_weights=class_w)\n",
                "                else:\n",
                "                    loss = crit(logits, yb)\n",
                "                opt.zero_grad(); loss.backward(); opt.step()\n",
                "        return self\n",
                "    def predict_proba(self, X):\n",
                "        self.model.eval(); p = []\n",
                "        for i in range(0, len(X), BATCH_SIZE):\n",
                "            xb = X[i:i + BATCH_SIZE]\n",
                "            nx = self.X_train_[self.knn.kneighbors(xb, return_distance=False)]\n",
                "            with torch.no_grad():\n",
                "                p.append(torch.softmax(self.model(torch.tensor(xb, dtype=torch.float32).to(DEVICE), torch.tensor(nx).to(DEVICE)), dim=1).cpu().numpy())\n",
                "        return np.vstack(p)\n",
                "\n",
                "class ThetaTabM(BaseEstimator, ClassifierMixin):\n",
                "    def __init__(self, input_dim, num_classes):\n",
                "        self.num_classes = int(num_classes)\n",
                "        self.model = nn.Sequential(\n",
                "            nn.Linear(input_dim, 256), nn.LayerNorm(256), nn.SiLU(), nn.Dropout(0.2),\n",
                "            nn.Linear(256, 128), nn.LayerNorm(128), nn.SiLU(), nn.Linear(128, num_classes),\n",
                "        ).to(DEVICE)\n",
                "    def fit(self, X, y, sample_weight=None):\n",
                "        opt = SAM(self.model.parameters(), optim.AdamW, lr=LR_SCALE, rho=SAM_RHO)\n",
                "        class_w = None\n",
                "        if USE_CLASS_BALANCED:\n",
                "            cw = compute_class_balanced_weights(y, self.num_classes, beta=CB_BETA)\n",
                "            class_w = torch.tensor(cw, dtype=torch.float32, device=DEVICE)\n",
                "        if LABEL_SMOOTHING > 0:\n",
                "            hard_crit = nn.CrossEntropyLoss(weight=class_w, label_smoothing=float(LABEL_SMOOTHING))\n",
                "        else:\n",
                "            hard_crit = nn.CrossEntropyLoss(weight=class_w)\n",
                "        self.model.train(); swa_model = None\n",
                "        if ENABLE_SWA:\n",
                "            try:\n",
                "                from torch.optim.swa_utils import AveragedModel\n",
                "                swa_model = AveragedModel(self.model)\n",
                "            except Exception:\n",
                "                swa_model = None\n",
                "        for ep in range(20):\n",
                "            use_mixup_local = USE_MIXUP and (sample_weight is None)\n",
                "            if use_mixup_local:\n",
                "                iterator = TopologyMixUpLoader(X, y, num_classes=self.num_classes, batch_size=BATCH_SIZE)\n",
                "                for xb, yb in iterator:\n",
                "                    yb = smooth_targets(yb, LABEL_SMOOTHING)\n",
                "                    opt.zero_grad(); logits = self.model(xb)\n",
                "                    if LOSS_NAME == 'focal':\n",
                "                        loss = soft_target_focal(logits, yb, gamma=FOCAL_GAMMA, class_weights=class_w)\n",
                "                    else:\n",
                "                        loss = soft_target_ce(logits, yb, class_weights=class_w)\n",
                "                    loss.backward(); opt.first_step(zero_grad=True)\n",
                "                    logits2 = self.model(xb)\n",
                "                    if LOSS_NAME == 'focal':\n",
                "                        loss2 = soft_target_focal(logits2, yb, gamma=FOCAL_GAMMA, class_weights=class_w)\n",
                "                    else:\n",
                "                        loss2 = soft_target_ce(logits2, yb, class_weights=class_w)\n",
                "                    loss2.backward(); opt.second_step(zero_grad=True); opt.base_optimizer.step()\n",
                "            else:\n",
                "                w_arr = np.ones(len(X), dtype=np.float32) if sample_weight is None else np.asarray(sample_weight, dtype=np.float32)\n",
                "                dl = DataLoader(\n",
                "                    TensorDataset(\n",
                "                        torch.tensor(X, dtype=torch.float32).to(DEVICE),\n",
                "                        torch.tensor(y, dtype=torch.long).to(DEVICE),\n",
                "                        torch.tensor(w_arr, dtype=torch.float32).to(DEVICE),\n",
                "                    ),\n",
                "                    batch_size=BATCH_SIZE, shuffle=True,\n",
                "                )\n",
                "                for xb, yb, wb in dl:\n",
                "                    opt.zero_grad(); logits = self.model(xb)\n",
                "                    if LOSS_NAME == 'focal':\n",
                "                        y_onehot = F.one_hot(yb, num_classes=self.num_classes).float()\n",
                "                        y_onehot = smooth_targets(y_onehot, LABEL_SMOOTHING)\n",
                "                        probs = torch.softmax(logits, dim=1).clamp(1e-8, 1.0 - 1e-8)\n",
                "                        pt = (probs * y_onehot).sum(dim=1)\n",
                "                        loss_vec = -torch.pow(1.0 - pt, float(FOCAL_GAMMA)) * torch.log(pt)\n",
                "                        if class_w is not None:\n",
                "                            w_class = (y_onehot * class_w.view(1, -1)).sum(dim=1)\n",
                "                            loss_vec = loss_vec * w_class\n",
                "                        loss = (loss_vec * wb).mean()\n",
                "                    else:\n",
                "                        loss_vec = F.cross_entropy(\n",
                "                            logits, yb,\n",
                "                            reduction='none', weight=class_w,\n",
                "                            label_smoothing=float(LABEL_SMOOTHING) if LABEL_SMOOTHING > 0 else 0.0,\n",
                "                        )\n",
                "                        loss = (loss_vec * wb).mean()\n",
                "                    loss.backward(); opt.first_step(zero_grad=True)\n",
                "                    logits2 = self.model(xb)\n",
                "                    if LOSS_NAME == 'focal':\n",
                "                        y_onehot = F.one_hot(yb, num_classes=self.num_classes).float()\n",
                "                        y_onehot = smooth_targets(y_onehot, LABEL_SMOOTHING)\n",
                "                        probs2 = torch.softmax(logits2, dim=1).clamp(1e-8, 1.0 - 1e-8)\n",
                "                        pt2 = (probs2 * y_onehot).sum(dim=1)\n",
                "                        loss2_vec = -torch.pow(1.0 - pt2, float(FOCAL_GAMMA)) * torch.log(pt2)\n",
                "                        if class_w is not None:\n",
                "                            w_class = (y_onehot * class_w.view(1, -1)).sum(dim=1)\n",
                "                            loss2_vec = loss2_vec * w_class\n",
                "                        loss2 = (loss2_vec * wb).mean()\n",
                "                    else:\n",
                "                        loss2_vec = F.cross_entropy(\n",
                "                            logits2, yb,\n",
                "                            reduction='none', weight=class_w,\n",
                "                            label_smoothing=float(LABEL_SMOOTHING) if LABEL_SMOOTHING > 0 else 0.0,\n",
                "                        )\n",
                "                        loss2 = (loss2_vec * wb).mean()\n",
                "                    loss2.backward(); opt.second_step(zero_grad=True); opt.base_optimizer.step()\n",
                "            if swa_model is not None and ep >= int(SWA_START_EPOCH):\n",
                "                swa_model.update_parameters(self.model)\n",
                "        if swa_model is not None and int(SWA_START_EPOCH) < 20:\n",
                "            self.model.load_state_dict(swa_model.module.state_dict())\n",
                "        return self\n",
                "    def finetune_on_pseudo(self, X_pseudo, y_pseudo, epochs=1, lr_mult=0.2):\n",
                "        if X_pseudo is None or len(X_pseudo) == 0:\n",
                "            return self\n",
                "        self.model.train(); lr = float(LR_SCALE) * float(lr_mult)\n",
                "        opt = optim.AdamW(self.model.parameters(), lr=lr)\n",
                "        class_w = None\n",
                "        if USE_CLASS_BALANCED:\n",
                "            cw = compute_class_balanced_weights(y_pseudo, self.num_classes, beta=CB_BETA)\n",
                "            class_w = torch.tensor(cw, dtype=torch.float32, device=DEVICE)\n",
                "        if LABEL_SMOOTHING > 0:\n",
                "            hard_crit = nn.CrossEntropyLoss(weight=class_w, label_smoothing=float(LABEL_SMOOTHING))\n",
                "        else:\n",
                "            hard_crit = nn.CrossEntropyLoss(weight=class_w)\n",
                "        X_t = torch.tensor(np.asarray(X_pseudo, dtype=np.float32)); y_t = torch.tensor(np.asarray(y_pseudo, dtype=np.int64))\n",
                "        dl = DataLoader(TensorDataset(X_t, y_t), batch_size=BATCH_SIZE, shuffle=True)\n",
                "        for _ in range(int(epochs)):\n",
                "            for xb, yb in dl:\n",
                "                xb = xb.to(DEVICE); yb = yb.to(DEVICE)\n",
                "                logits = self.model(xb)\n",
                "                if LOSS_NAME == 'focal':\n",
                "                    y_onehot = F.one_hot(yb, num_classes=self.num_classes).float()\n",
                "                    y_onehot = smooth_targets(y_onehot, LABEL_SMOOTHING)\n",
                "                    loss = soft_target_focal(logits, y_onehot, gamma=FOCAL_GAMMA, class_weights=class_w)\n",
                "                else:\n",
                "                    loss = hard_crit(logits, yb)\n",
                "                opt.zero_grad(); loss.backward(); opt.step()\n",
                "        return self\n",
                "    def predict_proba(self, X):\n",
                "        self.model.eval(); p = []\n",
                "        with torch.no_grad():\n",
                "            for i in range(0, len(X), BATCH_SIZE):\n",
                "                p.append(torch.softmax(self.model(torch.tensor(X[i:i + BATCH_SIZE], dtype=torch.float32).to(DEVICE)), dim=1).cpu().numpy())\n",
                "        return np.vstack(p)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3de730be",
            "metadata": {},
            "source": [
                "**Κελί 14: Νευρωνικά Μοντέλα (Neural Models)**\n",
                "\n",
                "Υλοποιεί προηγμένα νευρωνικά δίκτυα για tabular δεδομένα:\n",
                "\n",
                "**TrueTabR (Retrieval-Augmented):**\n",
                "- Χρησιμοποιεί attention πάνω σε K-nearest neighbors\n",
                "- Query: encoding του τρέχοντος sample\n",
                "- Keys/Values: encodings των γειτόνων\n",
                "- Αξιοποιεί τοπική δομή των δεδομένων\n",
                "\n",
                "**ThetaTabM (MLP with SAM):**\n",
                "- Sharpness-Aware Minimization για καλύτερη γενίκευση\n",
                "- Topology-aware MixUp βασισμένο σε KNN\n",
                "- LayerNorm και SiLU activations\n",
                "- Υποστηρίζει Focal loss και label smoothing\n",
                "\n",
                "**Κοινά χαρακτηριστικά:**\n",
                "- Optional class-balanced weights\n",
                "- Finetune on pseudo-labels για TTT (Test-Time Training)\n",
                "- Optional SWA (Stochastic Weight Averaging)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a1a2207e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# sigma_omega: calibration\n",
                "import copy\n",
                "import numpy as np\n",
                "from sklearn.isotonic import IsotonicRegression\n",
                "from sklearn.model_selection import StratifiedKFold\n",
                "\n",
                "class CalibratedModel:\n",
                "    def __init__(self, base_model, name):\n",
                "        self.base, self.name, self.ir = base_model, name, None\n",
                "    def fit(self, X, y, sample_weight=None, pseudo_X=None, pseudo_y=None, pseudo_w=None):\n",
                "        skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
                "        self.models = []\n",
                "        self.calibrators = []\n",
                "        for tr_idx, val_idx in skf.split(X, y):\n",
                "            X_tr, X_val = X[tr_idx], X[val_idx]\n",
                "            y_tr, y_val = y[tr_idx], y[val_idx]\n",
                "            sw_tr = sample_weight[tr_idx] if sample_weight is not None else None\n",
                "            if pseudo_X is not None and len(pseudo_X) > 0:\n",
                "                X_tr = np.vstack([X_tr, pseudo_X])\n",
                "                y_tr = np.concatenate([y_tr, pseudo_y])\n",
                "                if sw_tr is None and pseudo_w is not None:\n",
                "                    sw_tr = np.ones(len(y_tr), dtype=np.float32)\n",
                "                    sw_tr[: len(tr_idx)] = 1.0; sw_tr[len(tr_idx):] = np.asarray(pseudo_w, dtype=np.float32)\n",
                "                elif sw_tr is not None and pseudo_w is not None:\n",
                "                    sw_tr = np.concatenate([sw_tr, np.asarray(pseudo_w, dtype=np.float32)])\n",
                "            model = copy.deepcopy(self.base)\n",
                "            try:\n",
                "                model.fit(X_tr, y_tr, sample_weight=sw_tr)\n",
                "            except TypeError:\n",
                "                model.fit(X_tr, y_tr)\n",
                "            val_probs = model.predict_proba(X_val).astype(np.float32)\n",
                "            c_list = []\n",
                "            for c in range(val_probs.shape[1]):\n",
                "                iso = IsotonicRegression(out_of_bounds='clip')\n",
                "                iso.fit(val_probs[:, c], (y_val == c).astype(int))\n",
                "                c_list.append(iso)\n",
                "            self.models.append(model)\n",
                "            self.calibrators.append(c_list)\n",
                "        return self\n",
                "    def predict_proba(self, X):\n",
                "        total_probs = np.zeros((len(X), len(self.calibrators[0])))\n",
                "        for model, calib_list in zip(self.models, self.calibrators):\n",
                "            raw_p = model.predict_proba(X)\n",
                "            cal_p = np.zeros_like(raw_p)\n",
                "            for c in range(raw_p.shape[1]):\n",
                "                cal_p[:, c] = calib_list[c].predict(raw_p[:, c])\n",
                "            cal_p /= (cal_p.sum(axis=1, keepdims=True) + 1e-10)\n",
                "            total_probs += cal_p\n",
                "        return total_probs / len(self.models)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1e087d93",
            "metadata": {},
            "source": [
                "**Κελί 15: Calibration (Βαθμονόμηση Πιθανοτήτων)**\n",
                "\n",
                "Wrapper κλάση `CalibratedModel` για βαθμονόμηση πιθανοτήτων:\n",
                "\n",
                "**Μεθοδολογία:**\n",
                "- 10-fold stratified cross-validation\n",
                "- Εκπαίδευση Isotonic Regression ανά κλάση στις OOF πιθανότητες\n",
                "- Μέσος όρος calibrated πιθανοτήτων από όλα τα folds\n",
                "\n",
                "**Πλεονεκτήματα:**\n",
                "- Διορθώνει overconfident ή underconfident προβλέψεις\n",
                "- Βελτιώνει αξιοπιστία πιθανοτήτων για downstream tasks\n",
                "- Υποστηρίζει ενσωμάτωση pseudo-labeled δειγμάτων"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f78cff74",
            "metadata": {},
            "outputs": [],
            "source": [
                "# sigma_omega: stacking\n",
                "import copy\n",
                "import numpy as np\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.model_selection import StratifiedKFold\n",
                "\n",
                "# uses: prob_meta_features, tabpfn optional\n",
                "\n",
                "def tabpfn_oof_and_test_proba(X_train, y, X_test, num_classes, cv_splits=10, seed=42, n_ensembles=32, device=None):\n",
                "    try:\n",
                "        from tabpfn import TabPFNClassifier\n",
                "    except Exception as e:\n",
                "        raise RuntimeError('USE_TABPFN=1 but tabpfn not installed') from e\n",
                "    skf = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=42)\n",
                "    oof = np.zeros((len(X_train), num_classes), dtype=np.float32)\n",
                "    te_acc = np.zeros((len(X_test), num_classes), dtype=np.float32)\n",
                "    for tr_idx, val_idx in skf.split(X_train, y):\n",
                "        model = TabPFNClassifier(device=str(device), N_ensemble_configurations=int(n_ensembles), seed=int(seed))\n",
                "        model.fit(X_train[tr_idx], y[tr_idx])\n",
                "        oof[val_idx] = model.predict_proba(X_train[val_idx]).astype(np.float32)\n",
                "        te_acc += model.predict_proba(X_test).astype(np.float32)\n",
                "    te_acc /= cv_splits\n",
                "    return oof, te_acc\n",
                "\n",
                "\n",
                "def fit_predict_stacking(\n",
                "    names_models,\n",
                "    X_tree_tr, X_tree_te, X_neural_tr, X_neural_te, X_view_tr, X_view_te,\n",
                "    y, num_classes,\n",
                "    lid_tr=None, lid_te=None,\n",
                "    cv_splits=10, seed=42, sample_weight=None,\n",
                "    pseudo_X_tree=None, pseudo_X_neural=None, pseudo_y=None, pseudo_w=None,\n",
                "):\n",
                "    skf = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=42)\n",
                "    oof_blocks, test_blocks, meta_feat_oof, meta_feat_te = [], [], [], []\n",
                "    for name, base in names_models:\n",
                "        data_tr = X_tree_tr if ('XGB' in name or 'Cat' in name) else X_neural_tr\n",
                "        data_te = X_tree_te if ('XGB' in name or 'Cat' in name) else X_neural_te\n",
                "        pX = pseudo_X_tree if ('XGB' in name or 'Cat' in name) else pseudo_X_neural\n",
                "        oof = np.zeros((len(data_tr), num_classes), dtype=np.float32)\n",
                "        te_acc = np.zeros((len(data_te), num_classes), dtype=np.float32)\n",
                "        for tr_idx, val_idx in skf.split(data_tr, y):\n",
                "            model = copy.deepcopy(base)\n",
                "            sw_tr = sample_weight[tr_idx] if sample_weight is not None else None\n",
                "            X_fold, y_fold = data_tr[tr_idx], y[tr_idx]\n",
                "            if pX is not None and len(pX) > 0:\n",
                "                X_fold = np.vstack([X_fold, pX]); y_fold = np.concatenate([y_fold, pseudo_y])\n",
                "                if sw_tr is None and pseudo_w is not None:\n",
                "                    sw_tr = np.ones(len(y_fold), dtype=np.float32); sw_tr[: len(tr_idx)] = 1.0; sw_tr[len(tr_idx):] = np.asarray(pseudo_w, dtype=np.float32)\n",
                "                elif sw_tr is not None and pseudo_w is not None:\n",
                "                    sw_tr = np.concatenate([sw_tr, np.asarray(pseudo_w, dtype=np.float32)])\n",
                "            try:\n",
                "                model.fit(X_fold, y_fold, sample_weight=sw_tr)\n",
                "            except TypeError:\n",
                "                model.fit(X_fold, y_fold)\n",
                "            oof[val_idx] = model.predict_proba(data_tr[val_idx]).astype(np.float32)\n",
                "            te_acc += model.predict_proba(data_te).astype(np.float32)\n",
                "        te_acc /= cv_splits\n",
                "        oof_blocks.append(oof); test_blocks.append(te_acc)\n",
                "        meta_feat_oof.append(prob_meta_features(oof))\n",
                "        meta_feat_te.append(prob_meta_features(te_acc))\n",
                "    if USE_TABPFN:\n",
                "        tab_oof, tab_te = tabpfn_oof_and_test_proba(X_view_tr, y, X_view_te, num_classes, cv_splits, seed, TABPFN_N_ENSEMBLES, DEVICE)\n",
                "        oof_blocks.append(tab_oof); test_blocks.append(tab_te)\n",
                "        meta_feat_oof.append(prob_meta_features(tab_oof)); meta_feat_te.append(prob_meta_features(tab_te))\n",
                "    n_experts = len(oof_blocks)\n",
                "    meta_X = np.hstack(oof_blocks); meta_te = np.hstack(test_blocks)\n",
                "    meta_X_meta = np.hstack(meta_feat_oof) if len(meta_feat_oof) else None\n",
                "    meta_te_meta = np.hstack(meta_feat_te) if len(meta_feat_te) else None\n",
                "    if lid_tr is not None:\n",
                "        meta_X_meta = np.hstack([meta_X_meta, np.asarray(lid_tr, dtype=np.float32).reshape(-1, 1)]) if meta_X_meta is not None else np.asarray(lid_tr, dtype=np.float32).reshape(-1, 1)\n",
                "    if lid_te is not None:\n",
                "        meta_te_meta = np.hstack([meta_te_meta, np.asarray(lid_te, dtype=np.float32).reshape(-1, 1)]) if meta_te_meta is not None else np.asarray(lid_te, dtype=np.float32).reshape(-1, 1)\n",
                "    if META_LEARNER == 'moe':\n",
                "        y_int = np.asarray(y, dtype=np.int64); nll = np.zeros((len(y_int), n_experts), dtype=np.float64)\n",
                "        for i, oof in enumerate(oof_blocks):\n",
                "            p_true = np.clip(oof[np.arange(len(y_int)), y_int], 1e-12, 1.0)\n",
                "            nll[:, i] = -np.log(p_true)\n",
                "        expert_label = np.argmin(nll, axis=1).astype(np.int64)\n",
                "        gate_X = meta_X_meta if meta_X_meta is not None else meta_X\n",
                "        gate_te = meta_te_meta if meta_te_meta is not None else meta_te\n",
                "        try:\n",
                "            import importlib\n",
                "            LGBMClassifier = importlib.import_module('lightgbm').LGBMClassifier\n",
                "            gate = LGBMClassifier(objective='multiclass', num_class=int(n_experts), max_depth=2, num_leaves=min(31, max(2, 4)), n_estimators=300, learning_rate=0.05, subsample=0.9, colsample_bytree=0.9, random_state=42)\n",
                "        except Exception:\n",
                "            gate = LogisticRegression(max_iter=2000, multi_class='multinomial')\n",
                "        if sample_weight is None:\n",
                "            gate.fit(gate_X, expert_label)\n",
                "        else:\n",
                "            gate.fit(gate_X, expert_label, sample_weight=np.asarray(sample_weight, dtype=np.float32))\n",
                "        gate_w = gate.predict_proba(gate_te).astype(np.float64); gate_w = gate_w / (gate_w.sum(axis=1, keepdims=True) + 1e-12)\n",
                "        p_final = np.zeros_like(test_blocks[0], dtype=np.float64)\n",
                "        for i, p_i in enumerate(test_blocks):\n",
                "            p_final += gate_w[:, [i]] * p_i.astype(np.float64)\n",
                "        p_final = p_final / (p_final.sum(axis=1, keepdims=True) + 1e-12)\n",
                "        return p_final.astype(np.float32)\n",
                "    if meta_X_meta is not None:\n",
                "        meta_X = np.hstack([meta_X, meta_X_meta])\n",
                "    if meta_te_meta is not None:\n",
                "        meta_te = np.hstack([meta_te, meta_te_meta])\n",
                "    if META_LEARNER == 'lgbm':\n",
                "        try:\n",
                "            import importlib\n",
                "            LGBMClassifier = importlib.import_module('lightgbm').LGBMClassifier\n",
                "        except Exception as e:\n",
                "            raise RuntimeError('META_LEARNER=lgbm but lightgbm not installed') from e\n",
                "        meta = LGBMClassifier(objective='multiclass', num_class=int(num_classes), max_depth=int(LGBM_MAX_DEPTH), num_leaves=int(LGBM_NUM_LEAVES), n_estimators=int(LGBM_N_ESTIMATORS), learning_rate=0.05, subsample=0.9, colsample_bytree=0.9, random_state=42)\n",
                "    else:\n",
                "        meta = LogisticRegression(max_iter=2000, multi_class='multinomial')\n",
                "    if sample_weight is None:\n",
                "        meta.fit(meta_X, y)\n",
                "    else:\n",
                "        meta.fit(meta_X, y, sample_weight=np.asarray(sample_weight, dtype=np.float32))\n",
                "    return meta.predict_proba(meta_te)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "aafde68f",
            "metadata": {},
            "source": [
                "**Κελί 16: Stacking & Meta-Learning**\n",
                "\n",
                "Υλοποιεί stacking ensemble με διάφορες επιλογές meta-learner:\n",
                "\n",
                "**Διαδικασία:**\n",
                "1. Δημιουργία OOF (Out-of-Fold) predictions για κάθε base model\n",
                "2. Εξαγωγή meta-features από πιθανότητες (top1, gap, entropy)\n",
                "3. Εκπαίδευση meta-learner στα OOF\n",
                "4. Πρόβλεψη στο test set\n",
                "\n",
                "**Meta-Learner Επιλογές:**\n",
                "- `lr`: Logistic Regression (default)\n",
                "- `lgbm`: LightGBM Classifier\n",
                "- `moe`: Mixture of Experts (gating network επιλέγει τον καλύτερο expert)\n",
                "\n",
                "**Optional TabPFN:**\n",
                "- In-context learning model για tabular δεδομένα\n",
                "- Δεν απαιτεί hyperparameter tuning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2ef808c6",
            "metadata": {},
            "outputs": [],
            "source": [
                "# sigma_omega: pipeline\n",
                "import numpy as np\n",
                "\n",
                "# uses: apply_feature_view, build_streams, adversarial_weights, CalibratedModel, models, stacking helpers\n",
                "\n",
                "def predict_probs_for_view(view, seed, X_train_base, X_test_base, y_enc, num_classes, pseudo_idx=None, pseudo_y=None, pseudo_w=None,\n",
                "                           allow_transductive=False, enable_coral=False, coral_reg=1e-3,\n",
                "                           enable_adv=False, adv_model='lr', adv_clip=10.0, adv_power=1.0,\n",
                "                           use_stacking=False, enable_lid_scaling=False, lid_t_min=1.0, lid_t_max=2.5, lid_t_power=1.0,\n",
                "                           enable_ttt=False, ttt_gap_low=0.10, ttt_gap_high=0.35, ttt_max_samples=4096, ttt_epochs=1, ttt_lr_mult=0.2,\n",
                "                           manifold_k=20, enable_pagerank=True, dae_batch=2048, dae_lr=2e-3, dae_noise=0.1, dae_epochs=30):\n",
                "    pseudo = normalize_pseudo(pseudo_idx=pseudo_idx, pseudo_y=pseudo_y, pseudo_w=pseudo_w)\n",
                "    X_v, X_test_v = apply_feature_view(X_train_base, X_test_base, view=view, seed=seed, allow_transductive=allow_transductive, enable_coral=enable_coral, coral_reg=coral_reg)\n",
                "    X_tree_tr, X_tree_te, X_neural_tr, X_neural_te, lid_tr, lid_te = build_streams(\n",
                "        X_v, X_test_v,\n",
                "        allow_transductive=allow_transductive,\n",
                "        manifold_k=manifold_k,\n",
                "        enable_pagerank=enable_pagerank,\n",
                "        dae_epochs=dae_epochs,\n",
                "        batch_size=dae_batch,\n",
                "        lr_scale=dae_lr,\n",
                "        noise_std=dae_noise,\n",
                "    )\n",
                "    pseudo_X_tree = X_tree_te[pseudo.idx] if pseudo.active() else None\n",
                "    pseudo_X_neural = X_neural_te[pseudo.idx] if pseudo.active() else None\n",
                "    sample_weight = None\n",
                "    if enable_adv:\n",
                "        sample_weight = adversarial_weights(X_v, X_test_v, seed=seed, model=adv_model, clip=adv_clip, power=adv_power)\n",
                "    names_models = [\n",
                "        ('XGB_DART', get_xgb_dart(num_classes)),\n",
                "        ('Cat_Langevin', get_cat_langevin(num_classes)),\n",
                "        ('ThetaTabM', ThetaTabM(X_neural_tr.shape[1], num_classes)),\n",
                "        ('TrueTabR', TrueTabR(num_classes)),\n",
                "    ]\n",
                "    if use_stacking:\n",
                "        p = fit_predict_stacking(\n",
                "            names_models,\n",
                "            X_tree_tr, X_tree_te, X_neural_tr, X_neural_te, X_v, X_test_v,\n",
                "            y_enc, num_classes,\n",
                "            lid_tr=lid_tr, lid_te=lid_te,\n",
                "            cv_splits=10, seed=seed, sample_weight=sample_weight,\n",
                "            pseudo_X_tree=pseudo_X_tree, pseudo_X_neural=pseudo_X_neural,\n",
                "            pseudo_y=pseudo.y if pseudo.active() else None, pseudo_w=pseudo.w if pseudo.active() else None,\n",
                "        )\n",
                "        if enable_lid_scaling:\n",
                "            p = apply_lid_temperature_scaling(p, lid_te, t_min=lid_t_min, t_max=lid_t_max, power=lid_t_power)\n",
                "        return p\n",
                "    view_probs = 0\n",
                "    for name, base in names_models:\n",
                "        print(f'  > Calibrating {name} (10-Fold)...')\n",
                "        data_tr = X_tree_tr if ('XGB' in name or 'Cat' in name) else X_neural_tr\n",
                "        data_te = X_tree_te if ('XGB' in name or 'Cat' in name) else X_neural_te\n",
                "        calibrated = CalibratedModel(base, name)\n",
                "        calibrated.fit(\n",
                "            data_tr, y_enc,\n",
                "            sample_weight=sample_weight,\n",
                "            pseudo_X=pseudo_X_tree if ('XGB' in name or 'Cat' in name) else pseudo_X_neural,\n",
                "            pseudo_y=pseudo.y if pseudo.active() else None,\n",
                "            pseudo_w=pseudo.w if pseudo.active() else None,\n",
                "        )\n",
                "        p = calibrated.predict_proba(data_te)\n",
                "        if enable_ttt and is_torch_model(base):\n",
                "            if not allow_transductive:\n",
                "                raise RuntimeError('ENABLE_TTT requires ALLOW_TRANSDUCTIVE=1')\n",
                "            idx_silver, y_pseudo = select_silver_samples(\n",
                "                p, gap_low=ttt_gap_low, gap_high=ttt_gap_high, max_samples=ttt_max_samples, seed=seed,\n",
                "            )\n",
                "            if idx_silver.size > 0:\n",
                "                base.finetune_on_pseudo(data_te[idx_silver], y_pseudo, epochs=ttt_epochs, lr_mult=ttt_lr_mult)\n",
                "                p = base.predict_proba(data_te)\n",
                "        if enable_lid_scaling:\n",
                "            p = apply_lid_temperature_scaling(p, lid_te, t_min=lid_t_min, t_max=lid_t_max, power=lid_t_power)\n",
                "        view_probs += p\n",
                "    return view_probs / len(names_models)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "be1217f1",
            "metadata": {},
            "source": [
                "**Κελί 17: Per-View Pipeline**\n",
                "\n",
                "Κεντρική συνάρτηση `predict_probs_for_view()` που ενορχηστρώνει:\n",
                "\n",
                "1. **Feature Transform**: Εφαρμογή του επιλεγμένου view (raw, quantile, κλπ.)\n",
                "2. **Stream Building**: Δημιουργία tree/neural streams με DAE embeddings και manifold features\n",
                "3. **Adversarial Weights**: Υπολογισμός βαρών από domain discriminator\n",
                "4. **Model Training**: Εκπαίδευση XGBoost, CatBoost, ThetaTabM, TabR\n",
                "5. **Calibration**: Βαθμονόμηση πιθανοτήτων με isotonic regression\n",
                "6. **Optional Stacking**: Meta-learner πάνω στα OOF\n",
                "7. **Optional TTT**: Test-Time Training σε uncertain samples\n",
                "8. **LID Scaling**: Temperature scaling βάσει τοπικής πολυπλοκότητας\n",
                "\n",
                "**Επιστρέφει:** Μέσο όρο πιθανοτήτων από όλα τα μοντέλα"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3b65f1ec",
            "metadata": {},
            "outputs": [],
            "source": [
                "# main entrypoint\n",
                "import os\n",
                "import numpy as np\n",
                "import torch\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "\n",
                "\n",
                "# Uses all previously defined helpers\n",
                "\n",
                "def partd_main():\n",
                "    print('>>> INITIATING PART D GRANDMASTER PROTOCOL <<<')\n",
                "    X, y, X_test = load_data_safe()\n",
                "    le = LabelEncoder(); y_enc = le.fit_transform(y); num_classes = len(le.classes_)\n",
                "\n",
                "\n",
                "    # Razor\n",
                "    print('[RAZOR] Scanning for noise features...')\n",
                "    from catboost import CatBoostClassifier\n",
                "    scout = CatBoostClassifier(iterations=500, verbose=0, task_type='GPU' if torch.cuda.is_available() else 'CPU')\n",
                "    scout.fit(X, y_enc)\n",
                "    imps = scout.get_feature_importance(); thresh = np.percentile(imps, 20)\n",
                "    keep_mask = imps > thresh\n",
                "    X_razor = X[:, keep_mask]; X_test_razor = X_test[:, keep_mask]\n",
                "    print(f'  > Dropped {np.sum(~keep_mask)} features. New Dim: {X_razor.shape[1]}')\n",
                "\n",
                "\n",
                "    final_ensemble_probs = 0\n",
                "    if ENABLE_SELF_TRAIN and SELF_TRAIN_ITERS > 0:\n",
                "        if not ALLOW_TRANSDUCTIVE:\n",
                "            raise RuntimeError('ENABLE_SELF_TRAIN requires ALLOW_TRANSDUCTIVE=1')\n",
                "        pseudo = PseudoData.empty(); last_avg_probs = None\n",
                "        for it in range(int(SELF_TRAIN_ITERS) + 1):\n",
                "            print(f'\\n>>> SELF-TRAIN ITERATION {it} (pseudo={len(pseudo.idx)}) <<<')\n",
                "            probs_per_view = {v: [] for v in VIEWS}; preds_per_view = {v: [] for v in VIEWS}\n",
                "            for seed in SEEDS:\n",
                "                seed_everything(seed)\n",
                "                for view in VIEWS:\n",
                "                    p = predict_probs_for_view(\n",
                "                        view, seed, X_razor, X_test_razor, y_enc, num_classes,\n",
                "                        pseudo_idx=pseudo.idx, pseudo_y=pseudo.y, pseudo_w=pseudo.w,\n",
                "                        allow_transductive=ALLOW_TRANSDUCTIVE, enable_coral=ENABLE_CORAL, coral_reg=CORAL_REG,\n",
                "                        enable_adv=ENABLE_ADV_REWEIGHT, adv_model=ADV_MODEL, adv_clip=ADV_CLIP, adv_power=ADV_POWER,\n",
                "                        use_stacking=USE_STACKING, enable_lid_scaling=ENABLE_LID_SCALING,\n",
                "                        lid_t_min=LID_T_MIN, lid_t_max=LID_T_MAX, lid_t_power=LID_T_POWER,\n",
                "                        enable_ttt=ENABLE_TTT, ttt_gap_low=TTT_GAP_LOW, ttt_gap_high=TTT_GAP_HIGH, ttt_max_samples=TTT_MAX_SAMPLES,\n",
                "                        ttt_epochs=TTT_EPOCHS, ttt_lr_mult=TTT_LR_MULT,\n",
                "                        manifold_k=MANIFOLD_K, enable_pagerank=ENABLE_PAGERANK,\n",
                "                        dae_batch=BATCH_SIZE, dae_lr=LR_SCALE, dae_noise=DAE_NOISE_STD, dae_epochs=DAE_EPOCHS,\n",
                "                    )\n",
                "                    probs_per_view[view].append(p); preds_per_view[view].append(np.argmax(p, axis=1))\n",
                "            probs_tensor = np.stack([np.stack(probs_per_view[v], axis=0) for v in VIEWS], axis=0)\n",
                "            preds_tensor = np.stack([np.stack(preds_per_view[v], axis=0) for v in VIEWS], axis=0)\n",
                "            avg_probs = probs_tensor.mean(axis=(0, 1)); last_avg_probs = avg_probs\n",
                "            if it < int(SELF_TRAIN_ITERS):\n",
                "                votes = preds_tensor.reshape(preds_tensor.shape[0] * preds_tensor.shape[1], preds_tensor.shape[2])\n",
                "                mode_pred, agree_frac_votes = vote_mode_and_agreement(votes)\n",
                "                view_agree_frac = view_agreement_fraction(preds_tensor, mode_pred)\n",
                "                conf = np.max(avg_probs, axis=1)\n",
                "                mask = (\n",
                "                    (conf >= float(SELF_TRAIN_CONF))\n",
                "                    & (agree_frac_votes >= float(SELF_TRAIN_AGREE))\n",
                "                    & (view_agree_frac >= float(SELF_TRAIN_VIEW_AGREE))\n",
                "                )\n",
                "                idx = np.nonzero(mask)[0]\n",
                "                if idx.size > int(SELF_TRAIN_MAX):\n",
                "                    top = np.argsort(conf[idx])[::-1][: int(SELF_TRAIN_MAX)]; idx = idx[top]\n",
                "                pseudo_idx = idx.astype(np.int64)\n",
                "                pseudo_y = mode_pred[pseudo_idx]\n",
                "                pseudo_w = np.power(conf[pseudo_idx].astype(np.float32), float(SELF_TRAIN_WEIGHT_POWER))\n",
                "                pseudo = PseudoData(idx=pseudo_idx, y=pseudo_y, w=pseudo_w)\n",
                "                print(f'  [SELF-TRAIN] mined {len(pseudo_idx)} pseudo')\n",
                "        final_ensemble_probs = last_avg_probs\n",
                "    else:\n",
                "        for seed in SEEDS:\n",
                "            print(f'\\n>>> SEQUENCE START: SEED {seed} <<<')\n",
                "            seed_everything(seed)\n",
                "            view_probs_total = 0; view_count = 0\n",
                "            for view in VIEWS:\n",
                "                print(f'  [VIEW] {view}')\n",
                "                if USE_STACKING:\n",
                "                    print('  > Stacking meta-learner (OOF -> meta)...')\n",
                "                view_probs_total += predict_probs_for_view(\n",
                "                    view, seed, X_razor, X_test_razor, y_enc, num_classes,\n",
                "                    allow_transductive=ALLOW_TRANSDUCTIVE, enable_coral=ENABLE_CORAL, coral_reg=CORAL_REG,\n",
                "                    enable_adv=ENABLE_ADV_REWEIGHT, adv_model=ADV_MODEL, adv_clip=ADV_CLIP, adv_power=ADV_POWER,\n",
                "                    use_stacking=USE_STACKING, enable_lid_scaling=ENABLE_LID_SCALING,\n",
                "                    lid_t_min=LID_T_MIN, lid_t_max=LID_T_MAX, lid_t_power=LID_T_POWER,\n",
                "                    enable_ttt=ENABLE_TTT, ttt_gap_low=TTT_GAP_LOW, ttt_gap_high=TTT_GAP_HIGH, ttt_max_samples=TTT_MAX_SAMPLES,\n",
                "                    ttt_epochs=TTT_EPOCHS, ttt_lr_mult=TTT_LR_MULT,\n",
                "                    manifold_k=MANIFOLD_K, enable_pagerank=ENABLE_PAGERANK,\n",
                "                    dae_batch=BATCH_SIZE, dae_lr=LR_SCALE, dae_noise=DAE_NOISE_STD, dae_epochs=DAE_EPOCHS,\n",
                "                )\n",
                "                view_count += 1\n",
                "            final_ensemble_probs += (view_probs_total / max(1, view_count))\n",
                "        final_ensemble_probs /= len(SEEDS)\n",
                "    preds = np.argmax(final_ensemble_probs, axis=1); labels = le.inverse_transform(preds)\n",
                "    os.makedirs('PartD/outputs', exist_ok=True)\n",
                "    np.save('PartD/outputs/labelsX_grandmaster.npy', labels)\n",
                "    print('\\n>>> PART D PROTOCOL COMPLETE <<<')\n",
                "    return labels"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d2cadce4",
            "metadata": {},
            "source": [
                "**Κελί 18: Κύρια Ενορχήστρωση (Main Orchestration)**\n",
                "\n",
                "Η συνάρτηση `partd_main()` εκτελεί το πλήρες pipeline:\n",
                "\n",
                "**Βήματα:**\n",
                "1. **Φόρτωση Δεδομένων**: Χρήση `load_data_safe()`\n",
                "2. **Razor Feature Selection**: Αφαίρεση χαμηλής σημασίας features με CatBoost importance\n",
                "3. **Multi-Seed Ensembling**: Επανάληψη για κάθε seed στο SEEDS\n",
                "4. **Multi-View Ensembling**: Επανάληψη για κάθε view στο VIEWS\n",
                "5. **Optional Self-Training**: Επαναληπτική προσθήκη confident pseudo-labels\n",
                "6. **Final Prediction**: Μέσος όρος πιθανοτήτων, argmax για κλάση\n",
                "7. **Save Outputs**: Αποθήκευση σε `PartD/outputs/labelsX_grandmaster.npy`\n",
                "\n",
                "**Self-Training Mode:**\n",
                "- Απαιτεί `ENABLE_SELF_TRAIN=1` και `ALLOW_TRANSDUCTIVE=1`\n",
                "- Επιλέγει samples με υψηλή confidence και agreement μεταξύ μοντέλων/views\n",
                "- Επαναλαμβάνει με επαυξημένα δεδομένα"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ad3dd9bc",
            "metadata": {},
            "outputs": [],
            "source": [
                "**Κελί 19: Εκτέλεση (Runner)**\n",
                "\n",
                "Εκτελεί το ενσωματωμένο pipeline και αποθηκεύει τα αποτελέσματα.\n",
                "\n",
                "**Αρχεία Εξόδου:**\n",
                "- `PartD/outputs/labelsX_grandmaster.npy`: Κύριο αρχείο predictions\n",
                "- `Submission/labels1.npy`: Αντίγραφο για submission\n",
                "\n",
                "**Οδηγίες Χρήσης:**\n",
                "- Για smoke test: `DO_FULL_RUN=False`, `SMOKE_RUN=True`\n",
                "- Για πλήρη εκτέλεση: `DO_FULL_RUN=True` (απαιτεί GPU)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}